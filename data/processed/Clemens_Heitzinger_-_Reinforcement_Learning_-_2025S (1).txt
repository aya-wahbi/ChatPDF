REINFORCEMENT LEARNING: ALGORITHMS & CONVERGENCE
CLEMENS HEITZINGER mailto:Clemens.Heitzinger@TUWien.ac.at http://Clemens.Heitzinger.name
Edition 0.1 â€” June 24, 2025
ii
Contents
Preface
1 Introduction
1.1 The Concept of Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.2 Examples of Applications 1.2.1 Autonomous Driving . . . . . . . . . . . . . . . . . . . . . 1.2.2 Board Games: Backgammon, Chess, Shogi, and Go . . . . 1.2.3 Card Games: Poker and Schnapsen . . . . . . . . . . . . . 1.2.4 Computer Games . . . . . . . . . . . . . . . . . . . . . . . 1.2.5 Finance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.2.6 Medicine 1.2.7 Optimal Control and Robotics . . . . . . . . . . . . . . . 1.2.8 Recommendation Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.2.9 1.3 Bibliographical Remarks . . . . . . . . . . . . . . . . . . . . . . .
Supply Chains and Inventories
2 Markov Decision Processes and Dynamic Programming
. . . . . . . . . . . . . . . . . . . . . . . . 2.1 Multi-Armed Bandits 2.2 Markov Decision Processes . . . . . . . . . . . . . . . . . . . . . . 2.3 Rewards, Returns, and Episodes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.4 Standard Environments Simple Grid World (Discrete/Discrete) . . . . . . . . . . . 2.4.1 2.4.2 Windy Grid World (Discrete/Discrete) . . . . . . . . . . . 2.4.3 Cliff Walking (Discrete/Discrete) . . . . . . . . . . . . . . 2.4.4 Frozen Lake (Discrete/Discrete) . . . . . . . . . . . . . . 2.4.5 Rock Paper Scissors (Discrete/Discrete) . . . . . . . . . . 2.4.6 Prisonerâ€™s Dilemma (Discrete/Discrete) . . . . . . . . . . 2.4.7 Blackjack (Discrete/Discrete) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.4.8
Schnapsen (Discrete/Discrete)
ix
1 1 3 3 3 4 5 5 6 6 7 7 7
9 9 14 17 19 20 20 21 22 23 25 25 26
iii
iv
2.4.9 Autoregressive Trend Process (Continuous/Discrete) . . . 2.5 Policies, Value Functions, and Bellman Equations . . . . . . . . . 2.6 On-Policy and Off-Policy Learning . . . . . . . . . . . . . . . . . 2.7 Policy Evaluation (Prediction) . . . . . . . . . . . . . . . . . . . 2.8 Policy Improvement . . . . . . . . . . . . . . . . . . . . . . . . . 2.9 Policy Iteration . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.10 Value Iteration . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.11 Bibliographical and Historical Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.12 Exercises 2.12.1 Applications and Environments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.12.2 Multi-Armed Bandits 2.12.3 Step Sizes . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.12.4 Basic Definitions . . . . . . . . . . . . . . . . . . . . . . . 2.12.5 Dynamic Programming . . . . . . . . . . . . . . . . . . .
3 Taxonomy of Algorithms
3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2 Types of Errors . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2.1 The Mean Squared Value Error . . . . . . . . . . . . . . . 3.2.2 The Mean Squared Return Error . . . . . . . . . . . . . . 3.2.3 Mean Squared Bellman Errors . . . . . . . . . . . . . . . 3.2.4 The Mean Squared Temporal-Difference Error . . . . . . . 3.3 Learnability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3.1 The Mean Squared Return Error . . . . . . . . . . . . . . 3.3.2 The Mean Squared Value Error . . . . . . . . . . . . . . . 3.3.3 The Mean Squared Temporal-Difference Error . . . . . . .
4 Monte-Carlo Methods
4.1 Monte-Carlo Prediction . . . . . . . . . . . . . . . . . . . . . . . 4.2 On-Policy Monte-Carlo Control . . . . . . . . . . . . . . . . . . . 4.3 Off-Policy Methods and Importance Sampling . . . . . . . . . . . 4.4 Convergence of First-Visit Monte-Carlo Prediction . . . . . . . . 4.5 Convergence of Every-Visit Monte-Carlo Prediction . . . . . . . . 4.6 Bibliographical and Historical Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.7 Exercises
5 Temporal-Difference Learning
5.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.2 On-Policy Temporal-Difference Prediction: TD(0) . . . . . . . . . 5.3 On-Policy Temporal-Difference Control: SARSA . . . . . . . . .
Contents
26 27 29 29 30 32 33 33 36 36 37 37 38 38
41 41 42 42 43 44 44 45 45 46 47
49 49 51 51 54 55 56 56
59 59 59 61
Contents
61 5.4 On-Policy Temporal-Difference Control: Expected SARSA . . . . 62 5.5 Off-Policy Temporal-Difference Control: Q-Learning . . . . . . . 63 5.6 Double Q-Learning . . . . . . . . . . . . . . . . . . . . . . . . . . 5.7 Deep Q-Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . 64 5.8 On-Policy Multi-Step Temporal-Difference Prediction: ğ‘›-step TD 67 5.9 On-Policy Multi-Step Temporal-Difference Control: ğ‘›-step SARSA 69 71 5.10 Bibliographical and Historical Remarks 71 5.11 Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6 Convergence of Discrete Q-Learning
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.1 6.2 Convergence Proved Using Action Replay . . . . . . . . . . . . . 6.3 Convergence Proved Using Stochastic Approximation . . . . . . . . . . . . . . . . . . . . . 6.4 Bibliographical and Historical Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.5 Exercises
7 On-Policy Prediction with Approximation
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.1 7.2 Stochastic Gradient and Semi-Gradient Methods . . . . . . . . . 7.3 Linear Function Approximation . . . . . . . . . . . . . . . . . . . 7.4 Features for Linear Methods . . . . . . . . . . . . . . . . . . . . . 7.4.1 Polynomials . . . . . . . . . . . . . . . . . . . . . . . . . . 7.4.2 Fourier Basis . . . . . . . . . . . . . . . . . . . . . . . . . 7.4.3 Coarse Coding . . . . . . . . . . . . . . . . . . . . . . . . 7.4.4 Tile Coding . . . . . . . . . . . . . . . . . . . . . . . . . . 7.4.5 Radial Basis Functions . . . . . . . . . . . . . . . . . . . . 7.5 Nonlinear Function Approximation . . . . . . . . . . . . . . . . . 7.5.1 Artificial Neural Networks . . . . . . . . . . . . . . . . . . 7.5.2 Memory Based Function Approximation . . . . . . . . . . 7.5.3 Kernel-Based Function Approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7.6 Bibliographical and Historical Remarks Problems
8 Policy-Gradient Methods
99 99 . . . . . . . . . . . . . . . . . . . 100 . . . . . . . . . . . . . . . . . . . . . . 100 Infinite Action Sets . . . . . . . . . . . . . . . . . . . . . . 101 8.3 The Policy-Gradient Theorem . . . . . . . . . . . . . . . . . . . . 101 8.4 Monte-Carlo Policy-Gradient Method: REINFORCE . . . . . . . 105
8.1 8.2 Finite and Infinite Action Sets 8.2.1 Finite Action Sets 8.2.2
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
v
73 73 74 81 87 88
89 89 90 92 95 95 96 96 96 96 96 96 96 96 97 97
vi
Contents
8.5 Monte-Carlo Policy-Gradient Method: REINFORCE with Baseline107 8.6 Temporal-Difference Policy-Gradient Methods: Actor-Critic Meth-
ods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109 . . . . . . . . . . . . . . 109 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109
8.7 Bibliographical and Historical Remarks Problems
9 Hamilton-Jacobi-Bellman Equations
111 9.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111 9.2 The Hamilton-Jacobi-Bellman Equation . . . . . . . . . . . . . . 112 9.3 An Example of Optimal Control . . . . . . . . . . . . . . . . . . 115 9.4 Viscosity Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . 117 . . . . . . . . . . . . . . . . . . . . . 119 9.5 Stochastic Optimal Control 9.6 Bibliographical and Historical Remarks . . . . . . . . . . . . . . 120 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120 Problems
10 Deep Reinforcement Learning
121 10.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121 . . . . . . . . . . . . . . . . . . . . . . . . . . 121 10.2 Atari 2600 Games 10.3 Go and Tree Search (AlphaGo) . . . . . . . . . . . . . . . . . . . 124 10.4 Learning Go Tabula Rasa (AlphaGo Zero) . . . . . . . . . . . . . 125 10.5 Chess, Shogi, and Go through Self-Play (AlphaZero) . . . . . . . 125 10.6 Video Games of the 2010s (AlphaStar) . . . . . . . . . . . . . . . 126 10.7 Improvements to DQN and their Combination . . . . . . . . . . . 126 10.7.1 Double Q-Learning . . . . . . . . . . . . . . . . . . . . . . 127 10.7.2 Prioritized Replay . . . . . . . . . . . . . . . . . . . . . . 127 10.7.3 Dueling Networks . . . . . . . . . . . . . . . . . . . . . . . 127 10.7.4 Multi-Step Methods . . . . . . . . . . . . . . . . . . . . . 128 10.7.5 Distributional Reinforcement Learning . . . . . . . . . . . 128 10.7.6 Noisy Neural Networks . . . . . . . . . . . . . . . . . . . . 128
11 Distributional Reinforcement Learning
131 11.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131 11.2 Markov Decision Processes and Bellman Operators . . . . . . . . 131 11.3 Distributional Speedy Q-Learning . . . . . . . . . . . . . . . . . . 133 11.4 Bibliographical Remarks . . . . . . . . . . . . . . . . . . . . . . . 133 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133 11.5 Exercises
12 Large Language Models
135 12.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137 12.2 Transformers
Contents
12.3 InstructGPT and ChatGPT . . . . . . . . . . . . . . . . . . . . . 141 12.4 Proximal Policy Optimization (PPO) . . . . . . . . . . . . . . . . 142 . . . . . . . . . . . . . . 146 12.5 Bibliographical and Historical Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146 12.6 Problems
A Analysis
147 . . . . . . . . . . . . . . . . . . . 147 A.1 The Riemann-Stieltjes Integral A.2 The Banach Fixed-Point Theorem . . . . . . . . . . . . . . . . . 151 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152 A.3 Exercises
B Measure and Probability Theory
155 B.1 Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155 B.2 Measures and Measure Spaces . . . . . . . . . . . . . . . . . . . . 155 . . . . . . . . . . . . . . . . . . . . . . . . 159 B.3 The Lebesgue Integral B.3.1 Construction and Definition Using the Riemann Integral . 160 B.3.2 Construction and Definition Using Simple Functions . . . 162 B.3.3 Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . 164 B.4 The Radon-Nikodym Derivative . . . . . . . . . . . . . . . . . . . 165 B.5 Lebesgue Convergence Theorems . . . . . . . . . . . . . . . . . . 167 B.6 Probability Spaces and Random Variables . . . . . . . . . . . . . 170 B.7 Inequalities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173 B.7.1 Basic Inequalities . . . . . . . . . . . . . . . . . . . . . . . 173 B.7.2 Concentration Inequalities . . . . . . . . . . . . . . . . . . 175 B.8 Characteristic Functions . . . . . . . . . . . . . . . . . . . . . . . 186 B.9 Types of Convergence . . . . . . . . . . . . . . . . . . . . . . . . 187 B.10 LÃ©vyâ€™s Continuity Theorem . . . . . . . . . . . . . . . . . . . . . 189 B.11 The Laws of Large Numbers and the Central Limit Theorem . . 190 B.12 Waldâ€™s Equation . . . . . . . . . . . . . . . . . . . . . . . . . . . 194
C Stochastic Approximation
201 C.1 Dvoretzkyâ€™s Approximation Theorem . . . . . . . . . . . . . . . . 201 C.2 Venterâ€™s Generalization . . . . . . . . . . . . . . . . . . . . . . . 203 C.3 Example: Perturbed Fixed-Point Iteration . . . . . . . . . . . . . 205 C.4 Polyak and Tsypkinâ€™s Theorem . . . . . . . . . . . . . . . . . . . 208 C.5 Bibliographical and Historical Remarks . . . . . . . . . . . . . . 215 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215 C.6 Exercises
D Software Libraries
217 D.1 Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . 217 D.1.1 Environments and Applications . . . . . . . . . . . . . . . 217
vii
viii
Contents
D.1.2 Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . 218 D.1.3 Policy Evaluation . . . . . . . . . . . . . . . . . . . . . . . 219 D.2 Artificial Neural Networks / Deep Learning . . . . . . . . . . . . 219 . . . . . . . . . . . . . . . . . . . . . . . 220 D.3 Large Language Models
Bibliography
223
List of Algorithms
231
Index
233
Preface
Reinforcement learning (RL) is an incredibly appealing subject. Firstly, it is a very general concept: An agent interacts with an environment with the goal to maximize the rewards it receives. The environment is random and communicates states and rewards to the agent, while the agent chooses actions according to a possibly random policy. The challenge is to find policies that maximize the expected value of all future rewards. Because reinforcement learning is such a general concept, it encompasses many real-world applications and is at the core of artificial intelligence and machine learning.
Secondly, the concepts and algorithms developed in reinforcement learning are leading to more and more general capabilities of artificial-intelligence (AI) systems and will very likely be part of artificial general intelligence. There are many similarities between reinforcement learning and how (human) brains work. These similarities must be explored in order to further our understanding of brains and human intelligence.
Thirdly, superhuman capabilities of RL learning algorithms have been demon- strated in various areas, and the list of extraordinary capabilities of AI systems built on reinforcement learning is continually expanding. Prominent examples are playing backgammon, Atari 2600 games, many more computer games, card games, chess, Go, and shogi at superhuman levels. Probably most famously, however, reinforcement learning is the last and crucial step in training large language models such as ChatGPT.
This book derives and describes the theory of reinforcement learning. The most fundamental as well as the most powerful RL algorithms are discussed. In addition to calculating optimal policies using powerful learning algorithms, we must also strive to provide guarantees regarding the quality of the learned results, i.e. the performance of the policies calculated by the algorithms. Hence the two main kinds of theoretical results concern the convergence of an algorithm to an optimal policy and performance guarantees for these policies. This book collects the theoretic foundations of reinforcement learning in view of these main questions.
ix
x
Preface
To help translate theory into practice, this book also includes pseudocode and programming exercises that are concerned with the implementation of algo- rithms. The purpose of the programming exercises is to gain working knowledge, to try to exhibit both the advantages and disadvantages of certain methods, to show the challenges faced when developing new algorithms, and to inspire the reader to experiment with the algorithms. Both theoretic and programming ex- ercises are an invitation to the reader to further explore this captivating subject. Clearly, it was necessary to make choices â€“ in many cases hard ones â€“ about which algorithms, theorems, and proofs could be included in this book. Pref- erence has been given to the more fundamental and general ones. In any case, care was taken to provide a complete and self-contained treatment. The ap- pendix collects definitions, concepts, and results coming from outside reinforce- ment learning in a form that is expedient for our use here. Depending on the background of the reader, the appendix can be skipped in whole or in part.
This book can be used in various ways: It can be used for self-study, but also as the basis for courses on reinforcement learning. I hope that it is useful to various audiences such as anybody interested in AI and/or machine learn- ing, to theoreticians interested in algorithms, theoretic results, and proofs, and to practitioners interested in the inner workings of the algorithms and seeking assurances in the quality of the computations.
I hope that you have as much fun reading the book as I had writing it.
Acknowledgments. I am happy to acknowledge the productive discussion with my students Markus BÃ¶ck, Florian Chen, Patrizia Daxbacher, Sebastian Eresheim, Helmut Horvath, Lorenz Kapral, Tobias Kietreiber, Julien Malle, Daryna Oliynyk, Daniel Pasterk, Markus Peschl, Tobias Salzer, Jakob aus der Schmitten, Felix Wagner, and Richard Weiss as well as with my colleagues Felix Birkelbach, Rene Hofmann, and Carlotta Sophie von Tubeuf.
Furthermore, I am happy to acknowledge the support of the Austrian Re- search Promotion Agency (FFG) via the research project RELY (Reliable Rein- forcement Learning for Sustainable Energy Systems).
Vienna, June 2025
Clemens Heitzinger
Chapter 1
Introduction
Here the basic concept of reinforcement learning and the fundamental notions used in formulating problems in reinforcement learning are introduced. Re- inforcement learning is a very general concept and applies to time-dependent learning problems whenever an agent interacts with its environment. The main goal in reinforcement learning is to find optimal policies for the agent to follow. Examples of applications of reinforcement learning are given.
1.1 The Concept of Reinforcement Learning
One of the major appeals of reinforcement learning (RL) is that it applies to all situations where an agent interacts with an environment in a time-dependent manner.
The basic concept is illustrated in Figure 1.1. Firstly, the environment and the agent are possibly randomly initialized at the beginning of each episode. In each new time step ğ‘¡ (which is increased in the top right in the figure), the environment and the agent enter the next state ğ‘†ğ‘¡ and the agent receives the reward ğ‘…ğ‘¡. Then the agent chooses the next action ğ´ğ‘¡ according to its possibly random policy. In the next iteration, this action affects the environment and the agent in a possibly random manner; the environment and the agent enter the next state; the agent receives a reward; and so forth.
In this manner, sequences of states ğ‘†ğ‘¡, actions ğ´ğ‘¡, and rewards ğ‘…ğ‘¡ are gen- erated. These sequences may be infinite (at least theoretically) or they end in a terminal state (which always happens in practice) after a finite number of time steps. A collection of a sequence of states ğ‘†ğ‘¡, a sequence of actions ğ´ğ‘¡, and a sequence of rewards ğ‘…ğ‘¡ is called an episode. It is convenient to treat both cases, i.e., finite and infinite numbers of time steps, within the same theoretical
1
2
Chapter 1.
Introduction
Action ğ´ğ‘¡
Environment
Next time step, ğ‘¡ âˆ¶= ğ‘¡ + 1
Agent
State ğ‘†ğ‘¡, reward ğ‘…ğ‘¡
Figure 1.1: Environments, agents, and their interactions.
framework, which can be done under reasonable assumptions.
The return is the expected value of the sum of all (discounted) rewards the
agent receives. We call a policy optimal if it maximizes the return.
The main goal in RL is to find optimal policies for the agent to follow. The input to a policy is the current state the agent finds itself in. The output of a deterministic policy is the action to take, and â€“ more generally â€“ the output of a random policy is a probability distribution that assigns probabilities to all actions that are possible in the state. Hence policies may be random, just as environments may be random.
It is necessary to allow random policies. An well-known example where the optimal policy must be random is the game of Rock Paper Scissors (Lizard Spock); if it were not random, it could easily be exploited by the opponent.
Because the main goal in RL is to find optimal policies, the main purpose of this book is to present and to discuss methods and algorithms that calculate such optimal policies.
Because of its generality, the concept of RL encompasses almost all real- world time-dependent problems whose solution requires foresight or intelligence. Therefore, it is at the core of artificial intelligence (AI). Any situation where an agent interacts with an environment and tries to optimize the sum of its rewards is a problem in the realm of RL, irrespective of the cardinality of the sets of spaces and actions and irrespective of whether the environment is deterministic or stochastic.
Depending on the problem, some information may be hidden from the agent. For example, in chess, there is no hidden information; the whole state of the
1.2. Examples of Applications
game is known to both players. On the other hand, in poker, the agents do not have access to all the information and it is hence called a hidden-information In this game. Therefore, there is observable and unobservable information. book, the information that is observable by the agent determines the state, i.e., unobservable information is not included in the definition of the state.
If the discount factor is zero, RL simplifies to supervised learning. In other words, RL is a generalization of supervised learning, which deals with prob- lems that are not time-dependent. There is also a relation between RL and unsupervised learning. In RL, the initially unknown internal structure of the environment is learned and exploited in order to maximize the return. This is done implicitly, in contrast to unsupervised learning, whose goal is to bring hidden structures to light.
1.2 Examples of Applications
Clearly, the more complicated or random the environment is, the more challeng- ing the RL problem is. Thanks to advanced algorithms and â€“ in some cases â€“ to gigantic amounts of data or computational resources or both, it has become possible to solve real-world problems at the level of human intelligence or above. Some applications, mentioned in alphabetical order, are discussed the following.
1.2.1 Autonomous Driving
In autonomous driving, the agent has to traverse a stochastic environment quickly and safely. The agent should also drive smoothly, except in dangerous situations, when it should act decisively. Sophisticated simulation environments including simulated sensor output have been developed, e.g. [1]. For the pop- ular computer game Gran Turismo, agents that drive at a superhuman level have been developed [2]. RL agents can learn to drive on simulated highways and in simulated cities [3, 4], even in changing environments and with failing equipment [5].
1.2.2 Board Games: Backgammon, Chess, Shogi, and Go
Backgammon is a popular two-player board game that is played with counters and dice on tables boards. It is a member of the family of tables games, which date back nearly five thousand years. The use of dice implies that backgammon games include randomness.
In the 1990s, Gerald Tesauro developed the backgammon program TD- Gammon, which used temporal-difference learning and artificial neural networks
3
4
Chapter 1.
Introduction
[6]. It achieved a level of play only slightly below that of the best human backgammon players at the time; in 1998, it lost a 100-game competition against the world champion by only eight points [7]. TD-Gammon also had strong im- pact on the backgammon community, since professional players soon adopted its evaluation of certain opening strategies.
In 1996, then world chess champion Garry Kasparov won a six-game match against IBMâ€™s Deep Blue chess program by 4:2. In the next year, an updated version of Deep Blue defeated Garry Kasparov by 31 In 2002, a match between then world chess champion Vladimir Kramnik and the chess program Deep Fritz ended in a 4:4 draw. Ever since that time, chess has been solved in the sense that the best chess programs play better than the best humans.
2 :21 2.
It is important to note that these chess programs were not self-learning, but they were based on tree search and fixed rules to assess the values of positions. Since self-learning is a defining characteristic of algorithms in machine learning and AI, these programs were not artificial intelligences.
After chess, Go was the only remaining classical board game which humans could play better than computers. Go games have a much larger search spaces than chess games, which is the reason why Go was the last unsolved board game and remained a formidable challenge. It was commonly believed at the time that it would take decades till Go can be solved.
In [8], an RL algorithm called AlphaGo Zero learned to beat the best humans consistently using no prior knowledge apart from the rules of the game, i.e., tabula rasa, albeit using vast computational resources. AlphaGo Zero is a truly self-learning algorithm.
In the next step, in [9], a more general RL algorithm called AlphaZero learned to play the three games of chess, shogi (Japanese chess), and Go again tabula rasa, but now using only self-play. It defeated world-champion programs in each of the three games.
1.2.3 Card Games: Poker and Schnapsen
Card games differ from board games regarding the amount of information that is available to the players. In board games, the players know the full state of the game at all times, whereas in card games some information is available to all players, some information only to certain players, and some information is known by no player. Such games are called hidden-information games.
In 2019, Brown and Sandholm reported on their poker program program, dubbed Pluribus, that plays six-player no-limit Texas holdâ€™em [10]. Pluribus learned by self-play, playing against five copies of itself. In order to evaluate the performance of Pluribus, it had to compete with five elite professional poker
1.2. Examples of Applications
players or with five copies of itself playing against one professional. Over the course of 10000 hands of poker, it was found that it performs significantly better than humans [10].
Schnapsen is trick-taking card game of the ace-ten family for two players. It is the national card game of Austria and Hungary, and it is very popular in Bavaria and Upper Silesia as well. Schnapsen is a both a point-trick and trick-and-draw game and involves lots of strategy. Schnapsen is played in many tournaments, and there are also variants for three and four players (Dreierschnapsen and Bauernschnapsen, respectively). In contrast to poker, it is not considered a game of luck, but a game of skill, according to Austrian law. Schnapsen can be traced back to the 1700s, and the earliest description of its predecessor Mariage dates back to 1715.
The aim of the game is to take tricks in order to collect 66 or more card points as quickly as possible in rounds. In each round, a player can win one, two, or three game points. The player who wins seven game points first wins the game.
In his masterâ€™s thesis, Tobias Salzer developed a deep RL algorithm for Schnapsen, which beat the previously strongest Schnapsen program, playing above human expert level, in a tournament of twenty games by 12:8 [11].
1.2.4 Computer Games
The study of computer games in the context of AI dates back at least to Falkenâ€™s Maze [12].
In the context of RL playing computer games, the game is the environment and the agent has to learn an optimal strategy. One of the great successes in the history of RL and a precursor to the single algorithm that solved chess, Go, and shogi (see Section 1.2.2) was a single deep RL algorithm that can learn to play many (but not all) Atari 2600 games at the human level or above [13].
A few years later, an RL algorithm learned to play Quake III Arena in a mode called â€œCapture the Flagâ€ at the human level [14]. Also in 2019, a multi-agent RL algorithm learned to play StarCraft II, a contemporary computer game, at the grandmaster level. It did not achieve clearly superhuman capabilities, which may be due to the hidden information in this computer game.
1.2.5 Finance
Oftentimes in finance, the environment is a certain market, the state is given by the positions in the portfolio, and the profits become positive rewards, while a
5
6
Chapter 1.
Introduction
risk measure may become a negative reward. In this manner, wealth is maxi- mized, while the risk is managed.
1.2.6 Medicine
The main goal in medicine is to treat patients so that they recover as fully and as quickly as possible. In the nomenclature of RL, the patient is the environment, the agent is the medical doctor, the state is given by any measurements of the patientâ€™s condition, and the actions are the therapies or medications. In this manner, treating patient becomes an RL problem. Questions such as how to find optimal therapies, how to personalize therapies, and how to evaluate therapies thus become amenable to quantification and can be solved by RL algorithms. This approach to medicine paves the way towards truly personalized and predictive medicine.
Intensive care is particularly well-suited for RL, because many values are recorded over relatively short periods of time in intensive-care units. Typical episodes last a few days, and hundreds of values may be recorded every few hours.
The most prevalent condition and at the same time the most prevalent cause of death in intensive-care units is sepsis. Therefore sepsis is an obvious ap- plication of RL in medicine. After clustering of the patientsâ€™ conditions, RL algorithms for a finite number of states can be utilized. When the actions corre- spond to two medications at different doses, it was found that RL policies reach or slightly surpass the performance of human doctors [15, 16, 17].
1.2.7 Optimal Control and Robotics
Reinforcement learning can be viewed as optimal control of stochastic environ- ments or systems. It does not matter whether there is a model of the environ- ment or not. If there is model, it is called a model-based control problem, also often called a planning problem; if there is no model, it is called a model-free control problem, also often called a learning problem.
There is a vast number of applications of RL in automation, optimal control, and robotics. Examples for the optimal control of industrial systems are chemical and biological reactors. The output of the product is to be maximized, while the reactor must be kept within safe operating conditions. In robotics, the robot interacts with its environment in order to solve the task it has been assigned by specifying the rewards and/or penalties. Ideally, optimal policies are then learned without any further help or interaction. Generally speaking, RL can used to automate any kind of equipment or machinery, a few examples being
1.3. Bibliographical Remarks
[18, 19].
1.2.8 Recommendation Systems
Although recommendation problems have traditionally often been considered to be classification or prediction problems, they are nowadays often formulated as sequential decision problems, which can better reflect the multiple interactions between users and the recommendation system. RL is used to directly optimize for user satisfaction or other metrics in various areas such as music and video playlists [20, 21, 22]. It is known that traffic is routed through RL systems on many large websites.
1.2.9 Supply Chains and Inventories
In supply-chain and inventory management, goods must be moved efficiently such that they arrive at their points of destination in sufficient, but not abun- dant quantity. Rewards are obtained whenever the goods arrive on time in the specified quantity at their points of destination, while penalties are obtained when too few or too many goods arrive or when they are delayed.
1.3 Bibliographical Remarks
Here textbooks on RL are mentioned. The most important and influential in- troductory textbook is [23]. RL is closely related to optimal control and often builds on dynamic programming; textbooks at the intersections of these subjects are [24, 25, 26]. [27]. A textbook on distributional RL is [28]. A comprehensive collection of approaches to dynamic programming, RL, and stochastic optimiza- tion can be found in [29]. Practical approaches to deep RL are discussed in [30, 31, 32, 33].
7
8
Chapter 1.
Introduction
Chapter 2
Markov Decision Processes and Dynamic Programming
I was intrigued by dynamic programming. It was clear to me that there was a good deal of good analysis there. Furthermore, I could I could either be a see many applications. traditional intellectual, or a modern intellectual using the results of my research for the problems of contemporary society. This was a dangerous path. Either I could do too much research and too little application, or too little research and too much application. I had confidence that I could do this delicate activity, pie Ã  la mode.
It was a clear choice.
Richard Bellman [34, p. 173].
This chapter starts with one of the simplest learning problems, the so-called multi-armed bandits. Multi-armed bandits serve to introduce basic notions and challenges in time-dependent learning. Then Markov decision processes are defined, as they serve as the foundation for the description of reinforcement- learning problems. Once the basic language to describe reinforcement-learning problems has been defined, a collection of standard environments is presented. Based on the definitions, the most important notions in and results of dynamic programming are presented, since they serve as a foundation that inspires learn- ing algorithms.
2.1 Multi-Armed Bandits
A relatively simple, but illustrative example of a reinforcement-learning problem are multi-armed bandits. The name of the problem stems from slot machines.
9
10
Chapter 2. Markov Decision Processes and Dynamic Programming
There are ğ‘˜ = |ğ’œ| slot machines or bandits, and the action is to choose one machine and play it to receive a reward. The reward each slot machine or bandit distributes is taken from a stationary probability distribution, which is of course unknown to the agent and different for each machine.
In more abstract terms, the problem is to learn the best policy when being repeatedly faced with a choice among ğ‘˜ different actions. After each action, the reward is chosen from the stationary probability distribution associated with each action. The objective of the agent is to maximize the expected total reward over some time period or for all times.
In time step ğ‘¡, the action is denoted by ğ´ğ‘¡ âˆˆ ğ’œ and the reward by ğ‘…ğ‘¡. In
this example, we define the (true) value of an action ğ‘ to be
ğ‘âˆ—âˆ¶ ğ’œ â†’ â„,
ğ‘âˆ—(ğ‘) âˆ¶= ğ”¼[ğ‘…ğ‘¡ âˆ£ ğ´ğ‘¡ = ğ‘],
ğ‘ âˆˆ ğ’œ.
(This definition is simpler than the general one, since the time steps are inde- pendent from one another. There are no states.) This function is called the action-value function.
Since the true value of the actions is unknown (at least in the beginning), we calculate an approximation called ğ‘„ğ‘¡âˆ¶ ğ’œ â†’ â„ in each time step; it should be as close to ğ‘âˆ— as possible. A reasonable approximation is the sample mean of the observed rewards, i.e.,
ğ‘„ğ‘¡(ğ‘) âˆ¶=
sum of rewards obtained when action ğ‘ was taken prior to ğ‘¡ number of times action ğ‘ was taken prior to ğ‘¡
Based on this approximation, the greedy way to select an action is to choose
ğ´ğ‘¡ âˆ¶âˆˆ argmax
ğ‘
ğ‘„ğ‘¡(ğ‘),
which serves as a substitute for the ideal choice
argmax ğ‘
ğ‘âˆ—(ğ‘).
Note that argmaxğ‘ ğ‘„ğ‘¡(ğ‘) is a set, since multiple actions may maximize the argument. Hence the notation â€œğ‘¥ âˆ¶âˆˆ ğ‘‹â€ means that an element of ğ‘‹ is chosen randomly with each element having the same probability and assigned to ğ‘¥, analogously to the notation â€œâˆ¶=â€.
In summary, these simple considerations have led us to a first example of an action-value method. In general, an action-value method is a method which is based on (an approximation ğ‘„ğ‘¡ of) the action-value function ğ‘âˆ—.
Choosing the actions in a greedy manner is called exploitation. However, there is a problem. In each time step, we only have the approximation ğ‘„ğ‘¡ at
.
2.1. Multi-Armed Bandits
our disposal. For some of the ğ‘˜ bandits or actions, it may be a bad approxima- tion, in the sense that it leads us to choose an action ğ‘ whose estimated value ğ‘„ğ‘¡(ğ‘) is higher than its true value ğ‘âˆ—(ğ‘). Such an approximation error would be misleading and reduce or rewards.
In other words, exploitation by greedy actions is not enough. We also have to ensure that our approximations are sufficiently accurate; this process is called exploration. If we explore all actions sufficiently well, bad actions cannot hide behind high rewards obtained by chance.
The duality between exploitation and exploration is fundamental to rein- forcement learning, and it is worthwhile to always keep these two concepts in mind. Here we saw how these two concepts are linked to the quality of the approximation of the action-value function ğ‘âˆ—.
In general, a greedy policy always exploits the current knowledge (in the form of an approximation of the action-value function) in order to maximize the immediate reward, but it spends no time on the long-term picture. A greedy policy does not sample apparently worse actions to see whether their true action values are better or whether they lead to more desirable states. (Note that the multi-bandit problem is stateless.)
A common and simple way to combine exploitation and exploration into one policy in the case of finite action sets is to choose the greedy action most of the time, but any action with a (usually small) probability ğœ–. This is the subject of the following definition.
Definition 2.1 (ğœ–-greedy policy). Suppose that the action set ğ’œ is finite, that ğ‘„ğ‘¡ is an approximation of the action-value function, and that ğœ–ğ‘¡ âˆˆ [0,1]. Then the policy defined by
ğ´ğ‘¡ âˆ¶= {
argmaxğ‘âˆˆğ’œ ğ‘„ğ‘¡(ğ‘) with probability 1 âˆ’ ğœ–ğ‘¡ breaking ties randomly, a random action ğ‘ with probability ğœ–ğ‘¡
is called the ğœ–-greedy policy.
In the first case, it is important to break ties randomly, because otherwise a bias towards certain actions would be introduced. The random action in the second case is usually chosen uniformly from all actions ğ’œ.
Learning methods that use ğœ–-greedy policies are called ğœ–-greedy methods. Intuitively speaking, every action will be sampled an infinite number of times as ğ‘¡ â†’ âˆ, which ensures convergence of ğ‘„ğ‘¡ to ğ‘âˆ—.
Regarding the numerical implementation, it is clear that storing all previous actions and rewards becomes inefficient as the number of time steps increases. Can memory and the computational effort per time step be kept constant, which
11
12
Chapter 2. Markov Decision Processes and Dynamic Programming
would be the ideal case? Yes, it is possible to achieve this ideal case using a trick, which will lead to our first learning algorithm.
To simplify notation, we focus on the action-value function of a certain ac- tion. We denote the reward received after the ğ‘˜-th selection of this specific action by ğ‘…ğ‘˜ and use the approximation
ğ‘„ğ‘› âˆ¶=
1 ğ‘› âˆ’ 1
ğ‘›âˆ’1 âˆ‘ ğ‘˜=1
ğ‘…ğ‘˜
of its action value after the action has been chosen ğ‘› âˆ’ 1 times. This is called the sample-average method. The trick is to find a recursive formula for ğ‘„ğ‘› by calculating
ğ‘„ğ‘›+1 =
1 ğ‘›
ğ‘› âˆ‘ ğ‘˜=1
ğ‘…ğ‘˜
=
1 ğ‘›
(ğ‘…ğ‘› + (ğ‘› âˆ’ 1)
1 ğ‘› âˆ’ 1
ğ‘›âˆ’1 âˆ‘ ğ‘˜=1
ğ‘…ğ‘˜)
=
1 ğ‘›
(ğ‘…ğ‘› + (ğ‘› âˆ’ 1)ğ‘„ğ‘›)
= ğ‘„ğ‘› +
1 ğ‘›
(ğ‘…ğ‘› âˆ’ ğ‘„ğ‘›)
âˆ€ğ‘› â‰¥ 1.
(If ğ‘› = 1, this formula holds for arbitrary values of ğ‘„1 so that the starting point ğ‘„1 does not play a role.)
This yields our first learning algorithm, Algorithm 1. The implementation of this recursion requires only constant memory for ğ‘› and ğ‘„ğ‘› and a constant amount of computation in each time step. The recursion above has the form
new estimate âˆ¶= old estimate + learning rate â‹… (target âˆ’ old estimate),
which is a common theme in reinforcement learning. Its intuitive meaning is that the estimate is updated towards a target value. Since the environment is stochastic, the learning rate only moves the estimate towards the observed target value. Updates of this form will occur many times in this book.
In the most general case, the learning rate ğ›¼ depends on the time step and the action taken, i.e., ğ›¼ = ğ›¼ğ‘¡(ğ‘). In the sample-average method above, the learning rate is ğ›¼ğ‘›(ğ‘) = 1/ğ‘›. It can be shown that the sample-average approximation ğ‘„ğ‘› above converges to the true action-value function ğ‘âˆ— by using the law of large numbers.
(2.1)
2.1. Multi-Armed Bandits
Algorithm 1 a simple algorithm for the multi-bandit problem.
Initialization: choose ğœ– âˆˆ (0,1), initialize two vectors q and n of length |ğ’œ| with zeros.
loop
select an action ğ‘ ğœ–-greedily using q (see Definition 2.1) perform the action ğ‘ and receive the reward ğ‘Ÿ from the environment ğ‘›ğ‘ âˆ¶= ğ‘›ğ‘ + 1 ğ‘ğ‘ âˆ¶= ğ‘ğ‘ + 1 ğ‘›ğ‘
(ğ‘Ÿ âˆ’ ğ‘ğ‘)
end loop
return q
Sufficient conditions that yield convergence with probability one are
âˆ âˆ‘ ğ‘˜=1 âˆ âˆ‘ ğ‘˜=1
ğ›¼ğ‘˜(ğ‘) = âˆ,
ğ›¼ğ‘˜(ğ‘)2 < âˆ.
They are well-known in stochastic-approximation theory and are a recurring theme in convergence proofs. The first condition ensures that the steps are sufficiently large to eventually overcome any initial conditions or random fluc- tuations. The second condition means the steps eventually become sufficiently small. Of course, these two conditions are satisfied for the learning rate
ğ›¼ğ‘˜(ğ‘) âˆ¶=
1 ğ‘˜
,
but they are not satisfied for a constant learning rate ğ›¼ğ‘˜(ğ‘) âˆ¶= ğ›¼. How- ever, a constant learning rate may be desirable when the environment is time- dependent, since then the updates continue to adjust the policy to changes in the environment.
Finally, we shortly discuss an important improvement over ğœ–-greedy policies, namely action selection by upper confidence bounds. The disadvantage of an ğœ–-greedy policy is that it chooses the non-greedy actions without any further consideration. It is however better to select the non-greedy actions according to their potential to actually being an optimal action and according to the
13
(2.2a)
(2.2b)
14
Chapter 2. Markov Decision Processes and Dynamic Programming
uncertainty in our estimate of the value function. This can be done using the so-called upper-confidence-bound action selection
ğ´ğ‘¡ âˆ¶âˆˆ argmax
ğ‘âˆˆğ’œ
(ğ‘„ğ‘¡(ğ‘) + ğ‘âˆš
lnğ‘¡ ğ‘ğ‘¡(ğ‘)
),
where ğ‘ğ‘¡(ğ‘) is the number of times that action ğ‘ has been selected before time ğ‘¡. If ğ‘ğ‘¡(ğ‘) = 0, the action ğ‘ is treated as an optimal action. The constant ğ‘ controls the amount of exploration.
The term âˆš(lnğ‘¡)/ğ‘ğ‘¡(ğ‘) measures the uncertainty in the estimate ğ‘„ğ‘¡(ğ‘). When the action ğ‘ is selected, ğ‘ğ‘¡(ğ‘) increases and the uncertainty decreases. On the other hand, if an action other than ğ‘ is chosen, ğ‘¡ increases and the uncertainty relative to other actions increases. Therefore, since ğ‘„ğ‘¡(ğ‘) is the approximation of the value and ğ‘âˆš(lnğ‘¡)/ğ‘ğ‘¡(ğ‘) is the uncertainty, where ğ‘ is the confidence level, the sum of these two terms acts as an upper bound for the true value ğ‘âˆ—(ğ‘).
Since the logarithm is unbounded, all actions are ensured to be chosen even- tually. Actions with lower value estimates ğ‘„ğ‘¡(ğ‘) and actions that have often been chosen (large ğ‘ğ‘¡(ğ‘) and low uncertainty) are selected with lower frequency, just as they should in order to balance exploitation and exploration.
2.2 Markov Decision Processes
The mathematical language and notation for describing and solving reinforcement- learning problems is deeply rooted in Markov decision processes. Having dis- cussed multi-armed bandits as a concrete example for a reinforcement-learning problem, we now generalize some notions and fix the notation for the rest of the book using the language of Markov decision processes.
As already discussed in Chapter 1, the whole world is divided into an agent and an environment. The agent interacts with the environment iteratively. The agent takes actions in the environment, which changes the state of the environ- ment and for which it receives a reward (see Figure 1.1). The task of the agent is to learn optimal policies, i.e., to find the best action in order to maximize all future rewards it will receive. We will now formalize the problem of finding optimal policies.
We note the sequence of (usually discrete) time steps by ğ‘¡ âˆˆ {0,1,2,â€¦}. The state (or observation) that the agent receives in time step ğ‘¡ from the environment is denoted by ğ‘†ğ‘¡ âˆˆ ğ’®, where ğ’® is the set of all states. We use capital letters to denote random variables. In general, ğ’® âŠ‚ â„ğ‘‘ğ‘ , ğ‘‘ğ‘  âˆˆ â„•, but if there is a finite number of states, ğ’® âŠ‚ â„¤ âŠ‚ â„ suffices.
2.2. Markov Decision Processes
The action performed by the agent in time step ğ‘¡ is denoted by ğ´ğ‘¡ âˆˆ ğ’œ(ğ‘ ) if the environment is in state ğ‘  = ğ‘†ğ‘¡. In general, ğ’œ âŠ‚ â„ğ‘‘ğ‘, ğ‘‘ğ‘ âˆˆ â„•, but if there is a finite number of actions, ğ’œ(ğ‘ ) âŠ‚ â„¤ âŠ‚ â„ suffices. In general, the set ğ’œ(ğ‘ ) of all available actions depends on the very state ğ‘ , although this dependence is sometimes dropped to simplify notation. In the subsequent time step, the agent receives the reward ğ‘…ğ‘¡+1 âˆˆ â„› âŠ‚ â„ and finds itself in the next state ğ‘†ğ‘¡+1. Then the iteration is repeated.
The whole information about these interactions between the agent and the environment can be recorded in sequences âŸ¨ğ‘†ğ‘¡âŸ©ğ‘¡âˆˆâ„•, âŸ¨ğ´ğ‘¡âŸ©ğ‘¡âˆˆâ„•, and âŸ¨ğ‘…ğ‘¡âŸ©ğ‘¡âˆˆâ„• or in the sequence
âŸ¨ğ‘†0,ğ´0,ğ‘…1,ğ‘†1,ğ´1,ğ‘…2,ğ‘†2,ğ´2,ğ‘…3,ğ‘†3,ğ´3,â€¦âŸ©
These (finite or infinite) sequences are called episodes.
The random variables ğ‘†ğ‘¡ and ğ‘…ğ‘¡ provided by the environment depend only on the preceding state and action. This is the Markov property, and the whole process is a Markov decision process (MDP). We assume that the Markov prop- erty is always satisfied.
In a finite MDP, all three sets ğ’®, ğ’œ, and â„› are finite, and hence the random
variables ğ‘†ğ‘¡, ğ´ğ‘¡, and ğ‘…ğ‘¡ are discrete.
The purpose of the following definitions is to describe the dynamics of the environment, i.e., the random variables ğ‘†ğ‘¡+1 and ğ‘…ğ‘¡+1 knowing ğ‘†ğ‘¡ = ğ‘  and ğ´ğ‘¡ = ğ‘, starting from the general case of states and rewards that are real vectors and ending with finite MDP.
The cumulative distribution function ğ¹ğ‘‹ of a random variable ğ‘‹ is defined
as
ğ¹ğ‘‹(ğ‘¥) âˆ¶= â„™[ğ‘‹ â‰¤ ğ‘¥]
in the univariate case and as
ğ¹ğ‘‹1,â€¦,ğ‘‹ğ‘‘
(ğ‘¥1,â€¦,ğ‘¥ğ‘‘) âˆ¶= â„™[ğ‘‹1 â‰¤ ğ‘¥1 âˆ§ â€¦ âˆ§ ğ‘‹ğ‘‘ â‰¤ ğ‘¥ğ‘‘]
in the multivariate case, where ğ‘‘ âˆˆ â„•. For vectors x âˆˆ â„ğ‘‘ and y âˆˆ â„ğ‘‘, we define
x â‰¤ y
âˆ¶âŸº ğ‘¥ğ‘˜ â‰¤ ğ‘¦ğ‘˜ âˆ€ğ‘˜ âˆˆ {1,â€¦,ğ‘‘}.
Then we can write
ğ¹ğ‘‹(x) = â„™[ğ‘‹ â‰¤ x].
In general, the expectation of a function â„ of a random variable ğ‘‹ is defined as
âˆ
âˆ
ğ”¼[â„(ğ‘¥)] âˆ¶= âˆ«
â„(x)dğ¹ğ‘‹(x) = âˆ«
â‹¯âˆ«
â„(x)dğ¹ğ‘‹ğ‘‘
(ğ‘¥ğ‘‘)â‹¯dğ¹ğ‘‹1
xâˆˆâ„ğ‘‘
ğ‘¥1=âˆ’âˆ
ğ‘¥ğ‘‘=âˆ’âˆ
(ğ‘¥1).
15
16
Chapter 2. Markov Decision Processes and Dynamic Programming
The probability of being put into state ğ‘ â€² âˆˆ ğ’® and receiving a reward ğ‘Ÿ âˆˆ â„› after starting from a state ğ‘  âˆˆ ğ’® and performing action ğ‘ âˆˆ ğ’œ(ğ‘ ) is recorded by the transition probability
ğ‘âˆ¶ ğ’® Ã— â„› Ã— ğ’® Ã— ğ’œ â†’ [0,1], ğ‘(ğ‘ â€²,ğ‘Ÿ âˆ£ ğ‘ ,ğ‘) âˆ¶= â„™[ğ‘†ğ‘¡ = ğ‘ â€², ğ‘…ğ‘¡ = ğ‘Ÿ âˆ£ ğ‘†ğ‘¡âˆ’1 = ğ‘ , ğ´ğ‘¡âˆ’1 = ğ‘].
Despite the notation with the vertical bar in the argument list of ğ‘, which is reminiscent of a conditional probability, the function ğ‘ is a deterministic function of four arguments.
The function ğ‘ records the dynamics of the MDP, and it is therefore also called the dynamics function of the MDP. Since it is a probability distribution, the equality
âˆ€ğ‘  âˆˆ ğ’®âˆ¶
âˆ€ğ‘ âˆˆ ğ’œ(ğ‘ )âˆ¶ âˆ« ğ’®
âˆ« â„›
ğ‘(ğ‘ â€²,ğ‘Ÿ âˆ£ ğ‘ ,ğ‘)dğ‘ â€²dğ‘Ÿ = 1
holds.
If there is a finite number of states and rewards, the integrals become sums
and the equalities are
âˆ€ğ‘  âˆˆ ğ’®âˆ¶
âˆ€ğ‘ âˆˆ ğ’œ(ğ‘ )âˆ¶ âˆ‘ ğ‘ â€²âˆˆğ’®
âˆ‘ ğ‘Ÿâˆˆâ„›
ğ‘(ğ‘ â€²,ğ‘Ÿ âˆ£ ğ‘ ,ğ‘) = 1.
The requirement that the Markov property holds is met by ensuring that the information recorded in the states ğ‘  âˆˆ ğ’® is sufficient. This is an important point when translating an informal problem description into the framework of MDPs and reinforcement learning. In practice, this often means that the states become sufficiently long vectors that contain enough information about the past ensuring that the Markov property holds. This in turn has the disadvantage that the dimension of the state space may have to increase to ensure the Markov property.
It is important to note that the transition probability ğ‘, i.e., the dynamics of the MDP, is unknown. It is sometimes said that the term learning refers to problems where information about the dynamics of the system is absent. Learning algorithms face the task of calculating optimal strategies with only very little knowledge about the environment, i.e., just the sets ğ’® and ğ’œ(ğ‘ ).
The dynamics function contains all relevant information about the MDP, and therefore other quantities can be derived from it. The first quantity are the state-transition probabilities
ğ‘âˆ¶ ğ’® Ã— ğ’® Ã— ğ’œ â†’ [0,1],
2.3. Rewards, Returns, and Episodes
ğ‘(ğ‘ â€² âˆ£ ğ‘ ,ğ‘) âˆ¶= â„™{ğ‘†ğ‘¡ = ğ‘ â€² âˆ£ ğ‘†ğ‘¡âˆ’1 = ğ‘ , ğ´ğ‘¡âˆ’1 = ğ‘} = âˆ‘ ğ‘Ÿâˆˆâ„›
ğ‘(ğ‘ â€²,ğ‘Ÿ âˆ£ ğ‘ ,ğ‘),
also denoted by ğ‘, but taking only three arguments.
Next, the expected rewards for state-action pairs are
ğ‘Ÿâˆ¶ ğ’® Ã— ğ’œ â†’ â„,
ğ‘Ÿ(ğ‘ ,ğ‘) âˆ¶= ğ”¼[ğ‘…ğ‘¡ âˆ£ ğ‘†ğ‘¡âˆ’1 = ğ‘ , ğ´ğ‘¡âˆ’1 = ğ‘] = âˆ‘ ğ‘Ÿâˆˆâ„›
ğ‘Ÿâˆ‘ ğ‘ â€²âˆˆğ’®
ğ‘(ğ‘ â€²,ğ‘Ÿ âˆ£ ğ‘ ,ğ‘).
(Note that âˆ‘ğ‘Ÿâˆˆâ„› âˆ‘ğ‘ â€²âˆˆğ’® ğ‘(ğ‘ â€²,ğ‘Ÿ âˆ£ ğ‘ ,ğ‘) = 1 must hold.) Furthermore, the expected rewards for stateâ€“actionâ€“next-state triples are given by
ğ‘Ÿâˆ¶ ğ’® Ã— ğ’œ Ã— ğ’® â†’ â„,
ğ‘Ÿ(ğ‘ ,ğ‘,ğ‘ â€²) âˆ¶= ğ”¼[ğ‘…ğ‘¡ âˆ£ ğ‘†ğ‘¡âˆ’1 = ğ‘ , ğ´ğ‘¡âˆ’1 = ğ‘, ğ‘†ğ‘¡ = ğ‘ â€²] = âˆ‘ ğ‘Ÿâˆˆâ„›
ğ‘Ÿ
ğ‘(ğ‘ â€²,ğ‘Ÿ âˆ£ ğ‘ ,ğ‘) ğ‘(ğ‘ â€² âˆ£ ğ‘ ,ğ‘)
.
(Note that âˆ‘ğ‘Ÿâˆˆâ„› ğ‘(ğ‘ â€²,ğ‘Ÿ âˆ£ ğ‘ ,ğ‘)/ğ‘(ğ‘ â€² âˆ£ ğ‘ ,ğ‘) = 1 must hold.)
MDPs can be visualized as directed graphs. The nodes are the states, and the edges starting from a state ğ‘  correspond to the actions ğ’œ(ğ‘ ). The edges starting from state ğ‘  may split and end in multiple target nodes ğ‘ â€². The edges are labeled with the state-transition probabilities ğ‘(ğ‘ â€² âˆ£ ğ‘ ,ğ‘) and the expected reward ğ‘Ÿ(ğ‘ ,ğ‘,ğ‘ â€²).
Further important data structures are the transition and the experience. A
transition is a tuple
(ğ‘ ,ğ‘,ğ‘ â€²,ğ‘Ÿ,done?),
which records all information in a time step. Therefore an episode can be viewed as a sequence of transitions. The binary variable done? records whether a termi- nal state has been reached and the episode has ended; this information is useful in certain learning algorithms. An experience (also often called an experience- replay buffer) is a set of transitions.
2.3 Rewards, Returns, and Episodes
We start with a remark on how rewards should be defined in practice when translating an informal problem description into a precisely defined environment. It is important to realize that the learning algorithms will learn to maximize the expected value of the discounted sum of all future rewards (defined below), nothing more and nothing less.
For example, if the agent should learn to escape a maze quickly, it is expedient to set ğ‘…ğ‘¡ âˆ¶= âˆ’1 for all times ğ‘¡. This ensures that the task is completed quickly.
17
(2.3a)
(2.3b)
18
Chapter 2. Markov Decision Processes and Dynamic Programming
The obvious alternative to define ğ‘…ğ‘¡ âˆ¶= 0 before escaping the maze and a positive reward when escaping fails to convey to the agent that the maze should be escaped quickly; there is no penalty for lingering in the maze.
Furthermore, the temptation to give out rewards for solving subproblems must be resisted. For example, when the goal is to play chess, there should be no rewards to taking opponentsâ€™ pieces. Because otherwise the agent would become proficient in taking opponentsâ€™ pieces, but not in checkmating the king. From now on, we make the following assumption, which is needed for defining
the return in the general case, which is the next concept we discuss.
Assumption 2.2 (bounded rewards). The reward sequence âŸ¨ğ‘…ğ‘¡âŸ©ğ‘¡âˆˆâ„• is bounded.
There are two cases to be discerned, namely whether the episodes are finite or infinite. We denote the time of termination, i.e., the time when the terminal state of an episode is reached, by ğ‘‡. The case of a finite episode is called episodic, and ğ‘‡ < âˆ holds; the case of an infinite episode is called continuing, and ğ‘‡ = âˆ holds.
Definition 2.3 (discounted return). The discounted return is
ğºğ‘¡ âˆ¶=
ğ‘‡ âˆ‘ ğ‘˜=ğ‘¡+1
ğ›¾ğ‘˜âˆ’(ğ‘¡+1)ğ‘…ğ‘˜ = ğ‘…ğ‘¡+1 + ğ›¾ğ‘…ğ‘¡+2 + â‹¯,
where ğ‘‡ âˆˆ â„• âˆª {âˆ} is the terminal state of the episode and ğ›¾ âˆˆ [0,1] is the discount rate.
From now on, we also make the following assumption.
Assumption 2.4 (finite returns). ğ‘‡ = âˆ and ğ›¾ = 1 do not hold at the same time.
Assumptions 2.2 and 2.4 ensure that all returns are finite. There is an important recursive formula for calculating the returns from the
rewards of an episode. It is found by calculating
ğºğ‘¡ =
ğ‘‡ âˆ‘ ğ‘˜=ğ‘¡+1
ğ›¾ğ‘˜âˆ’(ğ‘¡+1)ğ‘…ğ‘˜
ğ‘‡ âˆ‘ ğ‘˜=ğ‘¡+2 = ğ‘…ğ‘¡+1 + ğ›¾ğºğ‘¡+1.
= ğ‘…ğ‘¡+1 + ğ›¾
ğ›¾ğ‘˜âˆ’(ğ‘¡+2)ğ‘…ğ‘˜
(2.4a)
(2.4b)
(2.4c)
2.4. Standard Environments
The calculation also works when ğ‘‡ < âˆ, since ğºğ‘‡ = 0, ğºğ‘‡+1 = 0, and so forth since then the sum in the definition of ğºğ‘¡ is empty and hence equal to zero. This formula is very useful to quickly compute returns from reward sequences.
At this point, we can formalize what (classical) problems in reinforcement
learning are.
Definition 2.5 (reinforcement-learning problem). Given the states ğ’®, the ac- tions ğ’œ(ğ‘ ), and the opaque transition function ğ’® Ã— ğ’œ â†’ ğ’® Ã— â„› of the environ- ment, a reinforcement-learning problem consists of finding policies for selecting the actions of an agent such that the expected discounted return is maximized.
The random transition provided by the MDP of the environment, namely going from a state-action pair to a state-reward pair, being opaque means that we consider it a black box. For any state-action pair as input, it is only required to yield a state-reward pair as output. In particular, the transition probabilities are considered to be unknown. Examples of such opaque environments are
functions ğ’® Ã— ğ’œ â†’ ğ’® Ã— â„› defined in a programming language,
more complex pieces of software such as computer games or simulation software,
historic data from which states, actions, and rewards can be observed.
The fact that the problem class in Definition 2.5 is so large adds to the appeal
of reinforcement learning.
2.4 Standard Environments
At this point, we have defined the basic concepts of RL that are necessary to translate applications into the language of RL. In this section, various standard environments are defined. They serve several purposes. First, they serve as leading examples of RL problems in different application areas. Several envi- ronments in this collection have a long history in RL. Second, most of these environments have well-known solutions, and thus they easily serve as test cases to verify implementations of algorithms. Third, these environments embody a wide variety of problems useful for the evaluation of algorithms. The challenge in developing learning algorithms is that they should be general; if a learning algorithm performs well on a wide variety of problems, a useful algorithm has been found.
19
20
Chapter 2. Markov Decision Processes and Dynamic Programming
The state and action spaces can be discrete or continuous. This is indicated in the titles below in this order; for example, â€œcontinuous/discreteâ€ indicates a continuous state space and a discrete action space.
A popular type of environment is the grid world. A grid world is an en- vironment that typically consists of a (two-dimensional) grid, i.e., the states, on which the agent moves. There are usually four actions: up, down, left, and right; sometimes there are eight, including the diagonal directions. If the agent is near the edge of the grid and the action would make it leave the grid, the state remains unchanged; in other words, the agents cannot leave the grid. Often the grid world is a maze, i.e., there is a start state and a goal or a terminal state. In the case of a maze, the goal state must be reached as quickly as possible. Therefore there is a reward of âˆ’1 in each step until the goal state is reached, and the learning task is episodic and undiscounted (ğ›¾ = 1). At first glance, it would also make sense to give out zero rewards during the episode and a posi- tive one when the goal is reached; however, such rewards provide no incentive to reach the goal state as quickly as possible.
Usually there are complications built into the environment, or additional tasks defined via rewards must be performed. In this manner, grid worlds be- come a versatile class of environments and are well-suited to elucidate certain behaviors of algorithms.
2.4.1 Simple Grid World (Discrete/Discrete)
Implement a 4 Ã— 4 grid world with two terminal states in the upper left and lower right corners, resulting in 14 non-terminal states (see [23, Example 4.1]. The four actions ğ’œ = {up,down,left,right} act deterministically, the discount factor is ğ›¾ = 1, and the reward is always equal to âˆ’1. Ensure that a maximum number of time steps can be specified.
2.4.2 Windy Grid World (Discrete/Discrete)
Windy Grid World is [23, Example 6.5]. The underlying grid has size 10 Ã— 7, see Figure 2.1. The start is in the middle of the first column, and the goal is in the middle of the eighth column. The four actions are up, down, left, and right, and the learning task is an undiscounted (ğ›¾ = 1) episodic task. As is usual in mazes, there is a reward of âˆ’1 in each time step until the goal or terminal state is reached.
This grid world is called windy because of its complication of a wind that sometimes displaces the agent (deterministically). In the middle region of the grid, the next states are shifted upward by the wind by one or two cells as
2.4. Standard Environments
S
â†‘ â†‘ â†‘ â†‘ â†‘ â†‘ â†‘ 1
â†‘ â†‘ â†‘ â†‘ â†‘ â†‘ â†‘ 1
â†‘ â†‘ â†‘ â†‘ â†‘ â†‘ â†‘ 1
â†‘ â†‘â†‘ â†‘â†‘ â†‘ â†‘â†‘ â†‘â†‘ â†‘ â†‘â†‘ â†‘â†‘ â†‘â†‘ G â†‘ â†‘ â†‘â†‘ â†‘â†‘ â†‘ â†‘â†‘ â†‘â†‘ â†‘ â†‘â†‘ â†‘â†‘ 1 2 2
0
0
0
0
Figure 2.1: Windy Grid World. The colored cells indicate an optimal policy, resulting in an episode of length 15.
200400600800100012001400160018002000220024002600280030003200340036003800400042004400460048005000
mean return (Â± stdev.)
Learning Progress
âˆ’120âˆ’110âˆ’100âˆ’90âˆ’80âˆ’70âˆ’60âˆ’50âˆ’40âˆ’30âˆ’20âˆ’10010
batch_num
Figure 2.2: Learning progress of ğ‘„-learning for Windy Grid World.
indicated in the bottom row in the figure. The number below each column indicates the number of cells shifted upward. For example, if the agent is located to the right of the goal and the action is going left, then the agent is taken to the cell just above the goal. Note that the agent never leaves the grid world (as usual in grid worlds), even if it is displaced by the wind.
Learning progress is shown in Figure 2.2.
2.4.3 Cliff Walking (Discrete/Discrete)
Cliff Walking is another grid world [23, Example 6.6]. The underlying grid has size 12 Ã— 4, see Figure 2.3. The start is in the bottom left, and the goal is in
21
22
Chapter 2. Markov Decision Processes and Dynamic Programming
S
G
Figure 2.3: Cliff Walking. The cliff is located at the bottom between the start and goal states. The colored cells indicate an optimal policy, resulting in an episode of length 13.
10020030040050060070080090010001100120013001400150016001700180019002000
âˆ’350âˆ’300âˆ’250âˆ’200âˆ’150âˆ’100âˆ’50050
batch_num
Learning Progress
mean return (Â± stdev.)
Figure 2.4: Learning progress of ğ‘„-learning for Cliff Walking.
the bottom right. Between the start and goal there is a cliff, representing the complication of this environment. The four actions are up, down, left, and right, and the learning task is an undiscounted (ğ›¾ = 1) episodic task. The reward is âˆ’1 as usual in mazes, but falling down the cliff incurs a reward of âˆ’100 and instantly teleports the agent back to the start state.
Learning progress is shown in Figure 2.4.
2.4.4 Frozen Lake (Discrete/Discrete)
Frozen Lake1 is a stochastic grid world. There are two versions regarding the size of the grid, namely 4 Ã— 4 (see Figure 2.5) and 8 Ã— 8 (see Figure 2.6). The start state is in the top left, and the goal state (a terminal state) is in the bottom right. The four actions are up, down, left, and right, and the learning task is an
1https://www.gymlibrary.dev/environments/toy_text/frozen_lake/
2.4. Standard Environments
S
G
Figure 2.5: Frozen Lake 4 Ã— 4. The colored cells are holes in the ice.
S
G
Figure 2.6: Frozen Lake 8 Ã— 8. The colored cells are holes in the ice.
undiscounted (ğ›¾ = 1) episodic task. There is a reward of +1 when reaching the goal, otherwise the rewards are zero. The complications are the holes in the ice, which are terminal states.
Another complication, which can be used optionally, is slippery ice. This is a complication that introduces lots of randomness. In the slippery version, the intended direction is followed with probability 1/3 only. Otherwise, the agent will move in either direction perpendicular to the intended direction with equal probability of 1/3 in both directions. For example, if the action is up, then the actions up, left, and right all have probability 1/3.
Learning progresses are shown in Figure 2.7 and Figure 2.8.
2.4.5 Rock Paper Scissors (Discrete/Discrete)
Rock paper scissors is a well-known simultaneous, zero-sum game for two players. In the first variant considered here, the opponent uses a fixed, stochastic policy. Each action (rock, paper, or scissors) is chosen with equal probability 1/3. As usual, the task of the agent is to learn an optimal policy.
In the second variant, the opponent uses a stochastic policy that varies with
time. At time ğ‘¡ âˆˆ â„•, the probabilities are
â„™[rock] = sin2 ğ›¼ğ‘¡cos2 ğ›½ğ‘¡, â„™[paper] = sin2 ğ›¼ğ‘¡sin2 ğ›½ğ‘¡,
23
24
Chapter 2. Markov Decision Processes and Dynamic Programming
batch_num
âˆ’0.10.00.10.20.30.40.50.60.70.80.91.0
100020003000400050006000700080009000100001100012000130001400015000160001700018000190002000021000220002300024000250002600027000280002900030000
Learning Progress
mean return (Â± stdev.)
Figure 2.7: Learning progress of ğ‘„-learning for Frozen Lake (size 4 Ã— 4, slip- pery).
mean return (Â± stdev.)
âˆ’0.2âˆ’0.10.00.10.20.30.40.50.60.70.80.91.01.11.2
250050007500100001250015000175002000022500250002750030000325003500037500400004250045000475005000052500550005750060000625006500067500700007250075000775008000082500850008750090000925009500097500100000
batch_num
Learning Progress
Figure 2.8: Learning progress of ğ‘„-learning for Frozen Lake (size 8 Ã— 8, slip- pery).
â„™[scissors] = cos2 ğ›¼ğ‘¡,
where ğ›¼ âˆ¶= 2ğœ‹/100 and ğ›½ âˆ¶= 2ğ›¼. Again, the task of the agent is to learn an optimal policy.
2.4. Standard Environments
2.4.6 Prisonerâ€™s Dilemma (Discrete/Discrete)
2.4.7 Blackjack (Discrete/Discrete)
Blackjack is the most widely played casino banking game. It is a descendent of the family of casino banking games known as â€œtwenty-one,â€ whose progenitor is recorded in Spain in the early 17th century. In the context of card games, a banking game is a gambling card game in which one or more players, the so-called punters, play against a banker or dealer who controls the game.
The objective of Blackjack is to draw cards such that the sum of their values is as great as possible without exceeding 21. An ace has a value of 1 or 11, whichever is more expedient to the player. All face cards (King, Queen, and [23, Example 5.1] is the basic version of the game Jack) have a value of 10. in which each player plays independently against the dealer and has only two choices, namely to â€œhitâ€ or to â€œstick.â€ In other versions, the player can also â€œdouble down,â€ â€œsplit,â€ or â€œsurrender.â€
All cards are dealt from an infinite deck of cards, which means that there is no advantage in counting cards. At the start of a each game, two cards are dealt to both the player and the dealer. Both cards of the player are only known to the player, but one card of the dealer is dealt face up and the other face down. If a player or the dealer has 21 immediately, it is called a â€œnatural.â€ If the player has a natural immediately, the game is a draw if the dealer also has a natural, or the player wins if the dealer does not have a natural. Later, if the player does not have a natural immediately, the player has two choices in each time step: he can request an additional card (â€œhitsâ€) or he can stop requesting additional cards (â€œsticksâ€). The episode continues until he sticks or exceeds 21 (â€œgoes bustâ€). If he goes bust, he loses the game. Otherwise, if the sticks, it becomes the dealerâ€™s turn.
Whenever it is the dealerâ€™s turn, he also requests cards one by one, but his strategy or policy is fixed without any choice. The dealer sticks on any sum of 17 or greater and hits otherwise.
If the dealer goes bust, then the player wins. Otherwise, the result of the game (win, draw, or lose) and hence the terminal reward (+1, 0, âˆ’1, respec- tively) is determined by whose sum is closer to 21. All rewards are 0 except the terminal rewards. The learning task is an undiscounted (ğ›¾ = 1) episodic task, and therefore the terminal rewards are the returns.
An ace that can count as 11 without going bust is called a â€œusableâ€ ace. A
usable ace always counts as 11.
Learning progress is shown in Figure 2.9.
25
26
Chapter 2. Markov Decision Processes and Dynamic Programming
5001000150020002500300035004000450050005500600065007000750080008500900095001000010500110001150012000125001300013500140001450015000
batch_num
Learning Progress
âˆ’0.22âˆ’0.20âˆ’0.18âˆ’0.16âˆ’0.14âˆ’0.12âˆ’0.10âˆ’0.08âˆ’0.06âˆ’0.04âˆ’0.020.00
mean return (Â± stdev.)
Figure 2.9: Learning progress of ğ‘„-learning for Blackjack.
2.4.8 Schnapsen (Discrete/Discrete)
The rules of Schnapsen2 are available from the playing-card and board-game manufacturer Wiener Spielkartenfabrik Ferd. Piatnik & SÃ¶hne.
2.4.9 Autoregressive Trend Process (Continuous/Discrete)
Autoregressive trend processes such as the following serve as models for stock prices. We define the stochastic process
ğ‘0 âˆ¶= 1, ğ‘0 âˆ¶= ğ‘(0,1), ğ‘ğ‘˜ âˆ¶= ğ‘ğ‘˜âˆ’1 + ğ‘ğ‘˜âˆ’1 + ğœ†ğ‘(0,1), ğ‘ğ‘˜ âˆ¶= ğœ…ğ‘ğ‘˜âˆ’1 + ğ‘(0,1).
Here ğ‘(ğœ‡,ğœ2) means drawing a random number from the normal distribution with mean ğœ‡ and variance ğœ2. We use ğœ† âˆ¶= 3, ğœ… âˆ¶= 0.9, and ğ‘˜ âˆˆ {0,â€¦,99}. Having calculated 100 values {ğ‘0,â€¦,ğ‘99}, the scaled values
ğ‘ğ‘˜ âˆ¶= exp(
ğ‘ğ‘˜ max0â‰¤ğ‘˜<100 ğ‘ğ‘˜ âˆ’ min0â‰¤ğ‘˜<100 ğ‘ğ‘˜
)
serve as simulated stock prices.
2https://www.piatnik.com/kartenâ€‘spielregel
2.5. Policies, Value Functions, and Bellman Equations
The actions in this episodic task are taking long, neutral, or short positions. The transactions costs are fractions of the stock prices; it is instructive to learn policies for cheap and expensive transaction costs.
2.5 Policies, Value Functions, and Bellman Equa-
tions
After the discussion of environments, rewards, returns, and episodes, we focus on concepts that underlie learning algorithms. Learning optimal policies is the goal.
Definition 2.6 (policy). A policy is a function ğ’® Ã— ğ’œ(ğ‘ ) â†’ [0,1]. An agent is said to follow a policy ğœ‹ at time ğ‘¡, if ğœ‹(ğ‘|ğ‘ ) is the probability that the action ğ´ğ‘¡ = ğ‘ is chosen if ğ‘†ğ‘¡ = ğ‘ .
Like the dynamics function ğ‘ of the environment, a policy ğœ‹ is a function despite the notation that is reminiscent of a conditional probability. We denote the set of all policies by ğ’«.
The following two functions, the (state-)value function and the action-value function, are useful for the agent because they indicate how expedient it is to be in a state or to be in a state and to take a certain action, respectively. Both functions depend on a given policy.
Definition 2.7 ((state-)value function). The value function of a state ğ‘  under a policy ğœ‹ is
ğ‘£âˆ¶ ğ’« Ã— ğ’® â†’ â„,
ğ‘£ğœ‹(ğ‘ ) âˆ¶= ğ”¼ğœ‹[ğºğ‘¡ âˆ£ ğ‘†ğ‘¡ = ğ‘ ],
i.e., it is the expected discounted return when starting in state ğ‘  and following the policy ğœ‹ until the end of the episode.
Definition 2.8 (action-value function). The action-value function of a state- action pair(ğ‘ ,ğ‘) under a policy ğœ‹ is
ğ‘âˆ¶ ğ’« Ã— ğ’® Ã— ğ’œ(ğ‘ ) â†’ â„,
ğ‘ğœ‹(ğ‘ ,ğ‘) âˆ¶= ğ”¼ğœ‹[ğºğ‘¡ âˆ£ ğ‘†ğ‘¡ = ğ‘ , ğ´ğ‘¡ = ğ‘],
i.e., it is the expected discounted return when starting in state ğ‘ , taking action ğ‘, and then following the policy ğœ‹ until the end of the episode.
Recursive formulae such as (2.4) are fundamental throughout reinforcement learning and dynamic programming. We now use (2.4) to find a recursive formula for the value function ğ‘£ğœ‹ by calculating
ğ‘£ğœ‹(ğ‘ ) = ğ”¼ğœ‹[ğºğ‘¡ âˆ£ ğ‘†ğ‘¡ = ğ‘ ]
27
(2.5a)
28
Chapter 2. Markov Decision Processes and Dynamic Programming
= ğ”¼ğœ‹[ğ‘…ğ‘¡+1 + ğ›¾ğºğ‘¡+1 âˆ£ ğ‘†ğ‘¡ = ğ‘ ] ğœ‹(ğ‘|ğ‘ )âˆ‘ = âˆ‘ ğ‘ â€²âˆˆğ’®
ğ‘(ğ‘ â€²,ğ‘Ÿ âˆ£ ğ‘ ,ğ‘)(ğ‘Ÿ + ğ›¾ğ”¼ğœ‹[ğºğ‘¡+1 âˆ£ ğ‘†ğ‘¡+1 = ğ‘ â€²])
âˆ‘ ğ‘Ÿâˆˆâ„›
ğ‘âˆˆğ’œ(ğ‘ )
= âˆ‘
ğ‘âˆˆğ’œ(ğ‘ )
ğœ‹(ğ‘|ğ‘ )âˆ‘ ğ‘ â€²âˆˆğ’®
âˆ‘ ğ‘Ÿâˆˆâ„›
ğ‘(ğ‘ â€²,ğ‘Ÿ âˆ£ ğ‘ ,ğ‘)(ğ‘Ÿ + ğ›¾ğ‘£ğœ‹(ğ‘ â€²))
âˆ€ğ‘  âˆˆ ğ’®.
This equation is called the Bellman equation for ğ‘£ğœ‹, and it is fundamental to computing, approximating, and learning ğ‘£ğœ‹. The solution ğ‘£ğœ‹ exists uniquely if ğ›¾ < 1 or all episodes are guaranteed to terminate from all states ğ‘  âˆˆ ğ’® under policy ğœ‹.
In the case of a finite MDP, the optimal policy is defined as follows. We start by noting that state-value functions can be used to define a partial ordering over the policies: a policy ğœ‹ is defined to be better than or equal to a policy ğœ‹â€² if its value function ğ‘£ğœ‹ is greater than or equal to the value function ğ‘£ğœ‹â€² for all states. We write
ğœ‹ â‰¥ ğœ‹â€²
âˆ¶âŸº ğ‘£ğœ‹(ğ‘ ) â‰¥ ğ‘£ğœ‹â€²(ğ‘ ) âˆ€ğ‘  âˆˆ ğ’®.
An optimal policy is a policy that is greater than or equal to all other policies. The optimal policy may not be unique. We denote optimal policies by ğœ‹âˆ—.
Optimal policies share the same state-value and action-value functions. The
optimal state-value function is given by
ğ‘£âˆ—âˆ¶ ğ’® â†’ â„,
ğ‘£âˆ—(ğ‘ ) âˆ¶= max ğœ‹âˆˆğ’«
ğ‘£ğœ‹(ğ‘ ),
and the optimal action-value function is given by
ğ‘âˆ—âˆ¶ ğ’® Ã— ğ’œ â†’ â„,
ğ‘âˆ—(ğ‘ ,ğ‘) âˆ¶= max ğœ‹âˆˆğ’«
ğ‘ğœ‹(ğ‘ ,ğ‘),
These two functions are related by the equation
ğ‘âˆ—(ğ‘ ,ğ‘) = ğ”¼[ğ‘…ğ‘¡+1 + ğ›¾ğ‘£âˆ—(ğ‘†ğ‘¡+1) âˆ£ ğ‘†ğ‘¡ = ğ‘ , ğ´ğ‘¡ = ğ‘]
âˆ€(ğ‘ ,ğ‘) âˆˆ ğ’® Ã— ğ’œ(ğ‘ ).
Next, we find a recursion for these two optimal value functions similar to the
Bellman equation above. Similarly to the derivation of (2.5), we calculate
ğ‘£âˆ—(ğ‘ ) = max ğ‘âˆˆğ’œ(ğ‘ )
ğ‘ğœ‹âˆ—
(ğ‘ ,ğ‘)
= max ğ‘âˆˆğ’œ(ğ‘ )
ğ”¼ğœ‹âˆ—
[ğºğ‘¡ âˆ£ ğ‘†ğ‘¡ = ğ‘ , ğ´ğ‘¡ = ğ‘]
= max ğ‘âˆˆğ’œ(ğ‘ )
ğ”¼ğœ‹âˆ—
[ğ‘…ğ‘¡+1 + ğ›¾ğºğ‘¡+1 âˆ£ ğ‘†ğ‘¡ = ğ‘ , ğ´ğ‘¡ = ğ‘]
= max ğ‘âˆˆğ’œ(ğ‘ )
ğ”¼[ğ‘…ğ‘¡+1 + ğ›¾ğ‘£âˆ—(ğ‘†ğ‘¡+1) âˆ£ ğ‘†ğ‘¡ = ğ‘ , ğ´ğ‘¡ = ğ‘]
(2.5b)
(2.5c)
(2.5d)
(2.6)
(2.7a)
(2.7b)
(2.7c)
(2.7d)
2.6. On-Policy and Off-Policy Learning
29
= max ğ‘âˆˆğ’œ(ğ‘ )
âˆ‘ ğ‘ â€²âˆˆğ’®
âˆ‘ ğ‘Ÿâˆˆâ„›
ğ‘(ğ‘ â€²,ğ‘Ÿ âˆ£ ğ‘ ,ğ‘)(ğ‘Ÿ + ğ›¾ğ‘£âˆ—(ğ‘ â€²))
âˆ€ğ‘  âˆˆ ğ’®.
(2.7e)
The last two equations are both called the Bellman optimality equation for ğ‘£âˆ—. Analogously, two forms of the Bellman optimality equation for ğ‘âˆ— are
ğ‘âˆ—(ğ‘ ,ğ‘) = ğ”¼[ğ‘…ğ‘¡+1 + ğ›¾ max ğ‘â€²âˆˆğ’œ
ğ‘âˆ—(ğ‘†ğ‘¡+1,ğ‘â€²) âˆ£ ğ‘†ğ‘¡ = ğ‘ , ğ´ğ‘¡ = ğ‘]
(2.8a)
= âˆ‘ ğ‘ â€²âˆˆğ’®
âˆ‘ ğ‘Ÿâˆˆâ„›
ğ‘(ğ‘ â€²,ğ‘Ÿ âˆ£ ğ‘ ,ğ‘)(ğ‘Ÿ + ğ›¾ max ğ‘â€²âˆˆğ’œ(ğ‘ )
ğ‘âˆ—(ğ‘ â€²,ğ‘â€²))
âˆ€(ğ‘ ,ğ‘) âˆˆ ğ’® Ã— ğ’œ(ğ‘ ).
(2.8b)
One can try to solve the Bellman optimality equations for ğ‘£âˆ— or ğ‘âˆ—; they are just systems of algebraic equations. If the optimal action-value function ğ‘âˆ— is known, an optimal policy ğœ‹âˆ— is easily found; we are still considering the case of a finite MDP. However, there are a few reasons why this approach is seldomly expedient for realistic problems:
The Markov property may not hold.
The dynamics of the environment, i.e., the function ğ‘, must be known.
The system of equations may be huge.
2.6 On-Policy and Off-Policy Learning
2.7 Policy Evaluation (Prediction)
Policy evaluation means that we evaluate how well a policy ğœ‹ does by computing its state-value function ğ‘£ğœ‹. The term â€œpolicy evaluationâ€ is common in dynamic programming, while the term â€œpredictionâ€ is common in reinforcement learn- ing. In policy evaluation, a policy ğœ‹ is given and its state-value function ğ‘£ğœ‹ is calculated.
Instead of solving the Bellman equation (2.5) for ğ‘£ğœ‹ directly, we follow an iterative approach. Starting from an arbitrary initial approximation ğ‘£0âˆ¶ ğ’® â†’ â„ (whose terminal state must have the value 0), we use (2.5) to define the iteration
ğ‘£ğ‘˜+1(ğ‘ ) = ğ”¼ğœ‹[ğ‘…ğ‘¡+1 + ğ›¾ğ‘£ğ‘˜(ğ‘†ğ‘¡+1) âˆ£ ğ‘†ğ‘¡ = ğ‘ ]
(2.9a)
= âˆ‘
ğ‘âˆˆğ’œ(ğ‘ )
ğœ‹(ğ‘|ğ‘ )âˆ‘ ğ‘ â€²âˆˆğ’®
âˆ‘ ğ‘Ÿâˆˆâ„›
ğ‘(ğ‘ â€²,ğ‘Ÿ âˆ£ ğ‘ ,ğ‘)(ğ‘Ÿ + ğ›¾ğ‘£ğ‘˜(ğ‘ â€²))
(2.9b)
30
Chapter 2. Markov Decision Processes and Dynamic Programming
for ğ‘£ğ‘˜+1âˆ¶ ğ’® â†’ â„. This iteration is called iterative policy evaluation.
If ğ›¾ < 1 or all episodes are guaranteed to terminate from all states ğ‘  âˆˆ ğ’® under policy ğœ‹, then this operator is a contraction and hence the approximations ğ‘£ğ‘˜ converge to the state-value function ğ‘£ğœ‹ as ğ‘˜ â†’ âˆ, since ğ‘£ğœ‹ is the fixed point by the Bellman equation (2.5) for ğ‘£ğœ‹.
The updates performed in (2.9) and in dynamic programming in general are called expected updates, since the expected value over all possible next states is computed in contrast to using a sample next state.
Algorithm 2 shows how this iteration can be implemented with updates per- formed in place. It also shows a common termination condition that uses the maximum norm and a prescribed accuracy threshold.
Algorithm 2 iterative policy evaluation for approximating v â‰ˆ ğ‘£ğœ‹ given ğœ‹ âˆˆ ğ’«.
Initialization: choose the accuracy threshold ğ›¿ âˆˆ â„+, initialize the vector v of length |ğ’®| arbitrarily
except that the value of the terminal state is 0.
repeat
ğ‘‘ âˆ¶= 0 for all ğ‘  âˆˆ ğ’® do ğ‘¤ âˆ¶= ğ‘£ğ‘  ğ‘£ğ‘  âˆ¶= âˆ‘ğ‘âˆˆğ’œ(ğ‘ ) ğœ‹(ğ‘|ğ‘ )âˆ‘ğ‘ â€²âˆˆğ’® âˆ‘ğ‘Ÿâˆˆâ„› ğ‘(ğ‘ â€²,ğ‘Ÿ âˆ£ ğ‘ ,ğ‘)(ğ‘Ÿ + ğ›¾ğ‘£ğ‘ â€²) ğ‘‘ âˆ¶= max(ğ‘‘,|ğ‘£ğ‘  âˆ’ ğ‘¤|)
â–· save the old value
end for until ğ‘‘ < ğ›¿
return v
2.8 Policy Improvement
Having seen how we can evaluate a policy, we now discuss how to improve it. To do so, the value functions show their usefulness. For now, we assume that the policies we consider here are deterministic.
Similarly to (2.6), the action value of selecting action ğ‘ and then following
policy ğœ‹ can be written as
ğ‘ğœ‹(ğ‘ ,ğ‘) = ğ”¼[ğ‘…ğ‘¡+1 + ğ›¾ğ‘£ğœ‹(ğ‘†ğ‘¡+1) âˆ£ ğ‘†ğ‘¡ = ğ‘ , ğ´ğ‘¡ = ğ‘]
(2.10a)
2.8. Policy Improvement
= âˆ‘ ğ‘ â€²âˆˆğ’®
âˆ‘ ğ‘Ÿâˆˆâ„›
ğ‘(ğ‘ â€²,ğ‘Ÿ âˆ£ ğ‘ ,ğ‘)(ğ‘Ÿ + ğ›¾ğ‘£ğœ‹(ğ‘ â€²))
âˆ€(ğ‘ ,ğ‘) âˆˆ ğ’® Ã— ğ’œ(ğ‘ ).
This formula helps us determine if the action ğœ‹â€²(ğ‘ ) of another policy ğœ‹â€² is an improvement over ğœ‹(ğ‘ ) in this time step. In order to be an improvement in this time step, the inequality
ğ‘ğœ‹(ğ‘ ,ğœ‹â€²(ğ‘ )) â‰¥ ğ‘£ğœ‹(ğ‘ )
must hold. If this inequality holds, we also expect that selecting ğœ‹â€²(ğ‘ ) instead of ğœ‹(ğ‘ ) every time the state ğ‘  occurs is an improvement. This is the subject of the following theorem.
Theorem 2.9 (policy improvement theorem). Suppose ğœ‹ and ğœ‹â€² are two deter- ministic policies such that
ğ‘ğœ‹(ğ‘ ,ğœ‹â€²(ğ‘ )) â‰¥ ğ‘£ğœ‹(ğ‘ )
âˆ€ğ‘  âˆˆ ğ’®.
Then
ğœ‹â€² â‰¥ ğœ‹,
i.e., the policy ğœ‹â€² is greater than or equal to ğœ‹.
Proof. By the definition of the partial ordering of policies, we must show that
ğ‘£ğœ‹â€²(ğ‘ ) â‰¥ ğ‘£ğœ‹(ğ‘ )
âˆ€ğ‘  âˆˆ ğ’®.
Using the assumption and (2.10), we calculate
ğ‘£ğœ‹(ğ‘ ) â‰¤ ğ‘ğœ‹(ğ‘ ,ğœ‹â€²(ğ‘ ))
= ğ”¼[ğ‘…ğ‘¡+1 + ğ›¾ğ‘£ğœ‹(ğ‘†ğ‘¡+1) âˆ£ ğ‘†ğ‘¡ = ğ‘ , ğ´ğ‘¡ = ğœ‹â€²(ğ‘ )] = ğ”¼ğœ‹â€²[ğ‘…ğ‘¡+1 + ğ›¾ğ‘£ğœ‹(ğ‘†ğ‘¡+1) âˆ£ ğ‘†ğ‘¡ = ğ‘ ] â‰¤ ğ”¼ğœ‹â€²[ğ‘…ğ‘¡+1 + ğ›¾ğ‘ğœ‹(ğ‘†ğ‘¡+1,ğœ‹â€²(ğ‘†ğ‘¡+1)) âˆ£ ğ‘†ğ‘¡ = ğ‘ ] = ğ”¼ğœ‹â€²[ğ‘…ğ‘¡+1 + ğ›¾ğ”¼ğœ‹â€²[ğ‘…ğ‘¡+2 + ğ›¾ğ‘£ğœ‹(ğ‘†ğ‘¡+2) âˆ£ ğ‘†ğ‘¡+1] âˆ£ ğ‘†ğ‘¡ = ğ‘ ] = ğ”¼ğœ‹â€²[ğ‘…ğ‘¡+1 + ğ›¾ğ‘…ğ‘¡+2 + ğ›¾2ğ‘£ğœ‹(ğ‘†ğ‘¡+2) âˆ£ ğ‘†ğ‘¡ = ğ‘ ] â‰¤ ğ”¼ğœ‹â€²[ğ‘…ğ‘¡+1 + ğ›¾ğ‘…ğ‘¡+2 + ğ›¾2ğ‘…ğ‘¡+3 + ğ›¾3ğ‘£ğœ‹(ğ‘†ğ‘¡+3) âˆ£ ğ‘†ğ‘¡ = ğ‘ ] â‹® â‰¤ ğ”¼ğœ‹â€²[ğ‘…ğ‘¡+1 + ğ›¾ğ‘…ğ‘¡+2 + ğ›¾2ğ‘…ğ‘¡+3 + ğ›¾3ğ‘…ğ‘¡+4 + â‹¯ âˆ£ ğ‘†ğ‘¡ = ğ‘ ] = ğ‘£ğœ‹â€²(ğ‘ ),
which concludes the proof.
31
(2.10b)
32
Chapter 2. Markov Decision Processes and Dynamic Programming
In addition to making changes to the policy in single states, we can define a new, greedy policy ğœ‹â€² by selection the action that appears best in each state according to a given action-value function ğ‘ğœ‹. This greedy policy is given by
ğœ‹â€²(ğ‘ ) âˆ¶âˆˆ argmax ğ‘âˆˆğ’œ(ğ‘ )
ğ‘ğœ‹(ğ‘ ,ğ‘),
(2.11a)
= argmax ğ‘âˆˆğ’œ(ğ‘ )
ğ”¼[ğ‘…ğ‘¡+1 + ğ›¾ğ‘£ğœ‹(ğ‘†ğ‘¡+1) âˆ£ ğ‘†ğ‘¡ = ğ‘ , ğ´ğ‘¡ = ğ‘]
(2.11b)
= argmax ğ‘âˆˆğ’œ(ğ‘ )
âˆ‘ ğ‘ â€²âˆˆğ’®
âˆ‘ ğ‘Ÿâˆˆâ„›
ğ‘(ğ‘ â€²,ğ‘Ÿ âˆ£ ğ‘ ,ğ‘)(ğ‘Ÿ + ğ›¾ğ‘£ğœ‹(ğ‘ â€²)).
(2.11c)
Any ties in the argmax are broken in a random manner. By construction, the policy ğœ‹â€² satisfies the assumption of Theorem 2.9, implying that it is better than or equal to ğœ‹. This process is called policy improvement. We have created a new policy that improves on the original policy by making it greedy with respect to the value function of the original policy.
In the case that the ğœ‹â€² = ğœ‹, i.e., the new, greedy policy is as good as the original one, the equation ğ‘£ğœ‹ = ğ‘£ğœ‹â€² follows, and using the definition (2.11) of ğœ‹â€², we find
ğ‘£ğœ‹â€²(ğ‘ ) = max ğ‘âˆˆğ’œ(ğ‘ )
ğ”¼[ğ‘…ğ‘¡+1 + ğ›¾ğ‘£ğœ‹â€²(ğ‘†ğ‘¡+1) âˆ£ ğ‘†ğ‘¡ = ğ‘ , ğ´ğ‘¡ = ğ‘]
(2.12a)
= max ğ‘âˆˆğ’œ(ğ‘ )
âˆ‘ ğ‘ â€²âˆˆğ’®
âˆ‘ ğ‘Ÿâˆˆâ„›
ğ‘(ğ‘ â€²,ğ‘Ÿ âˆ£ ğ‘ ,ğ‘)(ğ‘Ÿ + ğ›¾ğ‘£ğœ‹â€²(ğ‘ â€²))
âˆ€ğ‘  âˆˆ ğ’®,
(2.12b)
which is the Bellman optimality equation (2.7). Since ğ‘£ğœ‹â€² satisfies the optimality equation (2.7), ğ‘£ğœ‹â€² = ğ‘£âˆ— holds, meaning that both ğœ‹ and ğœ‹â€² are optimal policies. In other words, policy improvement yields a strictly better policy unless the original policy is already optimal.
So far we have considered only deterministic policies, but these ideas can be extended to stochastic policies, and Theorem 2.9 also holds for stochastic policies. When defining the greedy policy, all maximizing actions can be assigned some nonzero probability.
2.9 Policy Iteration
Policy iteration is the process of using policy evaluation and policy improvement to define a sequence of monotonically improving policies and value functions. . We start from a policy ğœ‹0 and evaluate it to find its state-value function ğ‘£ğœ‹0 , we use policy improvement to define a new policy ğœ‹1. This policy is Using ğ‘£ğœ‹0 evaluated, and so forth, resulting in the sequence
ğœ‹0
eval. âŸ¶ ğ‘£ğœ‹0
impr. âŸ¶ ğœ‹1
eval. âŸ¶ ğ‘£ğœ‹1
impr. âŸ¶ ğœ‹2
eval. âŸ¶ ğ‘£ğœ‹2
impr. âŸ¶ â‹¯
impr. âŸ¶ ğœ‹âˆ—
eval. âŸ¶ ğ‘£âˆ—.
2.10. Value Iteration
Unless a policy ğœ‹ğ‘˜ is already optimal, it is a strict improvement over the previous policy ğœ‹ğ‘˜âˆ’1. In the case of a finite MDP, there is only a finite number of policies, and hence the sequence converges to the optimal policy and value function within a finite number of iterations.
A policy-iteration algorithm is shown in Algorithm 3. Each policy evalua- tion is started with the value function of the previous policy, which speeds up convergence. Note that the update of ğ‘£ğ‘  has changed.
2.10 Value Iteration
A time-consuming property in Algorithm 3 is the fact that the repeat loop for policy evaluation is nested within the outer loop. Nesting these two loops may be quite time consuming. (The other inner loop is a for loop with a fixed number of iterations.) This suggests to try to get rid of these two nested loops while still guaranteeing convergence. An important simple case is to perform only one iteration policy evaluation, which makes it possible to combine policy evaluation and improvement into one loop. This algorithm is called value iteration, and it can be shown to convergence under the same assumptions that guarantee the existence of ğ‘£âˆ—.
Turning the fixed-point equation (2.12) into an iteration, value iteration
becomes the update
ğ‘£ğ‘˜+1(ğ‘ ) âˆ¶= max ğ‘âˆˆğ’œ(ğ‘ )
ğ”¼[ğ‘…ğ‘¡+1 + ğ›¾ğ‘£ğ‘˜(ğ‘†ğ‘¡+1) âˆ£ ğ‘†ğ‘¡ = ğ‘ , ğ´ğ‘¡ = ğ‘]
= max ğ‘âˆˆğ’œ(ğ‘ )
âˆ‘ ğ‘ â€²âˆˆğ’®
âˆ‘ ğ‘Ÿâˆˆâ„›
ğ‘(ğ‘ â€²,ğ‘Ÿ âˆ£ ğ‘ ,ğ‘)(ğ‘Ÿ + ğ›¾ğ‘£ğ‘˜(ğ‘ â€²)).
The algorithm is shown in Algorithm 4. Note that the update of ğ‘£ğ‘  has changed again, now incorporating taking the maximum from the policy-improvement part.
2.11 Bibliographical and Historical Remarks
The classic textbook on dynamic programming is [35] by Richard Bellman. The most influential introductory textbook on RL is [23]. An excellent summary of the theory of Markov decision processes is [27].
33
34
Chapter 2. Markov Decision Processes and Dynamic Programming
Algorithm 3 policy iteration for calculating v â‰ˆ ğ‘£âˆ— and ğœ‹ â‰ˆ ğœ‹âˆ—.
Initialization: choose the accuracy threshold ğ›¿ âˆˆ â„+, initialize the vector v of length |ğ’®| arbitrarily
except that the value of the terminal state is 0,
initialize the vector ğœ‹ of length |ğ’®| arbitrarily.
loop
policy evaluation: repeat
ğ‘‘ âˆ¶= 0 for all ğ‘  âˆˆ ğ’® do ğ‘¤ âˆ¶= ğ‘£ğ‘  ğ‘£ğ‘  âˆ¶= âˆ‘ğ‘ â€²âˆˆğ’® âˆ‘ğ‘Ÿâˆˆâ„› ğ‘(ğ‘ â€²,ğ‘Ÿ âˆ£ ğ‘ ,ğœ‹(ğ‘ ))(ğ‘Ÿ + ğ›¾ğ‘£ğ‘ â€²)
â–· save the old value
â–· note the change: ğœ‹ğ‘  is an optimal action
ğ‘‘ âˆ¶= max(ğ‘‘,|ğ‘£ğ‘  âˆ’ ğ‘¤|)
end for until ğ‘‘ < ğ›¿
policy improvement: policyIsStable := true for all ğ‘  âˆˆ ğ’® do
old_action := ğœ‹[ğ‘ ] ğœ‹ğ‘  âˆ¶âˆˆ argmaxğ‘âˆˆğ’œ(ğ‘ ) âˆ‘ğ‘ â€²âˆˆğ’® âˆ‘ğ‘Ÿâˆˆâ„› ğ‘(ğ‘ â€²,ğ‘Ÿ âˆ£ ğ‘ ,ğ‘)(ğ‘Ÿ + ğ›¾ğ‘£ğ‘ â€²)
â–· save the old value
â–· ğœ‹ğ‘  âˆ¶âˆˆ argmaxğ‘âˆˆğ’œ(ğ‘ ) ğ‘(ğ‘ ,ğ‘)
if old_action â‰  ğœ‹(ğ‘ ) then
policy_is_stable := false
end if
end for if policy_is_stable then
return v â‰ˆ ğ‘£âˆ— and ğœ‹ â‰ˆ ğœ‹âˆ—
end if end loop
2.11. Bibliographical and Historical Remarks
35
Algorithm 4 value iteration for calculating v â‰ˆ ğ‘£âˆ— and ğœ‹ â‰ˆ ğœ‹âˆ—.
Initialization: choose the accuracy threshold ğ›¿ âˆˆ â„+, initialize the vector v of length |ğ’®| arbitrarily
except that the value of the terminal state is 0,
initialize the vector ğœ‹ of length |ğ’®| arbitrarily.
policy evaluation and improvement: repeat
ğ‘‘ âˆ¶= 0 for all ğ‘  âˆˆ ğ’® do ğ‘¤ âˆ¶= ğ‘£ğ‘  ğ‘£ğ‘  âˆ¶= maxğ‘âˆˆğ’œ(ğ‘ ) âˆ‘ğ‘ â€²âˆˆğ’® âˆ‘ğ‘Ÿâˆˆâ„› ğ‘(ğ‘ â€²,ğ‘Ÿ âˆ£ ğ‘ ,ğ‘)(ğ‘Ÿ + ğ›¾ğ‘£ğ‘ â€²)
â–· save the old value
â–· note the change
ğ‘‘ âˆ¶= max(ğ‘‘,|ğ‘£ğ‘  âˆ’ ğ‘¤|)
end for until ğ‘‘ < ğ›¿
calculate deterministic policy: for all ğ‘  âˆˆ ğ’® do
ğœ‹ğ‘  âˆ¶âˆˆ argmaxğ‘âˆˆğ’œ(ğ‘ ) âˆ‘ğ‘ â€²âˆˆğ’® âˆ‘ğ‘Ÿâˆˆâ„› ğ‘(ğ‘ â€²,ğ‘Ÿ âˆ£ ğ‘ ,ğ‘)(ğ‘Ÿ + ğ›¾ğ‘£ğ‘ â€²)
â–· ğœ‹ğ‘  âˆ¶âˆˆ argmaxğ‘âˆˆğ’œ(ğ‘ ) ğ‘(ğ‘ ,ğ‘)
end for
return v â‰ˆ ğ‘£âˆ— and ğœ‹ â‰ˆ ğœ‹âˆ—
36
Chapter 2. Markov Decision Processes and Dynamic Programming
2.12 Exercises
2.12.1 Applications and Environments
Exercise 2.1. Think of another (preferably creative) application of reinforce- ment learning. Specify the environments, agents, states, actions, and rewards. Which assumptions are needed to satisfy the Markov property?
Exercise 2.2. Try to find a goal-directed learning task that cannot be repre- sented by a Markov decision process.
Exercise 2.3 (Simple Grid World). Implement Simple Grid World (see Sec- tion 2.4.1).
Exercise 2.4 (Windy Grid World). Implement Windy Grid World (see Sec- tion 2.4.2).
Exercise 2.5 (Cliff Walking). Implement Cliff Walking (see Section 2.4.3).
Exercise 2.6 (Frozen Lake). Implement Frozen Lake (see Section 2.4.4).
Exercise 2.7 (variants of Frozen Lake).
1. In Frozen Lake (see Section 2.4.4), there is a positive reward only when the goal is reached. However, in mazes, the rewards are usually âˆ’1 until the goal is reached. Implement the usual rewards as a variant and investigate any differences when learning the two variants.
2. Another variant is that holes do not end the episode, but incur a large negative reward and teleport the agent to the start. Implement this variant and investigate any differences in learning.
Exercise 2.8 (Rock paper scissors). Implement Rock paper scissors (see Sec- tion 2.4.5).
Exercise 2.9 (Blackjack). Implement Blackjack (see Section 2.4.7).
Exercise 2.10 (Schnapsen). * Implement Schnapsen (see Section 2.4.8). Exercise 2.11 (autoregressive trend process). Implement an autoregressive trend process (see Section 2.4.9).
Exercise 2.12 (ğœ–-greedy action selection). Assume that ğœ–-greedy action selec- tion is used and that there is a single greedy action.
1. Suppose |ğ’œ| = 4 and ğœ– = 1/5. What is the probability that the greedy action is selected?
2. Give a formula for calculating the probability of selecting the greedy action for any |ğ’œ| and any ğœ–.
2.12. Exercises
2.12.2 Multi-Armed Bandits
Exercise 2.13 (multi-armed bandits with ğœ–-greedy action selection (program- ming)). You play against a 10-armed bandit, where at the beginning of each episode the true value ğ‘âˆ—(ğ‘), ğ‘ âˆˆ {1,â€¦,10}, of each of the 10 actions is chosen to be normally distributed with mean zero and unit variance. The rewards af- ter choosing action/bandit ğ‘ are normally distributed with mean ğ‘âˆ—(ğ‘) and unit variance. Using the simple bandit algorithm and ğœ–-greedy action selection, you have 1000 time steps or tries in each episode to maximize the average reward starting from zero knowledge about the bandits.
Which value of ğœ– maximizes the average reward? Which value of ğœ– maximizes
the percentage of optimal actions taken?
Exercise 2.14 (multi-armed bandits with upper-confidence-bound action selec- tion (programming)). This exercise is the same as in Exercise 2.13, but now the actions
ğ´ğ‘¡ âˆ¶âˆˆ argmax
ğ‘
(ğ‘„ğ‘¡(ğ‘) + ğ‘âˆš
lnğ‘¡ ğ‘ğ‘¡(ğ‘)
)
are selected according to the upper-confidence bound. Which value of ğ‘ yields the largest average reward?
Exercise 2.15 (multi-armed bandits with soft-max action selection (program- ming)). This exercise is the same as Exercise 2.13, but now the actions ğ´ğ‘¡ âˆˆ ğ’œ = {1,â€¦,|ğ’œ|} are selected with probability
â„™[ğ‘] =
exp(ğ‘„ğ‘¡(ğ‘)/ğœ) ğ‘–=1 exp(ğ‘„ğ‘¡(ğ‘–)/ğœ)
âˆ‘|ğ’œ|
,
where the parameter ğœ is called the temperature. This probability distribution is called the soft-max or Boltzmann distribution.
What are the effects of low and high temperatures, i.e., how does the tem- perature influence the probability distribution all else being equal? Which value of ğœ yields the largest average reward?
2.12.3 Step Sizes
Exercise 2.16 (harmonic step sizes). Show that the step sizes
ğ›¼ğ‘› âˆ¶=
1 ğ‘ğ‘› + ğ‘
,
ğ‘,ğ‘ âˆˆ â„,
37
38
Chapter 2. Markov Decision Processes and Dynamic Programming
(where ğ‘ âˆˆ â„+ and ğ‘ âˆˆ â„ are chosen such that ğ‘ğ‘›+ğ‘ > 0) satisfy the convergence conditions
âˆ âˆ‘ ğ‘›=1
âˆ âˆ‘ ğ‘›=1
ğ›¼2
ğ›¼ğ‘› = âˆ,
ğ‘› < âˆ.
Exercise 2.17 (unbiased step sizes). We use the iteration
ğ‘„1 âˆˆ â„,
ğ‘„ğ‘›+1 âˆ¶= ğ‘„ğ‘› + ğ›¼ğ‘›(ğ‘…ğ‘› âˆ’ ğ‘„ğ‘›),
ğ‘› â‰¥ 1,
to estimate ğ‘„ğ‘› using ğ‘…ğ‘›, where
ğ›¼ğ‘› âˆ¶=
ğ›¼ ğ›½ğ‘›
,
ğ›¼ âˆˆ (0,1), ğ‘› â‰¥ 1,
and
ğ›½0 âˆ¶= 0, ğ›½ğ‘› âˆ¶= ğ›½ğ‘›âˆ’1 + ğ›¼(1 âˆ’ ğ›½ğ‘›âˆ’1),
ğ‘› â‰¥ 1.
Show that the iteration for ğ‘„ğ‘› above yields an exponential recency-weighted average without initial bias (i.e., the ğ‘„ğ‘› do not depend on the initial value ğ‘„1).
2.12.4 Basic Definitions
Exercise 2.18 (returns and episodes). Suppose ğ›¾ âˆ¶= 1/2 and the rewards ğ‘…1 âˆ¶= 1, ğ‘…2 âˆ¶= âˆ’1, ğ‘…3 âˆ¶= 2, ğ‘…4 âˆ¶= âˆ’1, and ğ‘…5 âˆ¶= 2 are received in an episode with length ğ‘‡ âˆ¶= 5. What are ğº0,â€¦,ğº5?
Exercise 2.19 (returns and episodes). Suppose ğ›¾ âˆ¶= 0.9 and the reward se- quence starts with ğ‘…1 âˆ¶= âˆ’1 and ğ‘…2 âˆ¶= 2 and is followed by an infinite sequence of 1s. What are ğº0, ğº1, and ğº2?
Exercise 2.20 (change of return). In episodic tasks and in continuing tasks, how does the return ğºğ‘¡ change if a constant ğ‘ is added to all rewards ğ‘…ğ‘¡? How does it change if all rewards ğ‘…ğ‘¡ are multiplied by a constant ğ‘?
2.12.5 Dynamic Programming
Exercise 2.21 (equation for ğ‘£ğœ‹). Give an equation for ğ‘£ğœ‹ in terms of ğ‘ğœ‹ and ğœ‹.
Exercise 2.22 (equation for ğ‘ğœ‹). Give an equation for ğ‘ğœ‹ in terms of ğ‘£ğœ‹ and the four-argument ğ‘.
2.12. Exercises
Exercise 2.23 (Bellman equation for ğ‘ğœ‹). Analogous to the derivation of the Bellman equation for ğ‘£ğœ‹, derive the Bellman equation for ğ‘ğœ‹.
Exercise 2.24 (equation for ğ‘£âˆ—). Give an equation for ğ‘£âˆ— in terms of ğ‘âˆ—.
Exercise 2.25 (equation for ğ‘âˆ—). Give an equation for ğ‘âˆ— in terms of ğ‘£âˆ— and the four-argument ğ‘.
Exercise 2.26 (equation for ğœ‹âˆ—). Give an equation for ğœ‹âˆ— in terms of ğ‘âˆ—.
Exercise 2.27 (equation for ğœ‹âˆ—). Give an equation for ğœ‹âˆ— in terms of ğ‘£âˆ— and the four-argument ğ‘.
Exercise 2.28 (update rule for ğ‘ğœ‹). Using the Bellman equation for ğ‘ğœ‹ (see Exercise 2.23), find an update rule for approximations ğ‘ğ‘˜+1 of ğ‘ğœ‹ (in terms of ğ‘ğ‘˜, ğœ‹, and ğ‘) analogous to the update rule for ğ‘£ğ‘˜+1.
Exercise 2.29 (iterative policy evaluation (programming)). Implement itera- tive policy evaluation and use it to estimate ğ‘£ğœ‹ for the grid world in Exercise 2.3, where ğœ‹ is the equiprobable random policy.
Exercise 2.30 (policy iteration (programming)). Implement policy iteration and use it to estimate ğœ‹âˆ— for the grid world in Exercise 2.3.
Exercise 2.31 (value iteration (programming)). Implement value iteration and use it to estimate ğœ‹âˆ— for the grid world in Exercise 2.3.
39
40
Chapter 2. Markov Decision Processes and Dynamic Programming
Chapter 3
Taxonomy of Algorithms
Greek tÃ¡xis: arrangement, order.
This chapter provides a taxonomy of reinforcement-learning algorithms for cal- culating state-value and action-value functions. In machine learning â€“ and hence in reinforcement learning in particular â€“, an optimization and a generalization problem must be solved. The source of the optimization problem in reinforce- ment learning is obvious; we seek optimal policies. The generalization problem arises whenever function approximations on an infinite state or state-action space are used to represent the solutions. The type of function approximation must fit the properties of the environment and the state or state-action space such that an optimal solution can be represented well and overfitting is avoided at the same time. The approach in this chapter is to start from various errors and loss functions to be minimized and elucidate the choices that are made in order to arrive at various learning algorithms.
3.1 Introduction
Optimization clearly plays a fundamental role in reinforcement learning, as op- timal policies are sought. In this chapter, we present and discuss optimization problems that occur on conjunction with function approximations of the state- value and action-value functions. In prediction, the state- and action-value func- tions of a given a policy are sought. In control, there are a indirect and direct ways to find an optimal policy. The indirect one is to calculate the action-value function of an optimal policy, which immediately yields an optimal policy when-
41
42
Chapter 3. Taxonomy of Algorithms
ever there is a finite number of actions; the direct one is to calculate the optimal policy.
The goal of the present taxonomy of algorithms is to elucidate the various choices that must be and can be made in order to arrive at learning algorithms starting from various choices of error expressions. In this way, a (hopefully) complete picture of learning algorithms and their interrelations obtained.
We start from a state space that is infinite; more precisely, it is a subset of a real vector space, i.e., ğ’® âŠ‚ â„dimğ’®. Later, we will also discuss how the case of a finite state space can be considered a special case of the infinite case, unifying the theory of reinforcement learning for both finite and infinite state spaces. The following discussion applies to both prediction and control problems, i.e., to both state-value functions and action-value functions. The true state-value functions are denoted by ğ‘£ğœ‹ and ğ‘£âˆ—, and the true action-value functions are denoted by ğ‘ğœ‹ and ğ‘âˆ—. Their approximations are denoted by hats; for example,
Ì‚ğ‘‰ (ğ‘ ,w) â‰ˆ ğ‘£ğœ‹(ğ‘ ), Ì‚ğ‘„(ğ‘ ,ğ‘,w) â‰ˆ ğ‘ğœ‹(ğ‘ ,ğ‘),
where the parameter vector
w âˆˆ â„ğ‘‘
and ğ‘‘ â‰ª dimğ’® in general. If it would not be the case that ğ‘‘ â‰ª dimğ’®, the approximation and hence dimension reduction would hardly be worthwhile. We use capital letters for the approximations, since they are generally random vari- ables, because they depend on the environment and on the policy.
3.2 Types of Errors
The first choice to be made in the derivation of a learning algorithm concerns the type of error. The error should be a norm, it is usually applied to the difference between the approximation and the true value, and it will be minimized.
We denote the ğ¿ğ‘ norm of a function ğ‘“ by â€–ğ‘“â€–ğ‘ and the weighted ğ¿ğ‘ norm
of a function ğ‘“ with respect to a weight function ğœ‡ by â€–ğ‘“â€–ğ‘,ğœ‡.
3.2.1 The Mean Squared Value Error
The state distribution ğœ‡ğœ‹âˆ¶ ğ’® â†’ [0,1] of a policy ğœ‹ is the probability of visiting a state while following the policy ğœ‹. State distributions are used in certain error definitions.
We consider episodic tasks first [23, Section 9.2]. We denote the initial state distribution by â„, i.e., the probability that an episode begins in state ğ‘  is â„(ğ‘ ).
3.2. Types of Errors
We denote the average number of time steps spent in state ğ‘  in a single episode by ğœ‚(ğ‘ ). Time is spent in state ğ‘ â€² either because the episode starts there or because the policy and the environment result in the agent visiting state ğ‘ â€² coming from any preceding state ğ‘ , i.e.,
ğœ‚(ğ‘ â€²) = â„(ğ‘ â€²) + ğ›¾ âˆ« ğ’®
ğœ‚(ğ‘ )âˆ«
ğ’œ(ğ‘ )
ğœ‹(ğ‘|ğ‘ )ğ‘(ğ‘ â€²|ğ‘ ,ğ‘)dğ‘dğ‘ 
âˆ€ğ‘ â€² âˆˆ ğ’®.
Using ğœ‚, we find the probabilities
ğœ‡ğœ‹(ğ‘ ) =
âˆ« ğ’®
ğœ‚(ğ‘ ) ğœ‚(ğ‘ â€²)dğ‘ â€²
=
ğœ‚(ğ‘ ) ğ”¼ğ‘ â€²âˆ¼ğœ‚[1]
.
In continuing tasks (ğ›¾ = 1), the state distribution is the stationary dis- tribution under the policy ğœ‹ (and the environment) and therefore satisfies the equation
ğœ‡ğœ‹(ğ‘ â€²) = âˆ« ğ’® = ğ”¼ğ‘ âˆ¼ğœ‡ğœ‹
ğœ‹(ğ‘|ğ‘ )ğ‘(ğ‘ â€²|ğ‘ ,ğ‘)dğ‘dğ‘ 
ğœ‡ğœ‹(ğ‘ )âˆ«
ğ’œ(ğ‘ )
ğ”¼ğ‘âˆ¼ğœ‹(ğ‘ )[ğ‘(ğ‘ â€²|ğ‘ ,ğ‘)]
âˆ€ğ‘ â€² âˆˆ ğ’®.
Definition 3.1 (mean squared value error). The mean squared value error is the function VEâˆ¶ ğ’« Ã— â„ğ‘‘ â†’ â„+ 0 ,
VEğœ‹(w) âˆ¶= â€–Ì‚ğ‘‰ (.,w) âˆ’ ğ‘£ğœ‹(.)â€–2
2,ğœ† = ğ”¼ğ‘ âˆ¼ğœ†[(Ì‚ğ‘‰ (ğ‘ ,w) âˆ’ ğ‘£ğœ‹(ğ‘ ))2]
= âˆ« ğ’®
ğœ†(ğ‘ )(Ì‚ğ‘‰ (ğ‘ ,w) âˆ’ ğ‘£ğœ‹(ğ‘ ))2dğ‘ .
There are two options for the weight function ğœ†âˆ¶ ğ’® â†’ â„+ 0 :
1. All the weights are equal to one, i.e.,
VEğœ‹(w) = â€–Ì‚ğ‘‰ (.,w) âˆ’ ğ‘£ğœ‹(.)â€–2 2.
2. The weights are the state distribution ğœ‡ğœ‹âˆ¶ ğ’® â†’ [0,1] of the policy ğœ‹.
3.2.2 The Mean Squared Return Error
The error between the value estimate at each time and the return from that time is always observable. The mean squared return error is the mean of the square of this error with respect to the state distribution.
43
44
Chapter 3. Taxonomy of Algorithms
Definition 3.2 (mean squared return error). The mean squared return error is the function REâˆ¶ ğ’« Ã— â„ğ‘‘ â†’ â„+ 0 , ğ”¼ğœ‹[(Ì‚ğ‘‰ (ğ‘†ğ‘¡,w) âˆ’ ğºğ‘¡)2 âˆ£ ğ‘†ğ‘¡ = ğ‘ ]
REğœ‹(w) âˆ¶= ğ”¼ğ‘ âˆ¼ğœ‡ğœ‹
= âˆ« ğ’®
ğœ‡ğœ‹(ğ‘ )ğ”¼ğœ‹[(Ì‚ğ‘‰ (ğ‘†ğ‘¡,w) âˆ’ ğºğ‘¡)2 âˆ£ ğ‘†ğ‘¡ = ğ‘ ]dğ‘ .
3.2.3 Mean Squared Bellman Errors
Definition 3.3 (mean squared Bellman error). The mean squared Bellman error is the function BEâˆ¶ ğ’« Ã— â„ğ‘‘ â†’ â„+ 0 ,
BEğœ‹(w) âˆ¶= â€–ğµğœ‹
Ì‚ğ‘‰ (w) âˆ’ Ì‚ğ‘‰ (w)â€–2
2,ğœ‡ğœ‹
.
The projected version of this error depends on the projection Î  that maps
an arbitrary function ğ‘£ to the closest representable function
ğ‘£wâˆ—
âˆ¶= Î [ğ‘£],
which is determined by the approximation method used and the minimizing parameter vector
wâˆ— âˆ¶= argmin
wâˆˆâ„ğ‘‘
â€–ğ‘£w âˆ’ ğ‘£â€–2
2,ğœ‡ğœ‹
âˆˆ â„ğ‘‘.
This projection is important, because computations can obviously only be per- formed on representable functions.
Definition 3.4 (mean squared projected Bellman error). The mean squared projected Bellman error is the function BEâˆ¶ ğ’« Ã— â„ğ‘‘ â†’ â„+ 0 ,
BEğœ‹(w) âˆ¶= â€–Î [ğµğœ‹
Ì‚ğ‘‰ (w) âˆ’ Ì‚ğ‘‰ (w)]â€–2
2,ğœ‡ğœ‹
.
3.2.4 The Mean Squared Temporal-Difference Error
We start by defining the temporal-difference error
ğ›¿ğ‘¡ âˆ¶= ğ‘ˆğ‘¡(wğ‘¡) âˆ’ Ì‚ğ‘‰ (ğ‘†ğ‘¡,wğ‘¡),
where ğ‘ˆğ‘¡(wğ‘¡) is any temporal-difference update target (see Chapter 4 and Chap- ter 5). By definition, a non-bootstrapping update target does not depend on any weight vector, while a bootstrapping one does. The simplest non-bootstrapping target is the return ğºğ‘¡, which results in the Monte-Carlo temporal-difference error
ğ›¿ğ‘¡ = ğºğ‘¡ âˆ’ Ì‚ğ‘‰ (ğ‘†ğ‘¡,wğ‘¡).
3.3. Learnability
Maybe the simplest of the bootstrapping update targets is the one-step temporal- difference target ğ‘…ğ‘¡+1+ğ›¾ Ì‚ğ‘‰ (ğ‘†ğ‘¡+1,wğ‘¡), resulting in the one-step temporal-difference error
ğ›¿ğ‘¡ = ğ‘…ğ‘¡+1 + ğ›¾ Ì‚ğ‘‰ (ğ‘†ğ‘¡+1,wğ‘¡) âˆ’ Ì‚ğ‘‰ (ğ‘†ğ‘¡,wğ‘¡).
Definition 3.5 (mean squared temporal-difference error). The mean squared temporal-difference error is the function TDEâˆ¶ ğ’« Ã— â„ğ‘‘ â†’ â„+ 0 ,
TDEğœ‹(w) âˆ¶= ğ”¼ğœ‹[ğ›¿2
ğ‘¡] = ğ”¼ğ‘ âˆ¼ğœ‡ğœ‹
ğ”¼[ğ›¿2
ğ‘¡ âˆ£ ğ‘†ğ‘¡ = ğ‘ ,ğ´ğ‘¡ âˆ¼ ğœ‹]
= âˆ« ğ’®
ğœ‡ğœ‹(ğ‘ )ğ”¼[ğ›¿2
ğ‘¡ âˆ£ ğ‘†ğ‘¡ = ğ‘ ,ğ´ğ‘¡ âˆ¼ ğœ‹]dğ‘ .
In off-policy learning (see Section 2.6), the behavior policy ğ‘ differs from the
target policy ğœ‹. We can use the importance-sampling ratio
ğœŒğ‘¡ âˆ¶=
ğœ‹(ğ´ğ‘¡|ğ‘†ğ‘¡) ğ‘(ğ´ğ‘¡|ğ‘†ğ‘¡)
(cf. Section 4.3) as a correction factor. This yields
TDEğœ‹(w) = ğ”¼ğ‘[ğœŒğ‘¡ğ›¿2
ğ‘¡] = ğ”¼ğ‘ âˆ¼ğœ‡ğ‘
ğ”¼[ğœŒğ‘¡ğ›¿2
ğ‘¡ âˆ£ ğ‘†ğ‘¡ = ğ‘ ,ğ´ğ‘¡ âˆ¼ ğ‘].
3.3 Learnability
Learnability is understood in the fundamental sense answering the question whether certain values can be calculated or learned given any amount of ob- servations or experience. It does not concern the question whether it is too computationally expensive to compute certain values.
3.3.1 The Mean Squared Return Error
The mean squared return error is observable, since the return ğºğ‘¡ is observable, and hence this error is learnable.
The mean squared value error VE and the mean squared return error RE
are connected by
REğœ‹(w) = VEğœ‹(w) + ğ”¼ğ‘ âˆ¼ğœ‡ğœ‹
ğ”¼ğœ‹[(ğ‘£ğœ‹(ğ‘†ğ‘¡) âˆ’ ğºğ‘¡)2]
in the on-policy case [23, Equation (11.24)]. The last term does not depend on the parameter vector w, and therefore the two errors always have the same minimizer.
45
(3.1)
46
Chapter 3. Taxonomy of Algorithms
3.3.2 The Mean Squared Value Error
Figure 3.1 shows two different Markov decision processes that result in the same reward sequence [23, Section 11.6.]. The left process has a single state, a single action, and the reward is zero or one with probability 1/2. The right process has two states and a single action in each state. This process stays in the same state or switches to the other state with probability 1/2. The reward is zero from the first state and one from the second state, i.e., the reward is deterministic.
0
0
A
1
0
B
C
1
1
Figure 3.1: Two Markov decision processes that result in the same reward In each state, sequence, a uniformly distributed sequence of zeros and ones. there is only one possible action, which is hence not shown.
The reward sequences of both processes are identical; more precisely, they are sequences of zeros and ones, both values occurring with probability 1/2 in the sequence. Since there is only a single action available in each state, the policy is always trivial and hence not indicated in the following.
We assume that ğ›¾ = 0. Then the value of the single state in the left process is ğ‘£(A) = 1/2, and the values of the states in the right process are ğ‘£(B) = 0 and ğ‘£(C) = 1.
We assume that all states appear the same after approximating the state- value function. In the left process, the value of the single state is approximated as Ì‚ğ‘‰ (A,ğ‘¤) = ğ‘¤. In the right process, the values of the two states are approximated as Ì‚ğ‘‰ (B,ğ‘¤) = ğ‘¤ = Ì‚ğ‘‰ (C,ğ‘¤). (This is the simplest special case of linear function approximation, see Section ??. In the right process, the two states share a single feature.)
In the left process, the mean squared value error is VE(ğ‘¤) = (Ì‚ğ‘‰ (A,ğ‘¤) âˆ’
ğ‘£(A))2 = (ğ‘¤ âˆ’ 1/2)2. Its minimizer ğ‘¤âˆ— âˆ¶= 1/2 results in VE(ğ‘¤âˆ—) = 0.
In the right process, the mean squared value error is VE(ğ‘¤) = (1/2)(Ì‚ğ‘‰ (B,ğ‘¤)âˆ’ ğ‘£(B))2 +(1/2)(Ì‚ğ‘‰ (C,ğ‘¤)âˆ’ğ‘£(C))2 = (1/2)ğ‘¤2 +(1/2)(ğ‘¤âˆ’1)2 = ğ‘¤2 âˆ’ğ‘¤+1/2. Its minimizer ğ‘¤âˆ— âˆ¶= 1/2, the same as before, now results in VE(ğ‘¤âˆ—) = 1/4, which is different from before.
Of course, if the values of the states of the right process would be approx- imated as Ì‚ğ‘‰ (B,w) = ğ‘¤1 and Ì‚ğ‘‰ (C,w) = ğ‘¤2, then the mean squared value error would be VE(w) = (1/2)(Ì‚ğ‘‰ (B,w) âˆ’ ğ‘£(B))2 + (1/2)(Ì‚ğ‘‰ (C,w) âˆ’ ğ‘£(C))2 = 1 +(1/2)(ğ‘¤2 âˆ’1)2 and the minimizer would be wâˆ— = (0,1), which results (1/2)ğ‘¤2
3.3. Learnability
in VE(wâˆ—) = 0.
In order to summarize this example, there are two different Markov decision If the states cannot processes with the (statistically) same reward sequence. be observed or the approximation of the state-value function does not discern between the two states in the right process, then it is impossible to learn the mean squared value error; the error is different in the two processes, but the observations are identical.
In this sense, the mean squared value error is not learnable from the obser- vations. It is no coincidence, however, that both processes in the example share the same minimizer of the error.
If two processes share the (statistically) same reward sequence, they have the same returns and hence the same mean squared return error, which is observable (see Section 3.3.1). Because they have the same mean squared return error, the minimizer is also the same. Next, due to (3.1), the mean squared return error and the mean squared value error have the same minimizer. Therefore, we have shown that two processes that have the (statistically) same reward sequence also have the same minimizer of the mean squared value error. In this sense, the mean squared value error is a viable objective for learning.
In short, the mean squared value error is not learnable, but its minimizer is.
3.3.3 The Mean Squared Temporal-Difference Error
The so-called A-split example is shown in Figure 3.2 [23, Example 11.2]. We use the one-step temporal difference update target. It can be shown that the minimal mean squared temporal-difference error is 1/16, while the error of the (true) value function (see Figure 3.2) is 1/8, i.e., larger.
Therefore, this example implies that minimizing the mean squared temporal-
difference error is not expedient.
B
1
T
b
0
A
0
c
C
0
T
Figure 3.2: The A-split example. The policy ğœ‹ is given by ğœ‹(b|A) âˆ¶= 1/2 and ğœ‹(c|A) âˆ¶= 1/2. Its value function is ğ‘£ğœ‹(A) = 1/2, ğ‘£ğœ‹(B) = 1, and ğ‘£ğœ‹(C) = 0.
47
48
Chapter 3. Taxonomy of Algorithms
Chapter 4
Monte-Carlo Methods
Monaco managed to keep up its veneer of sophistication only through the presence of its royal family and the high prices speculators, hote- liers, restaurateurs and shopkeepers charged. Even those had not created a safe buffer against some of the more garish encroachments of the 1980s. On his last visit, Bond had been horrified to find one- armed bandits installed in the exclusive Salles PrivÃ©es of the Casino. Now he would not be surprised if there were space invader games there as well.
John Gardner, James Bond â€“ Role of Honour, Chapter 4.
In science and technology, Monte-Carlo methods approximate the stochastic un- known quantity of interest (e.g., an expected value) by generating many samples and calculating their statistics (e.g., the sample mean). In RL, the Monte-Carlo idea can be applied to the prediction and the control problem, and various algo- rithms are presented here. Convergence of the two main versions of Monte-Carlo prediction, namely first-visit and every-visit Monte-Carlo prediction, is shown, including the convergence rate of first-visit Monte-Carlo prediction.
4.1 Monte-Carlo Prediction
Prediction means learning the state-value function for a given policy ğœ‹. The Monte-Carlo (MC) idea is to estimate the state-value function ğ‘£ğœ‹ at all states ğ‘  âˆˆ ğ’® by averaging the returns obtained after the occurrences of each state ğ‘  in many episodes. There are two variants:
In first-visit MC, only the first occurrence of a state ğ‘  in an episode is used to calculate the average and hence to estimate ğ‘£ğœ‹(ğ‘ ).
49
50
Chapter 4. Monte-Carlo Methods
In every-visit MC, all occurrences of a state ğ‘  in an episode are used to calculate the average.
First-visit MC prediction is shown in Algorithm 5. In the every-visit variant,
the check towards the end whether the occurrence is the first is left out.
Algorithm 5 first/every-visit MC prediction for calculating ğ‘£ â‰ˆ ğ‘£âˆ— given the policy ğœ‹.
initialization: initialize the vector ğ‘£ of length |ğ’®| arbitrarily initialize returns(ğ‘ ) to be an empty list for all ğ‘  âˆˆ ğ’®
loop
â–· for all episodes
generate an episode (ğ‘†0,ğ´0,ğ‘…1,â€¦,ğ‘†ğ‘‡âˆ’1,ğ´ğ‘‡âˆ’1,ğ‘…ğ‘‡) following ğœ‹ ğ‘” âˆ¶= 0 for ğ‘¡ âˆˆ (ğ‘‡ âˆ’ 1,ğ‘‡ âˆ’ 2,â€¦,0) do
â–· for all time steps
ğ‘” âˆ¶= ğ›¾ğ‘” + ğ‘…ğ‘¡+1 if ğ‘†ğ‘¡ âˆ‰ {ğ‘†0,â€¦,ğ‘†ğ‘¡âˆ’1} then
â–· remove check for every-visit MC
append ğ‘” to the list returns(ğ‘†ğ‘¡) ğ‘£(ğ‘†ğ‘¡) âˆ¶= average(returns(ğ‘†ğ‘¡))
end if
end for
end loop
The converge of first-visit MC prediction to ğ‘£ğœ‹ as the number of visits goes to infinity follows from the law of large numbers, since each return is an independent and identically distributed estimate of ğ‘£ğœ‹(ğ‘ ) with finite variance. It is well-known that each average calculated in this manner is an unbiased estimate and that the standard deviation of the error is proportional to ğ‘›âˆ’1/2, where ğ‘› is the number of occurrences of the state ğ‘ .
The convergence proof for every-visit MC is more involved, since the occur-
rences are not independent.
The main advantage of MC methods is that they are simple methods. It is
always possible to generate sample episodes.
Another feature of MC methods is that the approximations of ğ‘£ğœ‹(ğ‘ ) are independent from on another. The approximation for one state does not build on or depend on the approximation for another state, i.e., MC methods do not bootstrap.
Furthermore, the computational expense is independent of the number of states. Sometimes it is possible to steer the computation to interesting states
4.2. On-Policy Monte-Carlo Control
by choosing the starting states of the episodes suitably.
One can also try to estimate the action-value function ğ‘ğœ‹ using MC. However, it is possible that many state-action pairs are never visited or only very seldomly. In other words, there may not be sufficient exploration. Sometimes it is possible to prescribe the starting state-action pairs of the episodes, which are then called exploring starts. It is then possible to remedy this problem, but it depends on the environment if this is possible or not. Exploring starts are not a general solution.
4.2 On-Policy Monte-Carlo Control
Control means to approximate optimal policies. The idea is the same as in Section 2.9, namely to iteratively perform policy evaluation and policy improve- ment. By Theorem 2.9, the sequences of value functions and policies converge to the optimal value functions and to the optimal policies under the assumption of exploring starts and under the assumption that infinitely many episodes are available.
An on-policy method is a method where the policy that is used to generate episodes is the same as the policy that is being improved. This is in contrast to off-policy methods, where these two policies are different ones (see Section 4.3). But exploring starts are not available in general. Another important way to ensure sufficient exploration (and hence convergence) is to use ğœ–-greedy policies as shown in Algorithm 6.
4.3 Off-Policy Methods and Importance Sampling
The dilemma between exploitation and exploration is a fundamental one in learning. The goal in action-value methods is to learn the correct action values, which depend on future optimal behavior. But at the same time, the algorithm must perform sufficient exploration to be able to discover optimal actions first. The on-policy MC control algorithm, Algorithm 6 in the previous section comprises by using ğœ–-greedy policies. Off-policy methods clearly distinguish between two policies:
The behavior policy ğ‘ is used to generate episodes. It is usually stochastic.
The target policy ğœ‹ is the policy that is improved. It is usually the deter- ministic greedy policy with respect to the action-value function.
51
52
Chapter 4. Monte-Carlo Methods
Algorithm 6 on-policy first-visit MC control for calculating ğœ‹ â‰ˆ ğœ‹âˆ—.
initialization: choose ğœ– âˆˆ (0,1) initialize ğœ‹ to be an ğœ–-greedy policy initialize ğ‘(ğ‘ ,ğ‘) âˆˆ â„ arbitrarily for all (ğ‘ ,ğ‘) âˆˆ ğ’® Ã— ğ’œ(ğ‘ ) initialize returns(ğ‘ ,ğ‘) to be an empty list for all (ğ‘ ,ğ‘) âˆˆ ğ’® Ã— ğ’œ(ğ‘ )
loop
â–· for all episodes
generate an episode (ğ‘†0,ğ´0,ğ‘…1,â€¦,ğ‘†ğ‘‡âˆ’1,ğ´ğ‘‡âˆ’1,ğ‘…ğ‘‡) following ğœ‹ ğ‘” âˆ¶= 0 for ğ‘¡ âˆˆ (ğ‘‡ âˆ’ 1,ğ‘‡ âˆ’ 2,â€¦,0) do
â–· for all time steps
ğ‘” âˆ¶= ğ›¾ğ‘” + ğ‘…ğ‘¡+1 if ğ‘†ğ‘¡ âˆ‰ {ğ‘†0,â€¦,ğ‘†ğ‘¡âˆ’1} then
â–· remove check for every-visit MC
append ğ‘” to the list returns(ğ‘†ğ‘¡,ğ´ğ‘¡) ğ‘(ğ‘†ğ‘¡,ğ´ğ‘¡) âˆ¶= average(returns(ğ‘†ğ‘¡,ğ´ğ‘¡)) ğ‘âˆ— âˆ¶âˆˆ argmaxğ‘âˆˆğ’œ(ğ‘ ) ğ‘(ğ‘†ğ‘¡,ğ‘) for all ğ‘ âˆˆ ğ’œ(ğ‘†ğ‘¡) do
â–· break ties randomly
ğœ‹(ğ‘|ğ‘†ğ‘¡) âˆ¶= {
1 âˆ’ ğœ– + ğœ–/|ğ’œ(ğ‘†ğ‘¡)|, ğ‘ = ğ‘âˆ—, ğ‘ â‰  ğ‘âˆ—. ğœ–/|ğ’œ(ğ‘†ğ‘¡)|,
end for
end if
end for
end loop
4.3. Off-Policy Methods and Importance Sampling
The behavior and the target policies must satisfy the assumption of coverage, i.e., that ğœ‹(ğ‘|ğ‘ ) > 0 implies ğ‘(ğ‘|ğ‘ ) > 0. In other words, every action that the target policy ğœ‹ performs must also be performed by the behavior policy ğ‘.
The by far most common technique used in off-policy methods is importance sampling. Importance sampling is a general technique that makes it possible to estimate the expected value under one probability distribution by using samples from another distribution. In off-policy methods it is of course used to adjust the returns from the behavior to the target policy.
We start by considering the probability
â„™{ğ´ğ‘¡,ğ‘†ğ‘¡+1,ğ´ğ‘¡+1,â€¦,ğ‘†ğ‘‡ âˆ£ ğ‘†ğ‘¡, ğ´ğ‘¡âˆ¶ğ‘‡âˆ’1 âˆ¼ ğœ‹} =
ğ‘‡âˆ’1 âˆ ğ‘˜=ğ‘¡
ğœ‹(ğ´ğ‘˜|ğ‘†ğ‘˜)ğ‘(ğ‘†ğ‘˜+1 âˆ£ ğ‘†ğ‘˜,ğ´ğ‘˜)
that a trajectory (ğ´ğ‘¡,ğ‘†ğ‘¡+1,ğ´ğ‘¡+1,â€¦,ğ‘†ğ‘‡) occurs after starting from state ğ‘†ğ‘¡ and following policy ğœ‹. Then the relative probability
ğœŒğ‘¡âˆ¶ğ‘‡âˆ’1 âˆ¶=
âˆğ‘‡âˆ’1 âˆğ‘‡âˆ’1
ğ‘˜=ğ‘¡ ğœ‹(ğ´ğ‘˜|ğ‘†ğ‘˜)ğ‘(ğ‘†ğ‘˜+1 âˆ£ ğ‘†ğ‘˜,ğ´ğ‘˜) ğ‘˜=ğ‘¡ ğ‘(ğ´ğ‘˜|ğ‘†ğ‘˜)ğ‘(ğ‘†ğ‘˜+1 âˆ£ ğ‘†ğ‘˜,ğ´ğ‘˜)
=
âˆğ‘‡âˆ’1 âˆğ‘‡âˆ’1
ğ‘˜=ğ‘¡ ğœ‹(ğ´ğ‘˜|ğ‘†ğ‘˜) ğ‘˜=ğ‘¡ ğ‘(ğ´ğ‘˜|ğ‘†ğ‘˜)
of this trajectory under the target and behavior policies is called the importance- sampling ratio. Fortunately, the transition probabilities of the MDP cancel.
Now the importance-sampling ratio makes it possible to adjust the returns ğºğ‘¡ from the behavior policy ğ‘ to the target policy ğœ‹. We are not interested in the state-value function
ğ‘£ğ‘(ğ‘ ) = ğ”¼[ğºğ‘¡ âˆ£ ğ‘†ğ‘¡ = ğ‘ ]
of ğ‘, but in the state-value function
ğ‘£ğœ‹(ğ‘ ) = ğ”¼[ğœŒğ‘¡âˆ¶ğ‘‡âˆ’1ğºğ‘¡ âˆ£ ğ‘†ğ‘¡ = ğ‘ ]
of ğœ‹.
Within a MC method, this means that we calculate averages of these adjusted returns. There are two variants of averages that are used for this purpose. First, we define some notation. It is convenient to number all time steps across all episodes consecutively. We denote the set of all time steps when state ğ‘  occurs by ğ’¯(ğ‘ ), the first time the termination state is reached after time ğ‘¡ by ğ‘‡(ğ‘¡), and the return after time ğ‘¡ till the end of the episode at time ğ‘‡(ğ‘¡) again by ğºğ‘¡. Then the set {ğºğ‘¡}ğ‘¡âˆˆğ’¯(ğ‘ ) contains all returns after visiting state ğ‘  and the set {ğœŒğ‘¡âˆ¶ğ‘‡(ğ‘¡)âˆ’1}ğ‘¡âˆˆğ’¯(ğ‘ ) contains the important-sampling ratios.
The first variant is called ordinary importance sampling. It is the mean of
all adjusted returns ğœŒğ‘¡âˆ¶ğ‘‡âˆ’1ğºğ‘¡, i.e.,
ğ‘‰o(ğ‘ ) âˆ¶=
âˆ‘ğ‘¡âˆˆğ’¯(ğ‘ ) ğœŒğ‘¡âˆ¶ğ‘‡âˆ’1ğºğ‘¡ |ğ’¯(ğ‘ )|
.
53
54
Chapter 4. Monte-Carlo Methods
In the second variant, the factors ğœŒğ‘¡âˆ¶ğ‘‡âˆ’1 are interpreted as weights, and the weighted mean
ğ‘‰w(ğ‘ ) âˆ¶=
âˆ‘ğ‘¡âˆˆğ’¯(ğ‘ ) ğœŒğ‘¡âˆ¶ğ‘‡âˆ’1ğºğ‘¡ âˆ‘ğ‘¡âˆˆğ’¯(ğ‘ ) ğœŒğ‘¡âˆ¶ğ‘‡âˆ’1
is used. It is defined to be zero if the denominator is zero. This variant is called weighted importance sampling.
Although the weighted-average estimate has expectation ğ‘£ğ‘(ğ‘ ) rather than the desired ğ‘£ğœ‹(ğ‘ ), it is much preferred in practice because it is much lower variance. On the other hand, ordinary importance sampling is easier to extend to approximate methods.
4.4 Convergence of First-Visit Monte-Carlo Predic-
tion
In first-visit Monte Carlo, only the first visit to a state in every episode is used to estimate its value, while in every-visit Monte Carlo, all visits to a state in In the first-visit the episodes are used for calculating the value of the state. case, the theory is more straightforward due to the independence of the states in separate episodes, while an implementation needs to check in every episode whether a state has already been visited. First-visit Monte Carlo also generally requires more episodes to achieve the same confidence in estimating the value function, rendering it less data efficient.
Both first-visit and every-visit Monte-Carlo converge to the true value func- tion. In this section, the convergence result for the first-visit Monte-Carlo pre- diction algorithm is stated and proved.
Theorem 4.1 (convergence of first-visit variant of Algorithm 5). Suppose that all episodes consist of a finite number of iterations, that the rewards are bounded, and that each state is visited an infinite number of times. Denote the number of returns used to calculate the sample mean ğ‘‰ğ‘› of the value function in the first-visit variant of Algorithm 5 by ğ‘›. Then the sample mean ğ‘‰ğ‘› converges to the correct value function ğ‘£ğœ‹ for each state for a given policy ğœ‹ in distribution, more precisely,
âˆ€ğ‘  âˆˆ ğ’®âˆ¶
âˆƒğœğ‘  âˆˆ â„+âˆ¶
âˆš
ğ‘›(ğ‘‰ğ‘›(ğ‘ ) âˆ’ ğ‘£ğœ‹(ğ‘ ))
d âˆ’âˆ’âŸ¶ ğ‘›â†’âˆ
ğ‘(0,ğœ2
ğ‘ ).
Proof. The returns obtained in each state ğ‘  by following the policy ğœ‹ are stored in the algorithm, and then their sample mean ğ‘‰ğ‘›(ğ‘ ) is used as the estimate of the correct state value ğ‘£ğœ‹(ğ‘ ). Since only the first visit of any state is used in
4.5. Convergence of Every-Visit Monte-Carlo Prediction
each episode, each return is an independent and identically distributed random variable. Their expected values and their variances are finite, since the rewards are bounded by assumption. Therefore, by the central limit theorem, Theo- rem B.68, the sample means converge in distribution and the error decays as 1/
âˆš
ğ‘› for each state ğ‘  âˆˆ ğ’®.
In summary, the proof is a straightforward application of the central limit theorem, which is proved in Chapter B together with the law of large numbers.
4.5 Convergence of Every-Visit Monte-Carlo Pre-
diction
In this section, the convergence result for the every-visit Monte-Carlo prediction algorithm is stated and proved.
Theorem 4.2 (convergence of every-visit variant of Algorithm 5). Suppose that all episodes consist of a finite number of iterations, that the rewards are bounded, and that each state is visited an infinite number of times. Denote the number of returns used to calculate the sample mean ğ‘‰ğ‘› of the value function in the every-visit variant of Algorithm 5 by ğ‘›. Then the sample mean ğ‘‰ğ‘› almost surely converges to the correct value function ğ‘£ğœ‹ for a given policy ğœ‹, more precisely,
âˆ€ğ‘  âˆˆ ğ’®âˆ¶
ğ‘‰ğ‘›(ğ‘ )
a. s. âˆ’âˆ’âŸ¶ ğ‘›â†’âˆ
ğ‘£ğœ‹(ğ‘ ).
The proof follows [36, Section 5.2].
Proof. We denote the integer-valued random variable that yields the number of visits to state ğ‘  âˆˆ ğ’® in the ğ‘˜-th episode by ğ‘ğ‘ ,ğ‘˜, and the ğ‘ğ‘ ,ğ‘˜ samples of the return generated in the ğ‘˜-th episode are denoted by ğ‘…(ğ‘ ,ğ‘˜,ğ‘š), ğ‘š âˆˆ {1,â€¦,ğ‘ğ‘ ,ğ‘˜}. Conditioned on ğ‘ğ‘ ,ğ‘˜ â‰¥ 1, the random variables ğ‘ğ‘ ,ğ‘˜ are non-negative, inde- pendent, and identically distributed. They are independent because the episodes are independent and they are identically distributed by the Markov property of the environment. By the same reasons, the random variables âˆ‘ğ‘ğ‘ ,ğ‘˜ ğ‘š=1 ğ‘…(ğ‘ ,ğ‘˜,ğ‘š) conditioned on ğ‘ğ‘ ,ğ‘˜ â‰¥ 1 are also independent and identically distributed for different episodes ğ‘˜.
We denote the number of times that state ğ‘  has been visited in all episodes by ğ‘›ğ‘ , and the total number of visits to any state in all episodes by ğ‘›. By assumption, all ğ‘›ğ‘ , ğ‘  âˆˆ ğ’®, go to infinity as ğ‘› â†’ âˆ. The algorithm calculates âˆ‘{ğ‘˜âˆˆâ„•âˆ¶ğ‘ğ‘ ,ğ‘˜â‰¥1} âˆ‘ğ‘ğ‘ ,ğ‘˜
ğ‘š=1 ğ‘…(ğ‘ ,ğ‘˜,ğ‘š)
(ğ‘ ) =
ğ‘‰ğ‘›ğ‘ 
,
âˆ‘{ğ‘˜âˆˆâ„•âˆ¶ğ‘ğ‘ ,ğ‘˜â‰¥1} ğ‘ğ‘ ,ğ‘˜
55
56
Chapter 4. Monte-Carlo Methods
where the nominator is the sum of all returns received in state ğ‘  and the de- nominator is the number of all visits to state ğ‘  up to that point. The quotient can be rewritten as
ğ‘‰ğ‘›ğ‘ 
(ğ‘ ) =
1 ğ‘›ğ‘ 
âˆ‘{ğ‘˜âˆˆâ„•âˆ¶ğ‘ğ‘ ,ğ‘˜â‰¥1} âˆ‘ğ‘ğ‘ ,ğ‘˜
ğ‘š=1 ğ‘…(ğ‘ ,ğ‘˜,ğ‘š)
1 ğ‘›ğ‘ 
âˆ‘{ğ‘˜âˆˆâ„•âˆ¶ğ‘ğ‘ ,ğ‘˜â‰¥1} ğ‘ğ‘ ,ğ‘˜
.
By the strong law of large numbers, Theorem B.67, applied to the nominator
and the denominator, we have
ğ‘‰ğ‘›ğ‘ 
(ğ‘ )
a. s. âˆ’âˆ’âˆ’âŸ¶ ğ‘›ğ‘ â†’âˆ
ğ”¼[âˆ‘ğ‘ğ‘ ,ğ‘˜
ğ‘š=1 ğ‘…(ğ‘ ,ğ‘˜,ğ‘š) âˆ£ ğ‘ğ‘ ,ğ‘˜ â‰¥ 1] ğ”¼[ğ‘ğ‘ ,ğ‘˜ âˆ£ ğ‘ğ‘ ,ğ‘˜ â‰¥ 1]
.
By Waldâ€™s equation, Theorem B.69, the quotient is equal to
ğ”¼[âˆ‘ğ‘ğ‘ ,ğ‘˜
ğ‘š=1 ğ‘…(ğ‘ ,ğ‘˜,ğ‘š) âˆ£ ğ‘ğ‘ ,ğ‘˜ â‰¥ 1] ğ”¼[ğ‘ğ‘ ,ğ‘˜ âˆ£ ğ‘ğ‘ ,ğ‘˜ â‰¥ 1]
= ğ”¼[ğ‘…(ğ‘ ,ğ‘˜,1) âˆ£ ğ‘ğ‘ ,ğ‘˜ â‰¥ 1],
which is equal to ğ‘£ğœ‹(ğ‘ ) by the definition of the state-value function ğ‘£ğœ‹.
In summary, we have shown that
ğ‘‰ğ‘›(ğ‘ )
a. s. âˆ’âˆ’âŸ¶ ğ‘›â†’âˆ
ğ‘£ğœ‹(ğ‘ ),
which concludes the proof.
4.6 Bibliographical and Historical Remarks
Further investigations into the convergence properties of first- and every-visit Monte Carlo can be found in [37].
4.7 Exercises
Exercise 4.1 (off-policy MC control (programming)). Implement off-policy MC control and use it to estimate ğœ‹âˆ— for the grid world in Exercise 2.3.
Exercise 4.2 (recursive formula for weighted importance sampling for off-policy learning). In weighted importance sampling, we calculate the estimate
ğ‘‰ ğœ‹ ğ‘›+1 âˆ¶=
âˆ‘ğ‘› ğ‘˜=1 ğ‘Šğ‘˜ğºğ‘˜ âˆ‘ğ‘› ğ‘˜=1 ğ‘Šğ‘˜
,
ğ‘› â‰¥ 1,
4.7. Exercises
given the returns ğº1,ğº2,â€¦ and the weights ğ‘Š1,ğ‘Š2,â€¦.
Show that the iteration
ğ‘‰ğ‘›+1 âˆ¶= ğ‘‰ğ‘› +
ğ‘Šğ‘› ğ¶ğ‘›
(ğºğ‘› âˆ’ ğ‘‰ğ‘›),
ğ‘› â‰¥ 1,
where
ğ¶0 = 0,
ğ¶ğ‘›+1 âˆ¶= ğ¶ğ‘› + ğ‘Šğ‘›+1,
ğ‘› â‰¥ 0,
yields the same values ğ‘‰ ğœ‹
ğ‘› = ğ‘‰ğ‘› for all ğ‘› âˆˆ {2,3,â€¦}.
Hint: Follow the derivation of the formula ğ‘„ğ‘›+1 = ğ‘„ğ‘› + (1/ğ‘›)(ğ‘…ğ‘› âˆ’ ğ‘„ğ‘›).
Exercise 4.3 (importance-sampling ratio in off-policy MC control). In the off- policy MC-control algorithm in [23, Section 5.7], the importance-sampling ratio is updated according to
ğ‘Š âˆ¶=
ğ‘Š ğ‘(ğ´ğ‘¡|ğ‘†ğ‘¡)
,
although the definition of the importance-sampling ratio ğœŒğ‘¡âˆ¶ğ‘‡âˆ’1 implies
ğ‘Š âˆ¶=
ğœ‹(ğ´ğ‘¡|ğ‘†ğ‘¡) ğ‘(ğ´ğ‘¡|ğ‘†ğ‘¡)
ğ‘Š.
Why is the update in the algorithm nevertheless correct?
57
58
Chapter 4. Monte-Carlo Methods
Chapter 5
Temporal-Difference Learning
In a number of important situations (in fact, in all decision-making situations), we face the problem of making decisions without a full knowledge of the basic workings of the underlying system. [â€¦] It is necessary to learn and act at the same time. [â€¦] Not only do we have to decide upon the allocation of time and other resources to the control activity; we also must decide how much time and effort to devote to studying the intrinsic nature of the system.
Richard Bellman [38, p. 36].
5.1 Introduction
Temporal-difference (TD) methods are at the core of reinforcement learning. TD methods combine the advantages of dynamic programming (see Chapter 2) and MC (see Chapter 4). TD methods do not require any knowledge of the dynamics of the environments in contrast to dynamic programming, which requires full knowledge. The disadvantage of MC methods is that the end of an episode must be reached before any updates are performed; in TD updates are performed immediately or much earlier based on approximations learned earlier, i.e., they bootstrap approximations from previous approximations.
5.2 On-Policy Temporal-Difference Prediction: TD(0)
In prediction, an approximation ğ‘‰ of the state-value function ğ‘£ğœ‹ is calculated for a given policy ğœ‹ âˆˆ ğ’«. The simplest TD method is called TD(0) or one-step
59
60
Chapter 5. Temporal-Difference Learning
TD. It performs the update
ğ‘‰ (ğ‘†ğ‘¡) âˆ¶= ğ‘‰ (ğ‘†ğ‘¡) + ğ›¼(ğ‘…ğ‘¡+1 + ğ›¾ğ‘‰ (ğ‘†ğ‘¡+1) âˆ’ ğ‘‰ (ğ‘†ğ‘¡))
(5.1)
after having received the reward ğ‘…ğ‘¡+1. Note that this equation has the form (2.1) with ğ‘…ğ‘¡+1 + ğ›¾ğ‘‰ (ğ‘†ğ‘¡+1) being the target value. The new approximation on the left side is based on the previous approximation on the right side. Therefore this method is a bootstrapping method.
The algorithm based on this update is shown in Algorithm 7.
Algorithm 7 TD(0) for calculating ğ‘‰ â‰ˆ ğ‘£ğœ‹ given ğœ‹.
initialization: choose learning rate ğ›¼ âˆˆ (0,1] initialize the vector ğ‘£ of length |ğ’®| arbitrarily
except that the value of the terminal state is 0
loop
â–· for all episodes
initialize ğ‘  repeat
â–· for all time steps
set ğ‘ to be the action given by ğœ‹ for ğ‘  take action ğ‘ and receive the new state ğ‘ â€² and the reward ğ‘Ÿ ğ‘£[ğ‘ ] âˆ¶= ğ‘£[ğ‘ ] + ğ›¼(ğ‘Ÿ + ğ›¾ğ‘£[ğ‘ â€²] âˆ’ ğ‘£[ğ‘ ]) ğ‘  âˆ¶= ğ‘ â€²
until ğ‘  is the terminal state and the episode is finished
end loop
From Chapter 2, we know about the state-value function that
ğ‘£ğœ‹(ğ‘ ) = ğ”¼ğœ‹[ğºğ‘¡ âˆ£ ğ‘†ğ‘¡ = ğ‘ ]
(5.2a)
= ğ”¼ğœ‹[ğ‘…ğ‘¡+1 + ğ›¾ğºğ‘¡+1 âˆ£ ğ‘†ğ‘¡ = ğ‘ ] = ğ”¼ğœ‹[ğ‘…ğ‘¡+1 + ğ›¾ğ‘£ğœ‹(ğ‘†ğ‘¡+1) âˆ£ ğ‘†ğ‘¡ = ğ‘ ].
(5.2b)
(5.2c)
These equations help us to interpret the differences between dynamic program- ming, MC, and TD. Considering the target values in the updates, the target in dynamic programming is an estimate because ğ‘£ğœ‹(ğ‘†ğ‘¡+1) in (5.2c) is unknown and the previous approximation ğ‘‰ (ğ‘†ğ‘¡+1) is used instead. The target in MC is an estimate, because the expected value in (5.2a) is unknown and a sample of the return is used instead. The target in TD is an estimate because of both reasons: ğ‘£ğœ‹(ğ‘†ğ‘¡+1) in (5.2c) is replaced by the previous approximation ğ‘‰ (ğ‘†ğ‘¡+1) and the expected value in (5.2c) is replaced by a sample of the return.
5.3. On-Policy Temporal-Difference Control: SARSA
61
One advantage of TD methods is the fact that they do not require any knowledge of the environment just like MC methods. The advantage of TD methods over MC methods is that they perform an update immediately after having received a reward (or after some time steps in multi-step methods) instead of waiting till the end of the episode.
However, TD methods employ two approximations as discussed above. Do they still converge? The answer is yes. TD(0) converges to ğ‘£ğœ‹ (for given ğœ‹) in the mean if the learning rate is constant and sufficiently small. It converges with probability one if the learning rate satisfies the stochastic-approximation conditions (2.2).
It has not been possible so far to show stringent results about the which method, MC or TD, converges faster. However, it has been found empirically that TD methods usually converge faster than MC methods with constant learn- ing rates when the environment is stochastic.
5.3 On-Policy Temporal-Difference Control: SARSA
If we can approximate the optimal action-value function ğ‘âˆ—, we can immediately find the best action argmaxğ‘âˆˆğ’œ ğ‘(ğ‘ ,ğ‘) in state ğ‘  whenever the MDP is finite. In order to solve control problems, i.e., to calculate optimal policies, it therefore suffices to approximate the action-value function. In order to do so, we replace ğ‘‰ in (5.1) by the approximation ğ‘„ of the action-value function ğ‘ğœ‹ to find the update
ğ‘„(ğ‘†ğ‘¡,ğ´ğ‘¡) âˆ¶= ğ‘„(ğ‘†ğ‘¡,ğ´ğ‘¡) + ğ›¼(ğ‘…ğ‘¡+1 + ğ›¾ğ‘„(ğ‘†ğ‘¡+1,ğ´ğ‘¡+1) âˆ’ ğ‘„(ğ‘†ğ‘¡,ğ´ğ‘¡)).
(5.3)
This method is called SARSA due to the appearance of the values ğ‘†ğ‘¡, ğ´ğ‘¡, ğ‘…ğ‘¡+1, ğ‘†ğ‘¡+1, and ğ´ğ‘¡+1.
The corresponding control algorithm is shown in Algorithm 8. SARSA converges to an optimal policy and action-value function with prob- ability one if all state-action pairs are visited an infinite number of times and the policy converges to the greedy policy. The convergence of the policy to the greedy policy can be ensured by using an ğœ–-greedy policy and ğœ–ğ‘¡ â†’ 0.
5.4 On-Policy Temporal-Difference Control: Expected
SARSA
Expected SARSA is derived from SARSA by replacing the target ğ‘…ğ‘¡+1+ğ›¾ğ‘„(ğ‘†ğ‘¡+1,ğ´ğ‘¡+1) in the update by
ğ‘…ğ‘¡+1 + ğ›¾ğ”¼ğœ‹[ğ‘„(ğ‘†ğ‘¡+1,ğ´ğ‘¡+1) âˆ£ ğ‘†ğ‘¡+1].
62
Chapter 5. Temporal-Difference Learning
Algorithm 8 SARSA for calculating ğ‘„ â‰ˆ ğ‘âˆ— and ğœ‹âˆ—.
initialization: choose learning rate ğ›¼ âˆˆ (0,1] choose ğœ– > 0 initialize ğ‘(ğ‘ ,ğ‘) âˆˆ â„ arbitrarily for all (ğ‘ ,ğ‘) âˆˆ ğ’® Ã— ğ’œ(ğ‘ ) except that the value of the terminal state is 0
loop
â–· for all episodes
initialize ğ‘  choose action ğ‘ from ğ‘  using an (ğœ–-greedy) policy derived from ğ‘ repeat
â–· for all time steps
take action ğ‘ and receive the new state ğ‘ â€² and the reward ğ‘Ÿ choose action ğ‘â€² from ğ‘ â€² using an (ğœ–-greedy) policy derived from ğ‘ ğ‘[ğ‘ ,ğ‘] âˆ¶= ğ‘[ğ‘ ,ğ‘] + ğ›¼(ğ‘Ÿ + ğ›¾ğ‘[ğ‘ â€²,ğ‘â€²] âˆ’ ğ‘[ğ‘ ,ğ‘]) ğ‘  âˆ¶= ğ‘ â€² ğ‘ âˆ¶= ğ‘â€²
until ğ‘  is the terminal state and the episode is finished
end loop
This means that the updates in expected SARSA moves in a deterministic man- ner into the same direction as the updates in SARSA move in expectation.
The update in expected SARSA hence is
ğ‘„(ğ‘†ğ‘¡,ğ´ğ‘¡) âˆ¶= ğ‘„(ğ‘†ğ‘¡,ğ´ğ‘¡) + ğ›¼(ğ‘…ğ‘¡+1 + ğ›¾ğ”¼ğœ‹[ğ‘„(ğ‘†ğ‘¡+1,ğ´ğ‘¡+1) âˆ£ ğ‘†ğ‘¡+1] âˆ’ ğ‘„(ğ‘†ğ‘¡,ğ´ğ‘¡))
= ğ‘„(ğ‘†ğ‘¡,ğ´ğ‘¡) + ğ›¼(ğ‘…ğ‘¡+1 + ğ›¾ âˆ‘ ğ‘âˆˆğ’œ(ğ‘ )
ğœ‹(ğ‘|ğ‘†ğ‘¡+1)ğ‘„(ğ‘†ğ‘¡+1,ğ‘) âˆ’ ğ‘„(ğ‘†ğ‘¡,ğ´ğ‘¡)).
Each update is more computationally expense than an update in SARSA. The advantage, however, is that the variance that is introduced due to the random- ness in ğ´ğ‘¡ is reduced.
5.5 Off-Policy Temporal-Difference Control: Q-Learning
One of the mainstays in reinforcement learning is ğ‘„-learning, which is an off- policy TD control algorithm [39]. Its update is
ğ‘„(ğ‘†ğ‘¡,ğ´ğ‘¡) âˆ¶= ğ‘„(ğ‘†ğ‘¡,ğ´ğ‘¡) + ğ›¼(ğ‘…ğ‘¡+1 + ğ›¾ max
ğ‘âˆˆğ’œ(ğ‘†ğ‘¡+1)
ğ‘„(ğ‘†ğ‘¡+1,ğ‘) âˆ’ ğ‘„(ğ‘†ğ‘¡,ğ´ğ‘¡)).
Using this update, the approximation ğ‘„ approximates ğ‘âˆ— directly independently of the policy followed. Therefore, it is an off-policy method.
5.6. Double Q-Learning
63
The policy that is being following still influences convergence and conver- gence speed. In particular, it must be ensured that all state-action pairs occur an infinite number of times. But this is a reasonable assumption, because their action values cannot be updated without visiting them.
The corresponding algorithm is shown in Algorithm 9.
Algorithm 9 ğ‘„-learning for calculating ğ‘„ â‰ˆ ğ‘âˆ— and ğœ‹ â‰ˆ ğœ‹âˆ—.
initialization: choose learning rate ğ›¼ âˆˆ (0,1] choose ğœ– > 0 initialize ğ‘(ğ‘ ,ğ‘) âˆˆ â„ arbitrarily for all (ğ‘ ,ğ‘) âˆˆ ğ’® Ã— ğ’œ(ğ‘ ) except that the value of the terminal state is 0
loop
â–· for all episodes
initialize ğ‘  repeat
â–· for all time steps
choose action ğ‘ from ğ‘  using an (ğœ–-greedy) policy derived from ğ‘ take action ğ‘ and receive the new state ğ‘ â€² and the reward ğ‘Ÿ ğ‘[ğ‘ ,ğ‘] âˆ¶= ğ‘[ğ‘ ,ğ‘] + ğ›¼(ğ‘Ÿ + ğ›¾ maxğ‘âˆˆğ’œ(ğ‘ â€²) ğ‘[ğ‘ â€²,ğ‘] âˆ’ ğ‘[ğ‘ ,ğ‘]) ğ‘  âˆ¶= ğ‘ â€²
until ğ‘  is the terminal state and the episode is finished
end loop
Variants of ğ‘„-learning are presented in the following, and convergence results
for ğ‘„-learning are given in Chapter 6.
5.6 Double Q-Learning
Since the target value in ğ‘„-learning involves the maximum over the estimate used for bootstrapping, a significant positive bias is introduced. It is often call a maximization bias.
How can the maximization bias be overcome? One method is called double learning. We note that having obtained one sample, the regular algorithm, Algorithm 9, uses it both to determine the maximizing action and to estimate its value. The idea of double learning is to partition the samples into two sets and to use them to learn two independent estimates ğ‘„1 and ğ‘„2. One estimate, say ğ‘„1, is used to find the maximizing action
ğ‘âˆ— âˆ¶= argmax
ğ‘„1(ğ‘ ,ğ‘);
ğ‘âˆˆğ’œ
64
Chapter 5. Temporal-Difference Learning
the other estimate ğ‘„2 is used to estimate its value
ğ‘„2(ğ‘ ,ğ‘âˆ—) = ğ‘„2(ğ‘ ,argmax
ğ‘„1(ğ‘ ,ğ‘)).
ğ‘âˆˆğ’œ
Then this estimate is unbiased in the sense that
ğ”¼[ğ‘„2(ğ‘ ,ğ‘âˆ—)] = ğ‘(ğ‘ ,ğ‘âˆ—).
The roles of ğ‘„1 and ğ‘„2 can of course then be switched.
In double learning, two estimates are calculated, which doubles the memory requirement. In each iteration, only one of the two approximations is updated so that the amount of computation per iteration remains the same.
The update for ğ‘„1 is
ğ‘„1(ğ‘†ğ‘¡,ğ´ğ‘¡) âˆ¶= ğ‘„1(ğ‘†ğ‘¡,ğ´ğ‘¡)+ğ›¼ğ‘¡(ğ‘…ğ‘¡+1+ğ›¾ğ‘„2(ğ‘†ğ‘¡+1, argmax ğ‘âˆˆğ’œ(ğ‘†ğ‘¡+1)
ğ‘„1(ğ‘†ğ‘¡+1,ğ‘))âˆ’ğ‘„1(ğ‘†ğ‘¡,ğ´ğ‘¡)),
and the indices are switched in the update for ğ‘„2.
In a double-learning algorithm, the choice whether ğ‘„1 or ğ‘„2 is updated is usually performed randomly. The resulting algorithm is shown in Algorithm 10.
5.7 Deep Q-Learning
In deep ğ‘„-learning, the optimal action-state function is represented by an arti- ficial neural network. For example, in [13], a deep convolutional neural network was used. Convolutional neural networks are especially well suited for inputs that are images, which is the case in [13]. In [13], the state is the input of the neural network and there is a separate output neuron for each possible action. This has the advantage that the action-value function can be evaluated for all action in one forward pass.
Since artificial neural networks are nonlinear function approximators, con- vergence results are hard to obtain. In order to increase convergence speed or to ensure convergence at all, experience replay is usually used (cf. Remark 8.4 and Remark 6.8).
In [13], a separate neural network was used to generate the episodes. At regular intervals, i.e., after a fixed number of updates, the neural network was copied and this fixed copy was used to generate the next updates. This makes the algorithm more stable, since oscillations are prevented. In the Atari 2600 games played in [13], two consecutive states ğ‘†ğ‘¡ and ğ‘†ğ‘¡+1 are highly correlated hence oscillations are likely.
The algorithm in [13] is summarized in Algorithm 11.
5.7. Deep Q-Learning
65
Algorithm 10 double ğ‘„-learning for calculating ğ‘„ â‰ˆ ğ‘âˆ— and ğœ‹ â‰ˆ ğœ‹âˆ—.
initialization: choose learning rate ğ›¼ âˆˆ (0,1] choose ğœ– > 0 initialize ğ‘„1[ğ‘ ,ğ‘] âˆˆ â„ and ğ‘„2[ğ‘ ,ğ‘] arbitrarily for all (ğ‘ ,ğ‘) âˆˆ ğ’® Ã— ğ’œ(ğ‘ )
except that the value of the terminal state is 0
loop
â–· for all episodes
initialize ğ‘  repeat
â–· for all time steps choose action ğ‘ from ğ‘  using an (ğœ–-greedy) policy derived from ğ‘„ âˆ¶=
(ğ‘„1 + ğ‘„2)/2
take action ğ‘ and receive the new state ğ‘ â€² and the reward ğ‘Ÿ if a random number chosen uniformly in [0,1) is less than 1/2 then
ğ‘„1[ğ‘ ,ğ‘]
âˆ¶= ğ‘„1[ğ‘ ,ğ‘] + ğ›¼(ğ‘Ÿ + ğ›¾ğ‘„2[ğ‘ â€²,argmaxğ‘â€²âˆˆğ’œ(ğ‘ â€²) ğ‘„1[ğ‘ â€²,ğ‘â€²)] âˆ’
ğ‘„1[ğ‘ ,ğ‘])
else
ğ‘„2[ğ‘ ,ğ‘]
âˆ¶= ğ‘„2[ğ‘ ,ğ‘] + ğ›¼(ğ‘Ÿ + ğ›¾ğ‘„1[ğ‘ â€²,argmaxğ‘â€²âˆˆğ’œ(ğ‘ â€²) ğ‘„2[ğ‘ â€²,ğ‘â€²)] âˆ’
ğ‘„2[ğ‘ ,ğ‘])
end if ğ‘  âˆ¶= ğ‘ â€²
until ğ‘  is the terminal state and the episode is finished
end loop
66
Chapter 5. Temporal-Difference Learning
Algorithm 11 deep ğ‘„-learning for calculating ğ‘„ â‰ˆ ğ‘âˆ—.
initialization: initialize the replay memory ğ‘€ initialize action-value function ğ‘„ğœƒ with weights ğœƒ randomly initialize target action-value function Ì‚ğ‘„ğœƒ with weights ğœƒ randomly initialize ğ‘ âˆˆ â„•
loop
â–· for all episodes
initialize ğ‘  repeat
â–· for all time steps
choose action ğ‘ from ğ‘  using an (ğœ–-greedy) policy derived from ğ‘„ğœƒ take action ğ‘ and receive the new state ğ‘ â€² and the reward ğ‘Ÿ store the transition (ğ‘ ,ğ‘,ğ‘Ÿ,ğ‘ â€²) in the replay memory ğ‘€ sample a random minibatch of transitions (ğ‘ ğ‘–,ğ‘ğ‘–,ğ‘Ÿğ‘–+1,ğ‘ ğ‘–+1) from ğ‘€ set the targets
ğ‘¦ğ‘– âˆ¶= {
ğ‘Ÿğ‘–, ğ‘Ÿğ‘– + ğ›¾ maxğ‘â€²âˆˆğ’œ
Ì‚ğ‘„ğœƒ(ğ‘ ğ‘–+1,ğ‘â€²),
if ğ‘ ğ‘– is terminal state, otherwise
perform gradient descent step on (ğ‘¦ğ‘– âˆ’ ğ‘„ğœƒ(ğ‘ ğ‘–,ğ‘ğ‘–))2 every ğ‘ steps reset Ì‚ğ‘„ âˆ¶= ğ‘„ ğ‘  âˆ¶= ğ‘ â€²
until ğ‘  is the terminal state and the episode is finished
end loop
5.8. On-Policy Multi-Step Temporal-Difference Prediction: ğ‘›-step TD
5.8 On-Policy Multi-Step Temporal-Difference Pre-
diction: ğ‘›-step TD
The idea of multi-step temporal-difference methods is to perform not only one time step as in the methods in this chapter so far, but to perform multiple time steps and use the accumulated rewards as the target in the update. The multi-step methods will still be bootstrapping methods, of course.
Therefore we start by defining the return over ğ‘› time steps being based on
the state-value function afterwards.
Definition 5.1 (ğ‘›-step return (using ğ‘‰ )). The ğ‘›-step return using ğ‘‰ is defined as
ğºğ‘¡âˆ¶ğ‘¡+ğ‘› âˆ¶= {
ğ‘…ğ‘¡+1 + ğ›¾ğ‘…ğ‘¡+2 + â‹¯ + ğ›¾ğ‘›âˆ’1ğ‘…ğ‘¡+ğ‘› + ğ›¾ğ‘›ğ‘‰ğ‘¡+ğ‘›âˆ’1(ğ‘†ğ‘¡+ğ‘›), ğºğ‘¡,
ğ‘¡ + ğ‘› < ğ‘‡, ğ‘¡ + ğ‘› â‰¥ ğ‘‡
for all ğ‘› â‰¥ 1.
In the second case, when the time ğ‘¡ + ğ‘› is equal to the last time ğ‘‡ in an episode or larger, the ğ‘›-step return ğºğ‘¡âˆ¶ğ‘¡+ğ‘› is equal to the return ğºğ‘¡, which was defined to include all rewards up to the end of an episode.
Of course, the ğ‘›-step return ğºğ‘¡âˆ¶ğ‘¡+ğ‘› will only be available after having received the reward ğ‘…ğ‘¡+ğ‘› in time step ğ‘¡ + ğ‘›. Hence no updates are possible in the first ğ‘› âˆ’ 1 time steps of each episode. We also have to be careful when indexing the approximations ğ‘‰ğ‘¡. Approximation ğ‘‰ğ‘¡+ğ‘› is available in time step ğ‘¡ + ğ‘›, it uses the ğ‘›-step return ğºğ‘¡âˆ¶ğ‘¡+ğ‘› as its target, and it bootstraps from the previous approximation ğºğ‘¡âˆ¶ğ‘¡+ğ‘›+1. The state value of state ğ‘†ğ‘¡, ğ‘› steps in the past, is updated (with the information obtained in the future ğ‘› steps), and the state values of all other states remain unchanged as usual in such update formulae.
Therefore we define the ğ‘›-step TD update as
ğ‘‰ğ‘¡+ğ‘›(ğ‘ ) âˆ¶= {
ğ‘‰ğ‘¡+ğ‘›âˆ’1(ğ‘†ğ‘¡) + ğ›¼(ğºğ‘¡âˆ¶ğ‘¡+ğ‘› âˆ’ ğ‘‰ğ‘¡+ğ‘›âˆ’1(ğ‘†ğ‘¡)), ğ‘‰ğ‘¡+ğ‘›âˆ’1(ğ‘ ),
ğ‘  = ğ‘†ğ‘¡, ğ‘  â‰  ğ‘†ğ‘¡
for all 0 â‰¤ ğ‘¡ < ğ‘‡. In the case ğ‘› = 1, we recover the one-step update (5.1).
The corresponding algorithm is shown in Algorithm 12. Some bookkeeping is required because the rewards for the updates must be accumulated first. The states ğ‘†ğ‘¡ and rewards ğ‘…ğ‘¡ are saved in vectors of length ğ‘› + 1. Since only their history of this length is required, ğ‘†ğ‘¡ (and ğ‘…ğ‘¡) can be stored as the element number ğ‘¡ mod ğ‘› + 1.
67
68
Chapter 5. Temporal-Difference Learning
Algorithm 12 ğ‘›-step TD for calculating ğ‘‰ â‰ˆ ğ‘£ğœ‹ given ğœ‹.
initialization: choose number of steps ğ‘› choose learning rate ğ›¼ âˆˆ (0,1] initialize the vector ğ‘£ of length |ğ’®| arbitrarily
except that the value of the terminal state is 0
loop
â–· for all episodes
initialize and store ğ‘†0 ğ‘¡ âˆ¶= 0 ğ‘‡ âˆ¶= âˆ
repeat
â–· for all time steps
if ğ‘¡ < ğ‘‡ then
set ğ‘ to be the action given by ğœ‹ for ğ‘†ğ‘¡ take action ğ‘ and receive the new state ğ‘†ğ‘¡+1 and the reward ğ‘…ğ‘¡+1 if ğ‘†ğ‘¡+1 is terminal then
ğ‘‡ âˆ¶= ğ‘¡ + 1
end if
end if
ğœ âˆ¶= ğ‘¡ âˆ’ ğ‘› + 1 if ğœ â‰¥ 0 then
â–· time step whose approximation is now updated
ğº âˆ¶= âˆ‘min(ğœ+ğ‘›,ğ‘‡) if ğœ + ğ‘› < ğ‘‡ then
ğ‘˜=ğœ+1
ğ›¾ğ‘˜âˆ’ğœâˆ’1ğ‘…ğ‘˜
ğº âˆ¶= ğº + ğ›¾ğ‘›ğ‘£(ğ‘†ğœ+ğ‘›)
end if ğ‘£[ğ‘†ğœ] âˆ¶= ğ‘£[ğ‘†ğœ] + ğ›¼(ğº âˆ’ ğ‘£[ğ‘†ğœ])
end if ğ‘¡ âˆ¶= ğ‘¡ + 1
until ğœ +1 = ğ‘‡ â–· corresponding to the final non-zero ğº, ğº = ğ‘…ğœ+1 = ğ‘…ğ‘‡
end loop
5.9. On-Policy Multi-Step Temporal-Difference Control: ğ‘›-step SARSA
69
5.9 On-Policy Multi-Step Temporal-Difference Con-
trol: ğ‘›-step SARSA
The multi-step version of SARSA is an extension of the one-step SARSA method in Section 5.3. Analogously to ğ‘›-step TD, it uses ğ‘› future rewards, but replaces the approximation ğ‘‰ of the state-value function by the approximation ğ‘„ of the action-value function. We therefore redefine the ğ‘›-step return as follows.
Definition 5.2 (ğ‘›-step return (using ğ‘„)). The ğ‘›-step return using ğ‘„ is defined as
ğºğ‘¡âˆ¶ğ‘¡+ğ‘› âˆ¶= {
ğ‘…ğ‘¡+1 + ğ›¾ğ‘…ğ‘¡+2 + â‹¯ + ğ›¾ğ‘›âˆ’1ğ‘…ğ‘¡+ğ‘› + ğ›¾ğ‘›ğ‘„ğ‘¡+ğ‘›âˆ’1(ğ‘†ğ‘¡+ğ‘›,ğ´ğ‘¡+ğ‘›), ğºğ‘¡,
ğ‘¡ + ğ‘› < ğ‘‡, ğ‘¡ + ğ‘› â‰¥ ğ‘‡
for all ğ‘› â‰¥ 1.
Therefore the ğ‘›-step SARSA update is
ğ‘„ğ‘¡+ğ‘›(ğ‘ ,ğ‘) âˆ¶= {
ğ‘„ğ‘¡+ğ‘›âˆ’1(ğ‘†ğ‘¡,ğ´ğ‘¡) + ğ›¼(ğºğ‘¡âˆ¶ğ‘¡+ğ‘› âˆ’ ğ‘„ğ‘¡+ğ‘›âˆ’1(ğ‘†ğ‘¡,ğ´ğ‘¡)), ğ‘„ğ‘¡+ğ‘›âˆ’1(ğ‘ ,ğ‘),
(ğ‘ ,ğ‘) = (ğ‘†ğ‘¡,ğ´ğ‘¡), (ğ‘ ,ğ‘) â‰  (ğ‘†ğ‘¡,ğ´ğ‘¡)
for all 0 â‰¤ ğ‘¡ < ğ‘‡. In the case ğ‘› = 1, we recover the one-step update (5.3).
The corresponding algorithm is shown in Algorithm 13.
70
Chapter 5. Temporal-Difference Learning
Algorithm 13 ğ‘›-step SARSA for calculating ğ‘„ â‰ˆ ğ‘âˆ— and ğœ‹ â‰ˆ ğœ‹âˆ—.
initialization: choose number of steps ğ‘› choose learning rate ğ›¼ âˆˆ (0,1] initialize ğ‘„(ğ‘ ,ğ‘) âˆˆ â„ arbitrarily for all (ğ‘ ,ğ‘) âˆˆ ğ’® Ã— ğ’œ(ğ‘ ) except that the value of the terminal state is 0
initialize ğœ‹ to be ğœ–-greedy with respect to ğ‘„
loop
â–· for all episodes
initialize and store ğ‘†0 set ğ´0 to be the action given by ğœ‹ for ğ‘†0 ğ‘¡ âˆ¶= 0 ğ‘‡ âˆ¶= âˆ
repeat
â–· for all time steps
if ğ‘¡ < ğ‘‡ then
take action ğ‘ and receive the new state ğ‘†ğ‘¡+1 and the reward ğ‘…ğ‘¡+1 if ğ‘†ğ‘¡+1 is terminal then
ğ‘‡ âˆ¶= ğ‘¡ + 1
else
set ğ‘ to be the action given by ğœ‹ for ğ‘†ğ‘¡+1
end if
end if
ğœ âˆ¶= ğ‘¡ âˆ’ ğ‘› + 1 if ğœ â‰¥ 0 then
â–· time step whose approximation is now updated
ğº âˆ¶= âˆ‘min(ğœ+ğ‘›,ğ‘‡) if ğœ + ğ‘› < ğ‘‡ then
ğ‘˜=ğœ+1
ğ›¾ğ‘˜âˆ’ğœâˆ’1ğ‘…ğ‘˜
ğº âˆ¶= ğº + ğ›¾ğ‘›ğ‘„(ğ‘†ğœ+ğ‘›,ğ´ğœ+ğ‘›)
end if ğ‘„[ğ‘†ğœ,ğ´ğœ] âˆ¶= ğ‘„[ğ‘†ğœ,ğ´ğœ] + ğ›¼(ğº âˆ’ ğ‘„[ğ‘†ğœ,ğ´ğœ]) set ğœ‹(â‹…|ğ‘†ğœ) to be ğœ–-greedy with respect to ğ‘„
end if ğ‘¡ âˆ¶= ğ‘¡ + 1
until ğœ +1 = ğ‘‡ â–· corresponding to the final non-zero ğº, ğº = ğ‘…ğœ+1 = ğ‘…ğ‘‡
end loop
5.10. Bibliographical and Historical Remarks
5.10 Bibliographical and Historical Remarks
5.11 Exercises
71
72
Chapter 5. Temporal-Difference Learning
Chapter 6
Convergence of Discrete Q-Learning
In any case, the mathematician sees hundreds and thousands of formidable new problems in dozens of blossoming areas, puzzles ga- lore, and challenges to his heartâ€™s content. He may never resolve some of these, but he will never be bored. What more can he ask?
Richard Bellman [38, p. 37].
In this chapter, it is shown that discrete ğ‘„-learning converges to the opti- mal action-value function under certain benign assumptions. Two different ap- proaches and proofs are presented. The first is the original proof by Christopher Watkins and works by constructing another Markov decision process called the action-replay process, whose actions are optimal and which is sufficiently similar to the original process. The second approach uses a fixed-point argument to en- sure the unique existence of the optimal action-value function and then uses the theory of stochastic processes to show that the difference between the learned policy and the optimal action-value function goes to zero as learning continues.
6.1 Introduction
ğ‘„-learning is an off-policy temporal-difference control method that directly ap- proximates the optimal action-value function ğ‘âˆ—âˆ¶ ğ’® Ã— ğ’œ â†’ â„. We denote the approximation in time step ğ‘¡ of an episode by ğ‘„ğ‘¡âˆ¶ ğ’® Ã— ğ’œ â†’ â„. The initial ap- proximation ğ‘„0 is initialized arbitrarily except that it vanishes for all terminal In each iteration, an action ğ‘ğ‘¡ is chosen from state ğ‘ ğ‘¡ using a policy states.
73
74
Chapter 6. Convergence of Discrete Q-Learning
derived from the previous approximation of the action-value function ğ‘„ğ‘¡, e.g., using an ğœ–-greedy policy, and a reward ğ‘Ÿğ‘¡+1 is obtained and a new state ğ‘ ğ‘¡+1 is achieved. The next approximation ğ‘„ğ‘¡+1 is defined as
ğ‘„ğ‘¡+1(ğ‘ ,ğ‘) âˆ¶= {
(1 âˆ’ ğ›¼ğ‘¡)ğ‘„ğ‘¡(ğ‘ ğ‘¡,ğ‘ğ‘¡) + ğ›¼ğ‘¡(ğ‘Ÿğ‘¡+1 + ğ›¾ maxğ‘ ğ‘„ğ‘¡(ğ‘ ğ‘¡+1,ğ‘)), ğ‘„ğ‘¡(ğ‘ ,ğ‘),
(ğ‘ ,ğ‘) = (ğ‘ ğ‘¡,ğ‘ğ‘¡), (ğ‘ ,ğ‘) â‰  (ğ‘ ğ‘¡,ğ‘ğ‘¡).
(6.1) Here ğ›¼ğ‘¡ âˆˆ [0,1] is the step size or learning rate and ğ›¾ âˆˆ [0,1] denotes the discount factor.
In this value-iteration update, only the value for (ğ‘ ğ‘¡,ğ‘ğ‘¡) is updated. The update can be viewed as a weighted average of the old value ğ‘„ğ‘¡(ğ‘ ğ‘¡,ğ‘ğ‘¡) and the new information ğ‘Ÿğ‘¡+1 + ğ›¾ maxğ‘ ğ‘„ğ‘¡(ğ‘ ğ‘¡+1,ğ‘), which is an estimate of the action- value function a time step later. Since the approximation ğ‘„ğ‘¡+1 depends on the previous estimate of ğ‘âˆ—, ğ‘„-learning is a bootstrapping method.
The new value ğ‘„ğ‘¡+1(ğ‘ ğ‘¡,ğ‘ğ‘¡) can also be written as
ğ‘„ğ‘¡+1(ğ‘ ğ‘¡,ğ‘ğ‘¡) âŸâŸâŸâŸâŸ new value
âˆ¶= ğ‘„ğ‘¡(ğ‘ ğ‘¡,ğ‘ğ‘¡) âŸâŸâŸâŸâŸ old value
+ğ›¼ğ‘¡(ğ‘Ÿğ‘¡+1 + ğ›¾ max
ğ‘„ğ‘¡(ğ‘ ğ‘¡+1,ğ‘) âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸ target value
ğ‘âˆˆğ’œ(ğ‘ ğ‘¡+1)
âˆ’ğ‘„ğ‘¡(ğ‘ ğ‘¡,ğ‘ğ‘¡) âŸâŸâŸâŸâŸ old value
),
which is the form of a semigradient SGD method with a certain linear function approximation.
The learning rate ğ›¼ğ‘¡ must be chosen appropriately, and convergence results hold only under certain conditions on the learning rate. If the environment is fully deterministic, the learning rate ğ›¼ğ‘¡ âˆ¶= 1 is optimal. If the environment is stochastic, then a necessary condition for convergence is that limğ‘¡â†’âˆ ğ›¼ğ‘¡ = 0.
If the initial approximation ğ‘„0 is defined to have large values, exploration is encouraged at the beginning of learning. This kind of initialization is known as using optimistic initial conditions.
6.2 Convergence Proved Using Action Replay
ğ‘„-learning was introduced in [39, page 95], where an outline [39, Appendix 1] of the first convergence proof of one-step ğ‘„-learning was given as well. The original proof was extended and given in detail in [40]. It is presented in a summarized form in this section, because its approach is different from other, later proofs and because it gives intuitive insight into the convergence process.
The proof concerns the discrete or tabular method, i.e., the case of finite state and action sets. It is shown that ğ‘„-learning converges to the optimum action values with probability one if all actions are repeatedly sampled in all states.
6.2. Convergence Proved Using Action Replay
Before we can state the theorem, a definition is required. We assume that all states and actions observed during learning have been numbered consecutively; then ğ‘(ğ‘–,ğ‘ ,ğ‘) âˆˆ â„• is defined as the index of the ğ‘–-th time that action ğ‘ is taken in state ğ‘ .
Theorem 6.1 (convergence of ğ‘„-learning proved by action replay). Suppose that the state and action spaces are finite. Suppose that the episodes that form the basis of learning include an infinite number of occurrences of each pair (ğ‘ ,ğ‘) âˆˆ ğ’® Ã— ğ’œ (e.g., as starting state-action pairs). Given bounded rewards, the discount factor ğ›¾ < 1, learning rates ğ›¼ğ‘› âˆˆ [0,1), and
âˆ âˆ‘ ğ‘–=1
ğ›¼ğ‘(ğ‘–,ğ‘ ,ğ‘) = âˆ âˆ§
âˆ âˆ‘ ğ‘–=1
ğ›¼2
ğ‘(ğ‘–,ğ‘ ,ğ‘) < âˆ âˆ€(ğ‘ ,ğ‘) âˆˆ ğ’® Ã— ğ’œ,
then
ğ‘„ğ‘›(ğ‘ ,ğ‘) â†’ ğ‘âˆ—(ğ‘ ,ğ‘)
(as defined by (6.1)) as ğ‘› â†’ âˆ for all (ğ‘ ,ğ‘) âˆˆ ğ’® Ã— ğ’œ with probability one.
Sketch of the proof. The main idea is to construct an artificial Markov process called the action-replay process (ARP) from the sequence of episodes and the sequence of learning rates of the real process.
The ARP is defined as follows. It has the same discount factor as the real process. Its state space is (ğ‘ ,ğ‘›), where ğ‘  is either a state of the real process or a new, absorbing and terminal state, and ğ‘› âˆˆ â„• is an index whose significance is discussed below. The action space is the same as the real process.
Action ğ‘ at state (ğ‘ ,ğ‘›1) in the ARP is performed as follows. All transitions
observed during learning are recorded in a sequence consisting of tuples
ğ‘‡ğ‘¡ âˆ¶= (ğ‘ ğ‘¡,ğ‘ğ‘¡,ğ‘ â€²
ğ‘¡,ğ‘Ÿâ€²
ğ‘¡,ğ›¼ğ‘¡).
Here ğ‘ğ‘¡ is the action taken in state ğ‘ ğ‘¡ yielding the new state ğ‘ â€² ğ‘¡ and the reward ğ‘Ÿâ€² ğ‘¡ while using the learning rate ğ›¼ğ‘¡. To perform action ğ‘ at state (ğ‘ ,ğ‘›1) in the ARP, all transitions after and including transition ğ‘›1 are eliminated and not considered further. Starting at transition ğ‘›1 âˆ’ 1 and counting down, the first transition ğ‘‡ğ‘¡ = (ğ‘ ğ‘¡,ğ‘ğ‘¡,ğ‘ â€² ğ‘¡,ğ›¼ğ‘¡) before transition number ğ‘›1 whose starting state ğ‘ ğ‘¡ and action ğ‘ğ‘¡ match (ğ‘ ,ğ‘) is found and its index is called ğ‘›2, i.e., ğ‘›2 is the largest index less than ğ‘›1 such that (ğ‘ ,ğ‘) = (ğ‘ ğ‘›2 ). With probability ) is replayed. Otherwise, with = (ğ‘ ğ‘›2 ğ›¼ğ‘›2 probability 1 âˆ’ ğ›¼ğ‘›2 , the search is repeated towards the beginning. If, however, there is no such ğ‘›2, i.e., if there is no matching state-action pair, the reward
ğ‘¡,ğ‘Ÿâ€²
,ğ‘ğ‘›2
,ğ‘ â€² ğ‘›2
,ğ‘Ÿâ€² ğ‘›2
, the transition ğ‘‡ğ‘›2
,ğ‘ğ‘›2
,ğ›¼ğ‘›2
75
76
Chapter 6. Convergence of Discrete Q-Learning
ğ‘Ÿâ€² ğ‘›1 an absorbing, terminal state.
= ğ‘„0(ğ‘ ,ğ‘) of transition ğ‘‡ğ‘›1
is recorded and the episode in the ARP stops in
Replaying a transition ğ‘‡ğ‘›2
in the ARP is defined to mean that the ARP was followed
state (ğ‘ ,ğ‘›1) is followed by the state (ğ‘ â€² ğ‘›2 by the state ğ‘ â€² in the real process after taking action ğ‘ = ğ‘ğ‘›2 ğ‘›2
,ğ‘›2 âˆ’1); the state ğ‘  = ğ‘ ğ‘›2
.
The next transition in the ARP is found by following this search process towards the beginning of the transitions recorded during learning, and so forth. The ARP episode ultimately terminates after a finite number of steps, since the index ğ‘› in its states (ğ‘ ,ğ‘›) decreases strictly monotonically. Because of this construction, the ARP is a Markov decision process just as the real process.
Having constructed the ARP, the proof proceeds in two steps recorded as lemmata. The first lemma says that the approximations ğ‘„ğ‘›(ğ‘ ,ğ‘) calculated during ğ‘„-learning are the optimal action values for the ARP states and actions. The rest of the lemmata say that the ARP converges to the real process. We start with the first lemma.
Lemma 6.2. The optimal action values for ARP states (ğ‘ ,ğ‘›) and ARP actions ğ‘ are ğ‘„ğ‘›(ğ‘ ,ğ‘), i.e.,
ğ‘„ğ‘›(ğ‘ ,ğ‘) = ğ‘„âˆ—
ARP((ğ‘ ,ğ‘›),ğ‘)
âˆ€(ğ‘ ,ğ‘) âˆˆ ğ’® Ã— ğ’œ âˆ€ğ‘› âˆˆ â„•.
Proof. The proof is by induction with respect to ğ‘›. Due to the construction of the ARP, the action value ğ‘„0(ğ‘ ,ğ‘) is optimal, since it is the only possible action value of (ğ‘ ,0) and ğ‘. In other words, the induction basis
ğ‘„0(ğ‘ ,ğ‘) = ğ‘„âˆ—
ARP((ğ‘ ,0),ğ‘)
holds true.
The show the induction step, we suppose that the action values ğ‘„ğ‘› as cal- culated by the ğ‘„-learning iteration are optimal action values for the ARP at level ğ‘›, i.e.,
ğ‘„ğ‘›(ğ‘ ,ğ‘) = ğ‘„âˆ—
ARP((ğ‘ ,ğ‘›),ğ‘)
âˆ€(ğ‘ ,ğ‘) âˆˆ ğ’® Ã— ğ’œ,
which implies
ğ‘‰ âˆ— ARP((ğ‘ ,ğ‘›)) = max
ğ‘
ğ‘„ğ‘›(ğ‘ ,ğ‘)
for the optimal state values ğ‘‰ âˆ— equation motivates the use of the maximum in ğ‘„-learning.)
ARP of the ARP at level ğ‘›. (Note that the last
In order to perform action ğ‘ in state (ğ‘ ,ğ‘› + 1), we consider two cases. The first case is (ğ‘ ,ğ‘) â‰  (ğ‘ ğ‘›,ğ‘ğ‘›). In this case, performing ğ‘ in state (ğ‘ ,ğ‘› + 1) is the same as performing ğ‘ in state (ğ‘ ,ğ‘›) by the definition of ğ‘„-learning; nothing changes in the second case in (6.1). Therefore we have
6.2. Convergence Proved Using Action Replay
77
ğ‘„ğ‘›+1(ğ‘ ,ğ‘) = ğ‘„ğ‘›(ğ‘ ,ğ‘) = ğ‘„âˆ—
ARP((ğ‘ ,ğ‘›),ğ‘) = ğ‘„âˆ—
ARP((ğ‘ ,ğ‘› + 1),ğ‘)
âˆ€(ğ‘ ,ğ‘) âˆˆ ğ’® Ã— ğ’œ âˆ– {(ğ‘ ğ‘›,ğ‘ğ‘›)} âˆ€ğ‘› âˆˆ â„•.
The second case is (ğ‘ ,ğ‘) = (ğ‘ ğ‘›,ğ‘ğ‘›). In this case, performing ğ‘ğ‘› in the state
(ğ‘ ğ‘›,ğ‘› + 1) is equivalent
to obtaining the reward ğ‘Ÿâ€² or
to obtaining the reward ğ‘Ÿâ€² or
ğ‘› and the new state (ğ‘ â€²
to performing ğ‘ğ‘› in the state (ğ‘ ğ‘›,ğ‘›) with probability 1 âˆ’ ğ›¼ğ‘›.
The induction hypothesis and the definition of ğ‘„-learning hence yield
ğ‘„âˆ—
ARP((ğ‘ ğ‘›,ğ‘› + 1),ğ‘ğ‘›) = (1 âˆ’ ğ›¼ğ‘›)ğ‘„âˆ—
ARP((ğ‘ ğ‘›,ğ‘›),ğ‘ğ‘›) + ğ›¼ğ‘›(ğ‘Ÿâ€²
ğ‘› + ğ›¾ğ‘‰ âˆ— ğ‘„ğ‘›(ğ‘ â€²
ARP((ğ‘ â€² ğ‘›,ğ‘))
ğ‘›,ğ‘›))
= (1 âˆ’ ğ›¼ğ‘›)ğ‘„ğ‘›(ğ‘ ğ‘›,ğ‘ğ‘›) + ğ›¼ğ‘›(ğ‘Ÿâ€²
ğ‘› + ğ›¾ max
ğ‘
= ğ‘„ğ‘›+1(ğ‘ ğ‘›,ğ‘ğ‘›)
âˆ€ğ‘› âˆˆ â„•.
Both cases together mean that
ğ‘„ğ‘›+1(ğ‘ ,ğ‘) = ğ‘„âˆ—
ARP((ğ‘ ,ğ‘› + 1),ğ‘)
âˆ€(ğ‘ ,ğ‘) âˆˆ ğ’® Ã— ğ’œ âˆ€ğ‘› âˆˆ â„•,
which concludes the proof of the induction step.
The rest of the lemmata say that the ARP converges to the real process.
Here we prove only the first and second one; the rest are shown in [40].
Lemma 6.3. Consider a finite Markov process with a discount factor ğ›¾ < 1 and bounded rewards. Also consider two episodes, both starting at state ğ‘  and followed by the same finite sequence of ğ‘˜ actions, before continuing with any other actions. Then the difference of the values of the starting state ğ‘  in both episodes tends to zero as ğ‘˜ â†’ âˆ.
Proof. The assertion follows because ğ›¾ < 1 and the rewards are bounded.
Lemma 6.4. Consider the ARP defined above with states (ğ‘ ,ğ‘›) and call ğ‘› the level. Then, given any level ğ‘›1 âˆˆ â„•, there exists another level ğ‘›2 âˆˆ â„•, ğ‘›2 > ğ‘›1, such that the probability of reaching a level below ğ‘›1 after taking ğ‘˜ âˆˆ â„• actions starting from a level above ğ‘›2 is arbitrarily small.
In other words, the probability of reaching any fixed level ğ‘›1 after starting at level ğ‘›2 of the ARP tends to zero as ğ‘›2 â†’ âˆ. Therefore there is a sufficiently high level ğ‘›2 from which ğ‘˜ actions can be performed with an arbitrarily high probability of leaving the ARP episode above level ğ‘›1.
78
Chapter 6. Convergence of Discrete Q-Learning
Proof. We start by determining the probability ğ‘ƒ of reaching a level below ğ‘›1 starting from a state (ğ‘ ,ğ‘›) with ğ‘› > ğ‘›1 and performing action ğ‘. Recall that ğ‘(ğ‘–,ğ‘ ,ğ‘) is the index of the ğ‘–-th time that action ğ‘ is taken in state ğ‘ . We define ğ‘–1 to be the smallest index ğ‘– such that ğ‘(ğ‘–,ğ‘ ,ğ‘) â‰¥ ğ‘›1 and ğ‘–2 to be the largest index ğ‘– such that ğ‘(ğ‘–,ğ‘ ,ğ‘) â‰¤ ğ‘›2. We also define ğ›¼ğ‘(0,ğ‘ ,ğ‘) âˆ¶= 1. Then the probability is
ğ‘–2 âˆ ğ‘–=ğ‘–1
ğ‘ƒ = (
(1 âˆ’ ğ›¼ğ‘(ğ‘–,ğ‘ ,ğ‘))) âŸâŸâŸâŸâŸâŸâŸâŸâŸ continue above level ğ‘–1
ğ‘–1âˆ’1 ğ‘–1âˆ’1 âˆ‘ (1 âˆ’ ğ›¼ğ‘(ğ‘˜,ğ‘ ,ğ‘)) âˆ ğ‘—=0 ğ‘˜=ğ‘—+1 âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸ stop below level ğ‘–1
ğ›¼ğ‘(ğ‘—,ğ‘ ,ğ‘)
.
The sum is less equal one, since it is the probability that the episode ends (below level ğ‘–1). Therefore we have the estimate
ğ‘ƒ â‰¤
ğ‘–2 âˆ ğ‘–=ğ‘–1
(1 âˆ’ ğ›¼ğ‘(ğ‘–,ğ‘ ,ğ‘)) â‰¤ exp(âˆ’
ğ‘–2 âˆ‘ ğ‘–=ğ‘–1
ğ›¼ğ‘(ğ‘–,ğ‘ ,ğ‘)),
where the second inequality holds because the inequality 1 âˆ’ ğ‘¥ â‰¤ exp(âˆ’ğ‘¥) for all ğ‘¥ âˆˆ [0,1] has been applied to each term. Since the sum of the learning rates diverges by assumption, we find that ğ‘ƒ â‰¤ exp(âˆ’âˆ‘ğ‘–2 ğ›¼ğ‘(ğ‘–,ğ‘ ,ğ‘)) â†’ 0 as ğ‘›2 â†’ âˆ and hence ğ‘–2 â†’ âˆ.
ğ‘–=ğ‘–1
Since the state and action spaces are finite, for each probability ğœ– âˆˆ (0,1], there exists a level ğ‘›2 such that starting above it from any state ğ‘  and taking action ğ‘ leads to a level above ğ‘›1 with probability at least 1âˆ’ğœ–. This argument can be applied ğ‘˜ times for each action, and the probability ğœ– can be chosen small enough such that the overall probability of reaching a level below ğ‘›1 after taking ğ‘˜ actions becomes arbitrarily small, which concludes the proof.
Before stating the next lemma, we denote the transition probabilities of the ARP by ğ‘ARP((ğ‘ â€²,ğ‘›â€²) âˆ£ (ğ‘ ,ğ‘›),ğ‘) and its expected rewards by ğ‘…ğ‘›(ğ‘ ,ğ‘). We also define the probability
ğ‘ğ‘›(ğ‘ â€² âˆ£ ğ‘ ,ğ‘) âˆ¶=
ğ‘›âˆ’1 âˆ‘ ğ‘›â€²=1
ğ‘ARP((ğ‘ â€²,ğ‘›â€²) âˆ£ (ğ‘ ,ğ‘›),ğ‘)
that performing action ğ‘ at state (ğ‘ ,ğ‘›) (at level ğ‘›) in the ARP leads to the state ğ‘ â€² at a lower level.
Lemma 6.5. With probability one, the transition probabilities ğ‘ğ‘›(ğ‘ â€² âˆ£ ğ‘ ,ğ‘) at level ğ‘› and the expected rewards ğ‘…ğ‘›(ğ‘ ,ğ‘) at level ğ‘› of the ARP converge to the transition probabilities and expected rewards of the real process as the level ğ‘› tends to infinity.
6.2. Convergence Proved Using Action Replay
Sketch of the proof. The proof of this lemma [40, Lemma B.3] relies on a stan- dard theorem in stochastic convergence (see, e.g., [41, Theorem 2.3.1]), which states that if random variables ğ‘‹ğ‘› are updated according to
ğ‘‹ğ‘›+1 âˆ¶= ğ‘‹ğ‘› + ğ›½ğ‘›(ğœ‰ğ‘› âˆ’ ğ‘‹ğ‘›), ğ‘›=1 ğ›½2
where ğ›½ğ‘› âˆˆ [0,1), âˆ‘âˆ are bounded and have mean ğœ‰, then
ğ‘›=1 ğ›½ğ‘› = âˆ, âˆ‘âˆ
ğ‘› < âˆ, and the random variables ğœ‰ğ‘›
ğ‘‹ğ‘› â†’ ğœ‰
as ğ‘› â†’ âˆ with probability one.
This theorem is applied to the two update formulae for the transition prob- abilities and expected rewards for going from occurrence ğ‘– + 1 to occurrence ğ‘–. Since there is only a finite number of states and actions, the convergence is uniform.
Lemma 6.6. Consider episodes of ğ‘˜ âˆˆ â„• actions in the ARP and in the real If the transition probabilities ğ‘ğ‘›(ğ‘ â€² âˆ£ ğ‘ ,ğ‘) and the expected rewards process. ğ‘…ğ‘›(ğ‘ ,ğ‘) at appropriate levels of the ARP for each of the actions are sufficiently close to the transition probabilities ğ‘(ğ‘ â€² âˆ£ ğ‘ ,ğ‘) and expected rewards ğ‘…(ğ‘ ,ğ‘) of the real process for all actions ğ‘, for all states ğ‘ , and for all states ğ‘ â€², then the value of the episode in the ARP is close to its value in the real process.
Sketch of the proof. The difference in the action values of a finite number ğ‘˜ of ac- tions between the ARP and the real process grows at most quadratically with ğ‘˜. Therefore, if the transition probabilities and mean rewards are sufficiently close, the action values must also be close.
Using these lemmata, we can finish the proof of the theorem. The idea is that the ARP tends towards the real process and hence its optimal action values do as well. The values ğ‘„ğ‘›(ğ‘ ,ğ‘) are the optimal action values for level ğ‘› of the ARP by Lemma 6.2, and therefore they tend to ğ‘„âˆ—(ğ‘ ,ğ‘).
More precisely, we denote the bound of the rewards by ğ‘… âˆˆ â„+
0 such that |ğ‘Ÿğ‘›| â‰¤ ğ‘… for all ğ‘› âˆˆ â„•. Without loss of generality, it can be assumed that ğ‘„0(ğ‘ ,ğ‘) < ğ‘…/(1âˆ’ğ›¾) and that ğ‘… â‰¥ 1. For an arbitrary ğœ– âˆˆ â„+, we choose ğ‘˜ âˆˆ â„• such that
ğ›¾ğ‘˜ ğ‘… 1 âˆ’ ğ›¾
<
ğœ– 6
holds.
By Lemma 6.5, with probability one, it is possible to find a sufficiently large
ğ‘›1 âˆˆ â„• such that the inequalities
|ğ‘ğ‘›(ğ‘ â€² âˆ£ ğ‘ ,ğ‘) âˆ’ ğ‘(ğ‘ â€² âˆ£ ğ‘ ,ğ‘)| <
ğœ– 3ğ‘˜(ğ‘˜ + 1)ğ‘…
,
79
80
Chapter 6. Convergence of Discrete Q-Learning
|ğ‘…ğ‘›(ğ‘ ,ğ‘) âˆ’ ğ‘…(ğ‘ ,ğ‘)| <
ğœ– 3ğ‘˜(ğ‘˜ + 1)
hold for the differences between the transition probabilities and expected rewards of the ARP and the real process for all ğ‘› > ğ‘›1 and for all actions ğ‘, for all states ğ‘ , and for all states ğ‘ â€².
By Lemma 6.4, it is possible to find a sufficiently large ğ‘›2 âˆˆ â„• such that for all ğ‘› > ğ‘›2 the probability of reaching a level lower than ğ‘›1 after taking ğ‘˜ actions is less than min{ğœ–(1âˆ’ğ›¾)/6ğ‘˜ğ‘…,ğœ–/3ğ‘˜(ğ‘˜+1)ğ‘…}. This implies that the inequalities
|ğ‘â€²
ğ‘›(ğ‘ â€² âˆ£ ğ‘ ,ğ‘) âˆ’ ğ‘(ğ‘ â€² âˆ£ ğ‘ ,ğ‘)| <
|ğ‘…â€²
ğ‘›(ğ‘ ,ğ‘) âˆ’ ğ‘…(ğ‘ ,ğ‘)| <
2ğœ– 3ğ‘˜(ğ‘˜ + 1)ğ‘… 2ğœ– 3ğ‘˜(ğ‘˜ + 1)
,
hold, where the primes on the probabilities indicate that they are conditional on the level of the ARP after ğ‘˜ actions being greater than ğ‘›1.
Then, by Lemma 6.6, the difference between the value ğ‘„ARP((ğ‘ ,ğ‘›),ğ‘1,â€¦,ğ‘ğ‘˜) of performing actions ğ‘1,â€¦,ğ‘ğ‘˜ at state (ğ‘ ,ğ‘›) in the ARP and the value ğ‘„(ğ‘ ,ğ‘1,â€¦,ğ‘ğ‘˜) of performing these actions in the real process is bounded by the inequality
|ğ‘„ARP((ğ‘ ,ğ‘›),ğ‘1,â€¦,ğ‘ğ‘˜) âˆ’ ğ‘„(ğ‘ ,ğ‘1,â€¦,ğ‘ğ‘˜)| ğœ–(1 âˆ’ ğ›¾) 6ğ‘˜ğ‘…
<
2ğ‘˜ğ‘… 1 âˆ’ ğ›¾
+
2ğœ– 3ğ‘˜(ğ‘˜ + 1)
ğ‘˜(ğ‘˜ + 1) 2
=
2ğœ– 3
The first term is the difference if the conditions for Lemma 6.4 are not satisfied, since the cost of reaching a level below ğ‘›1 is bounded by 2ğ‘˜ğ‘…/(1 âˆ’ ğ›¾). The sec- ond term is the difference from Lemma 6.6 stemming from imprecise transition probabilities and expected rewards.
By Lemma 6.3, the difference due to taking only ğ‘˜ actions is less than ğœ–/6
for both the ARP and the real process.
Since the inequality above applies to any sequence of actions, it applies in particular to a sequence of actions optimal for either the ARP or the real process. Therefore the estimate
|ğ‘„âˆ—
ARP((ğ‘ ,ğ‘›),ğ‘) âˆ’ ğ‘„âˆ—(ğ‘ ,ğ‘)| < ğœ–
holds. In conclusion, ğ‘„ğ‘›(ğ‘ ,ğ‘) â†’ ğ‘„âˆ—(ğ‘ ,ğ‘) as ğ‘› â†’ âˆ with probability one, which concludes the proof of the theorem.
Remark 6.7 (the non-discounted case ğ›¾ = 1 with absorbing goal states). If the discount factor ğ›¾ = 1, but the Markov process has absorbing goal states, the
.
6.3. Convergence Proved Using Stochastic Approximation
proof can be modified [40, Section 4]. The certainty of being trapped in an absorbing goal state then plays the role of ğ›¾ < 1 and ensures that the value of each state is bounded under any policy and that Lemma 6.3 holds.
Remark 6.8 (action replay and experience replay). The assumption that all pairs of states and actions occur an infinite number of times during learning is crucial for the proof. The proof also suggests that convergence is faster if the occurrences of states and actions are equidistributed. This fact motivates the use of action or experience replay. Experience replay is a method (not limited to be used in conjunction with ğ‘„-learning) which keeps a cache of states and actions that have been visited and which are replayed during learning in order to ensure that the whole space is sampled in an equidistributed manner. This is beneficial when the states of the Markov chain are highly correlated. Experience replay was used, e.g., in [13]. Cf. Remark 8.4.
6.3 Convergence Proved Using Stochastic Approxi-
mation
It is natural to consider the iterates of temporal-difference learning algorithms as a stochastic process. Then the theory of stochastic approximation (see Chap- ter C) may provide tools to show almost sure convergence or convergence in mean square (or in quadratic mean). Unfortunately, however, the theory of stochastic approximation for multidimensional stochastic processes requires an underlying Hilbert space (see Section C.2), while the empirical Bellman operator in the ğ‘„-learning iteration is a contraction only with respect to the maximum norm, and the maximum norm does not stem from any inner product, which would be necessary for a Hilbert space. This major technical hurdle must be overcome when applying the theory of stochastic approximation to the conver- gence of ğ‘„-learning. Furthermore, the noise terms may not satisfy standard assumptions made in stochastic approximation.
The general approach is to show that the empirical Bellman operator is a contraction with respect to the maximum norm first. Its unique fixed point is the supposed limit, i.e., the optimal action-value function. Then the iteration formula is split into a deterministic and a stochastic (or noise) term and it is matched to a stochastic process whose convergence is known by a lemma. This lemma is usually a generalization of a result known from stochastic approxima- tion.
This is the basic approach taken in [42] for ğ‘„-learning and TD(ğœ†) and in [43] and [36, Section 5.6] for ğ‘„-learning. By extending the same approach in
81
82
Chapter 6. Convergence of Discrete Q-Learning
[44, 45], convergence theorems for ğ‘„-learning for environments that change over time, but whose accumulated changes remain bounded, were shown in [46].
Theorem 6.9 (stochastic approximation). The stochastic process
ğ‘¤ğ‘¡+1(ğ‘ ) âˆ¶= (1 âˆ’ ğ›¼ğ‘¡(ğ‘ ))ğ‘¤ğ‘¡(ğ‘ ) + ğ›½ğ‘¡(ğ‘ )ğ‘Ÿğ‘›(ğ‘ ),
where all random variables may depend on the past ğ‘ƒğ‘¡, converges to zero with probability one if the following conditions are satisfied.
1. The step sizes ğ›¼ğ‘¡(ğ‘ ) and ğ›½ğ‘¡(ğ‘ ) satisfy the equalities and inequalities âˆ‘ğ‘¡ ğ›¼ğ‘¡(ğ‘ ) = âˆ, âˆ‘ğ‘¡ ğ›¼ğ‘¡(ğ‘ )2 < âˆ, âˆ‘ğ‘¡ ğ›½ğ‘¡(ğ‘ ) = âˆ, and âˆ‘ğ‘¡ ğ›½ğ‘¡(ğ‘ )2 < âˆ uniformly.
2. ğ”¼[ğ‘Ÿğ‘¡(ğ‘ ) âˆ£ ğ‘ƒğ‘¡] = 0 and there exists a constant ğ¶ âˆˆ â„+ such that ğ”¼[ğ‘Ÿğ‘¡(ğ‘ )2 âˆ£ ğ‘ƒğ‘¡] â‰¤ ğ¶ with probability one.
Lemma 6.10. Consider the stochastic process
ğ‘‹ğ‘¡+1(ğ‘ ) = ğºğ‘¡(ğ‘‹ğ‘¡,ğ‘ ),
where
ğºğ‘¡(ğ›½ğ‘‹ğ‘¡,ğ‘ ) = ğ›½ğºğ‘¡(ğ‘‹ğ‘¡,ğ‘ ).
Suppose that if the norm â€–ğ‘‹ğ‘¡â€– were kept bounded by scaling in all iterations, then ğ‘‹ğ‘¡ would converge to zero with probability one. Then the original stochastic process converges to zero with probability one.
Lemma 6.11. The stochastic process
ğ‘‹ğ‘¡+1(ğ‘ ) = (1 âˆ’ ğ›¼(ğ‘ ))ğ‘‹ğ‘¡(ğ‘ ) + ğ›¾ğ›½ğ‘¡(ğ‘ )â€–ğ‘‹ğ‘¡â€–
converges to zero with probability one if the following conditions are satisfied.
1. The state space ğ’® is finite.
2. The step sizes ğ›¼ğ‘¡(ğ‘ ) and ğ›½ğ‘¡(ğ‘ ) satisfy the equalities and inequalities âˆ‘ğ‘¡ ğ›¼ğ‘¡(ğ‘ ) = âˆ, âˆ‘ğ‘¡ ğ›¼ğ‘¡(ğ‘ )2 < âˆ, âˆ‘ğ‘¡ ğ›½ğ‘¡(ğ‘ ) = âˆ, and âˆ‘ğ‘¡ ğ›½ğ‘¡(ğ‘ )2 < âˆ uniformly.
3. The step sizes satisfy the inequality
ğ”¼[ğ›½ğ‘¡(ğ‘ )] â‰¤ ğ”¼[ğ›¼ğ‘¡(ğ‘ )]
uniformly with probability one.
6.3. Convergence Proved Using Stochastic Approximation
Theorem 6.12 (convergence of a stochastic process [42]). The stochastic process
Î”ğ‘¡+1(ğ‘ ) âˆ¶= (1 âˆ’ ğ›¼ğ‘¡(ğ‘ ))Î”ğ‘¡(ğ‘ ) + ğ›½ğ‘¡(ğ‘ )ğ¹ğ‘¡(ğ‘ )
converges to zero almost surely if the following assumptions hold.
1. The state space is finite.
2. The equalities and inequalities
âˆ‘ ğ‘¡
ğ›¼ğ‘¡ = âˆ,
âˆ‘ ğ‘¡
ğ›¼2
ğ‘¡ < âˆ,
âˆ‘ ğ‘¡
ğ›½ğ‘¡ = âˆ,
âˆ‘ ğ‘¡
ğ›½2 ğ‘¡ < âˆ,
ğ”¼[ğ›½ğ‘¡|ğ‘ƒğ‘¡] â‰¤ ğ”¼[ğ›¼ğ‘¡|ğ‘ƒğ‘¡]
are satisfied uniformly and almost surely.
3. The inequality
â€–ğ”¼[ğ¹ğ‘¡(ğ‘ )|ğ‘ƒğ‘¡]â€–ğ‘Š â‰¤ ğ¶â€–Î”ğ‘¡â€–ğ‘Š
âˆƒğ¶ âˆˆ (0,1)
holds.
4. The inequality
ğ•[ğ¹ğ‘¡(ğ‘ )|ğ‘ƒğ‘¡] â‰¤ ğ¶(1 + â€–Î”ğ‘¡â€–ğ‘Š)2
âˆƒğ¶ âˆˆ â„+
holds.
Here ğ‘ƒğ‘¡ âˆ¶= {Î”ğ‘¡,Î”ğ‘¡âˆ’1,â€¦,ğ¹ğ‘¡âˆ’1,ğ¹ğ‘¡âˆ’2,â€¦,ğ›¼ğ‘¡âˆ’1,ğ›¼ğ‘¡âˆ’2,â€¦,ğ›½ğ‘¡âˆ’1,ğ›½ğ‘¡âˆ’2,â€¦} denotes the past in iteration ğ‘›. The values ğ¹ğ‘¡(ğ‘ ), ğ›¼ğ‘¡, and ğ›½ğ‘¡ may depend on the past ğ‘ƒğ‘¡ as long as the assumptions are satisfied. Furthermore, â€–Î”ğ‘¡(ğ‘ )â€–ğ‘Š denotes the weighted maximum norm â€–Î”ğ‘¡â€–ğ‘Š âˆ¶= maxğ‘  |Î”ğ‘¡(ğ‘ )/ğ‘Š(ğ‘ )|.
Proof. The proof uses the three lemmata above. The stochastic process Î”ğ‘¡(ğ‘ ) can be decomposed into two processes
Î”ğ‘¡(ğ‘ ) = ğ›¿ğ‘¡(ğ‘ ) + ğ‘¤ğ‘¡(ğ‘ )
83
(6.2)
84
Chapter 6. Convergence of Discrete Q-Learning
by defining
ğ‘Ÿğ‘¡(ğ‘ ) âˆ¶= ğ¹ğ‘¡(ğ‘ ) âˆ’ ğ”¼[ğ¹ğ‘¡(ğ‘ ) âˆ£ ğ‘ƒğ‘¡],
ğ›¿ğ‘¡+1(ğ‘ ) âˆ¶= (1 âˆ’ ğ›¼ğ‘¡(ğ‘ ))ğ›¿ğ‘¡(ğ‘ ) + ğ›½ğ‘¡(ğ‘ )ğ”¼[ğ¹ğ‘¡(ğ‘ ) âˆ£ ğ‘ƒğ‘¡], ğ‘¤ğ‘¡+1(ğ‘ ) âˆ¶= (1 âˆ’ ğ›¼ğ‘¡(ğ‘ ))ğ‘¤ğ‘›(ğ‘ ) + ğ›½ğ‘¡(ğ‘ )ğ‘Ÿğ‘¡(ğ‘ ).
When the last theorem is applied, the stochastic process Î”ğ‘¡ is usually the difference between the iterates generated by an algorithm and an optimal value such as the optimal action-value function characterized by the Bellman optimal- ity equation.
The following is [43, Lemma 1].
Lemma 6.13 (convergence of a stochastic process). Let âŸ¨â„±ğ‘¡âŸ©ğ‘¡âˆˆâ„• be an increasing sequence of ğœ-fields. For all ğ‘¡ âˆˆ â„•, let ğ›¼ğ‘¡, ğ‘Šğ‘¡, and ğµğ‘¡ be â„±ğ‘¡-measurable scalar random variables. Suppose that the following hold with probability one:
1. ğ”¼[ğ‘Šğ‘¡|â„±ğ‘¡] = 0,
2. ğ”¼[ğ‘Š 2
ğ‘¡ |â„±ğ‘¡] â‰¤ ğµğ‘¡,
3. the sequence âŸ¨ğµğ‘¡âŸ©ğ‘¡âˆˆâ„• is bounded,
4. ğ›¼ğ‘¡ âˆˆ [0,1],
5. âˆ‘ğ‘¡âˆˆâ„• ğ›¼ğ‘¡ = âˆ, 6. âˆ‘ğ‘¡âˆˆâ„• ğ›¼2 ğ‘¡ < âˆ.
Let ğ‘‹ğ‘¡ satisfy the recursion
ğ‘‹ğ‘¡+1 = (1 âˆ’ ğ›¼ğ‘¡)ğ‘‹ğ‘¡ + ğ›¼ğ‘¡ğ‘Šğ‘¡.
Then the iterates ğ‘‹ğ‘¡ converge to zero with probability one, i.e.,
â„™[ lim ğ‘¡â†’âˆ
ğ‘‹ğ‘¡ = 0] = 1.
The first application of the last theorem is the convergence of ğ‘„-learning.
Theorem 6.14 (convergence of ğ‘„-learning). The ğ‘„-learning iterates
ğ‘„ğ‘¡+1(ğ‘ ,ğ‘) âˆ¶= (1 âˆ’ ğ›¼ğ‘¡(ğ‘ ,ğ‘))ğ‘„ğ‘¡(ğ‘ ğ‘¡,ğ‘ğ‘¡) + ğ›¼ğ‘¡(ğ‘ ,ğ‘)(ğ‘Ÿğ‘¡+1 + ğ›¾ max
ğ‘
ğ‘„ğ‘¡(ğ‘ ğ‘¡+1,ğ‘)), (6.3)
where ğ›¼ğ‘¡âˆ¶ ğ’®Ã—ğ’œ â†’ [0,1), converge to the optimal action-value function ğ‘âˆ— if the following assumptions hold.
6.3. Convergence Proved Using Stochastic Approximation
85
1. The state space ğ’® and the action space ğ’œ are finite.
2. The equality âˆ‘ğ‘¡ ğ›¼ğ‘¡(ğ‘ ,ğ‘) = âˆ and the inequality âˆ‘ğ‘¡ ğ›¼ğ‘¡(ğ‘ ,ğ‘)2 < âˆ hold for all (ğ‘ ,ğ‘) âˆˆ ğ’® Ã— ğ’œ.
3. The variance ğ•[ğ‘Ÿğ‘¡] of the rewards is bounded.
4. In the case ğ›¾ = 1, all episodes must almost surely terminate in a terminal state with zero reward.
Remark 6.15. The difference between the two ğ‘„-learning iterations (6.1) and (6.3) is in the step sizes ğ›¼ğ‘¡. In the first form (6.1), the step size ğ›¼ğ‘¡ is a real number and it is clear that ğ‘„ğ‘¡ and ğ‘„ğ‘¡+1 differ only in their values for a single argument pair (ğ‘ ,ğ‘). In the second form (6.3), the step size ğ›¼ğ‘¡ is a function of ğ‘  and ğ‘. The assumption âˆ‘ğ‘¡ ğ›¼ğ‘¡(ğ‘ ,ğ‘) = âˆ (and the bound 0 â‰¤ ğ›¼ğ‘¡(ğ‘ ,ğ‘) < 1 for all ğ‘  and ğ‘) again ensures that each pair (ğ‘ ,ğ‘) is visited infinitely often (as in Theorem 6.1).
Proof. The basic idea in applying Theorem 6.12, where the stochastic process converges to zero, is to consider the difference between the stochastic process calculated by the algorithm and the supposed limit value, which is characterized here by the Bellman optimality equation.
We start by defining the operator ğ¾âˆ¶ (ğ’® Ã— ğ’œ â†’ â„) â†’ â„ as
(ğ¾ğ‘)(ğ‘ ,ğ‘) âˆ¶= âˆ‘ ğ‘ â€²
ğ‘(ğ‘ â€² âˆ£ ğ‘ ,ğ‘)(ğ‘Ÿ(ğ‘ ,ğ‘,ğ‘ â€²) + ğ›¾ max
ğ‘â€²
ğ‘(ğ‘ â€²,ğ‘â€²)).
Recall that the expected reward is given by
ğ‘Ÿ(ğ‘ ,ğ‘,ğ‘ â€²) = ğ”¼[ğ‘…ğ‘¡ âˆ£ ğ‘†ğ‘¡âˆ’1 = ğ‘ , ğ´ğ‘¡âˆ’1 = ğ‘, ğ‘†ğ‘¡ = ğ‘ â€²] = âˆ‘ ğ‘Ÿâˆˆâ„›
ğ‘(ğ‘ â€²,ğ‘Ÿ âˆ£ ğ‘ ,ğ‘) ğ‘(ğ‘ â€² âˆ£ ğ‘ ,ğ‘)
(see (2.3)). To show that ğ¾ is a contraction with respect to the maximum norm (with respect to both ğ‘  and ğ‘), we calculate
â€–ğ¾ğ‘1 âˆ’ ğ¾ğ‘2â€–âˆ = max ğ‘ , ğ‘
âˆ£âˆ‘ ğ‘ â€²
ğ‘(ğ‘ â€² âˆ£ ğ‘ ,ğ‘)(ğ‘Ÿ(ğ‘ ,ğ‘,ğ‘ â€²) + ğ›¾ max
ğ‘â€²
ğ‘1(ğ‘ â€²,ğ‘â€²) âˆ’ ğ‘Ÿ(ğ‘ ,ğ‘,ğ‘ â€²) âˆ’ ğ›¾ max ğ‘â€²
â‰¤ ğ›¾ max ğ‘ , ğ‘
âˆ‘ ğ‘ â€²
ğ‘(ğ‘ â€² âˆ£ ğ‘ ,ğ‘)âˆ£max
ğ‘â€²
ğ‘1(ğ‘ â€²,ğ‘â€²) âˆ’ max ğ‘â€²
ğ‘2(ğ‘ â€²,ğ‘â€²)âˆ£
â‰¤ ğ›¾ max ğ‘ , ğ‘
âˆ‘ ğ‘ â€²
ğ‘(ğ‘ â€² âˆ£ ğ‘ ,ğ‘) max ğ‘ â€³, ğ‘â€²
|ğ‘1(ğ‘ â€³,ğ‘â€²) âˆ’ ğ‘2(ğ‘ â€³,ğ‘â€²)|
= ğ›¾ max ğ‘ , ğ‘
âˆ‘ ğ‘ â€²
ğ‘(ğ‘ â€² âˆ£ ğ‘ ,ğ‘)â€–ğ‘1 âˆ’ ğ‘2â€–âˆ
ğ‘2(ğ‘ â€²,ğ‘â€²))âˆ£
86
Chapter 6. Convergence of Discrete Q-Learning
= ğ›¾â€–ğ‘1 âˆ’ ğ‘2â€–âˆ.
In summary,
â€–ğ¾ğ‘1 âˆ’ ğ¾ğ‘2â€–âˆ â‰¤ ğ›¾â€–ğ‘1 âˆ’ ğ‘2â€–âˆ.
(6.4)
The Bellman optimality equation (2.8) for ğ‘âˆ— implies that ğ‘âˆ— is a fixed point
of ğ¾, i.e.,
ğ¾ğ‘âˆ— = ğ‘âˆ—.
(6.5)
In order to relate the iteration (6.3) to the stochastic process in Theorem 6.12,
we define
ğ›½ğ‘¡ âˆ¶= ğ›¼ğ‘¡,
Î”ğ‘¡(ğ‘ ,ğ‘) âˆ¶= ğ‘„ğ‘¡(ğ‘ ,ğ‘) âˆ’ ğ‘âˆ—(ğ‘ ,ğ‘), ğ¹ğ‘¡(ğ‘ ,ğ‘) âˆ¶= ğ‘Ÿğ‘¡+1 + ğ›¾ max
ğ‘„ğ‘¡(ğ‘ ğ‘¡+1,ğ‘) âˆ’ ğ‘âˆ—(ğ‘ ,ğ‘).
ğ‘
With these definitions, the ğ‘„-learning iteration (6.3) and the stochastic process (6.2) are identical.
The expected value of ğ¹ğ‘¡(ğ‘ ,ğ‘) given the past ğ‘ƒğ‘¡ as it appears in Theorem 6.12
is
ğ”¼[ğ¹ğ‘¡(ğ‘ ,ğ‘) âˆ£ ğ‘ƒğ‘¡] = âˆ‘ ğ‘ â€²
ğ‘(ğ‘ â€²,ğ‘Ÿğ‘¡+1 âˆ£ ğ‘ ,ğ‘)(ğ‘Ÿğ‘¡+1 + ğ›¾ max
ğ‘
ğ‘„ğ‘¡(ğ‘ â€²,ğ‘) âˆ’ ğ‘âˆ—(ğ‘ ,ğ‘))
= (ğ¾ğ‘„ğ‘¡)(ğ‘ ,ğ‘) âˆ’ ğ‘âˆ—(ğ‘ ,ğ‘) = (ğ¾ğ‘„ğ‘¡)(ğ‘ ,ğ‘) âˆ’ (ğ¾ğ‘âˆ—)(ğ‘ ,ğ‘)
âˆ€(ğ‘ ,ğ‘) âˆˆ ğ’® Ã— ğ’œ,
where the last equation follows from (6.5).
Using (6.4), this yields
â€–ğ”¼[ğ¹ğ‘¡(ğ‘ ,ğ‘) âˆ£ ğ‘ƒğ‘¡]â€–âˆ â‰¤ ğ›¾â€–ğ‘„ğ‘¡ âˆ’ ğ‘âˆ—â€–âˆ = ğ›¾â€–Î”ğ‘¡â€–âˆ,
which means that the third assumption of Theorem 6.12 is satisfied if ğ›¾ < 1. In order to check the fourth assumption Theorem 6.12, we calculate
ğ•[ğ¹ğ‘¡(ğ‘ ,ğ‘) âˆ£ ğ‘ƒğ‘¡] = ğ”¼[(ğ‘Ÿğ‘¡+1 + ğ›¾ max
ğ‘
ğ‘„ğ‘¡(ğ‘ ğ‘¡+1,ğ‘) âˆ’ ğ‘âˆ—(ğ‘ ,ğ‘) âˆ’ ((ğ¾ğ‘„ğ‘¡)(ğ‘ ,ğ‘) âˆ’ ğ‘âˆ—(ğ‘ ,ğ‘)))
= ğ”¼[(ğ‘Ÿğ‘¡+1 + ğ›¾ max
ğ‘ = ğ•[ğ‘Ÿğ‘¡+1 + ğ›¾ max
ğ‘
ğ‘„ğ‘¡(ğ‘ ğ‘¡+1,ğ‘) âˆ’ (ğ¾ğ‘„ğ‘¡)(ğ‘ ,ğ‘))
ğ‘„ğ‘¡(ğ‘ ğ‘¡+1,ğ‘) âˆ£ ğ‘ƒğ‘¡].
2
]
Since the variance ğ•[ğ‘Ÿğ‘¡] of the rewards is bounded by the third assumption, we hence find
ğ•[ğ¹ğ‘¡(ğ‘ ,ğ‘) âˆ£ ğ‘ƒğ‘¡] â‰¤ ğ¶(1 + â€–Î”ğ‘¡â€–ğ‘Š)2
âˆƒğ¶ âˆˆ â„+.
2
]
6.4. Bibliographical and Historical Remarks
In the case ğ›¾ = 1, the usual assumptions that ensure that all episodes are
finite are necessary.
In summary, all assumptions of Theorem 6.12 are satisfied.
The second application of Theorem 6.12 is the convergence of TD(ğœ†).
Theorem 6.16 (convergence of TD(ğœ†)). Suppose that the lengths of the episodes are finite, that there is no inaccessible state in the distribution of the starting states, that the reward distribution has finite variance, that the step sizes satisfy âˆ‘ğ‘¡ ğ›¼ğ‘¡(ğ‘ ) = âˆ and âˆ‘ğ‘¡ ğ›¼ğ‘¡(ğ‘ )2 < âˆ, and that ğ›¾ğœ† < 1 holds for ğ›¾ âˆˆ [0,1] and ğœ† âˆˆ [0,1]. Then the iterates ğ‘‰ğ‘¡ in the TD(ğœ†) algorithm almost surely converge to the optimal prediction ğ‘£âˆ—.
6.4 Bibliographical and Historical Remarks
ğ‘„-learning was introduced in [39, page 95] and demonstrated at the example of a route-finding problem and a Skinner box. A first convergence proof for one-step ğ‘„-learning was given there as well [39, Appendix 1]. An extended, more detailed version of this proof was given in [40]. Convergence proofs based on stochastic processes and stochastic approximation were given in [42], [43], and [36, Section 5.6]. Further references [44, 45, 46].
87
88
6.5 Exercises
Chapter 6. Convergence of Discrete Q-Learning
Chapter 7
On-Policy Prediction with Approximation
7.1 Introduction
Starting in this chapter, we consider the case of infinite state sets ğ’®. Then it is obviously not possible anymore to store the state-value function (or the policy) in tabular form, but instead we have to approximate it. We write
Ì‚ğ‘£ğ‘¤(ğ‘ ) â‰ˆ ğ‘£ğœ‹(ğ‘ )
for the approximation Ì‚ğ‘£ğ‘¤ of the state-value function ğ‘£ğœ‹ based on the a weight vector ğ‘¤ âˆˆ ğ‘Š âŠ‚ â„ğ‘‘. Typically, the dimension of the weight vector is much smaller than the dimension of the state space, i.e.,
ğ‘‘ â‰ª dimğ’®.
This implies that change one element of the weight vector changes the estimated value of many states, resulting in generalization. This effect has advantages and disadvantages: it may make learning more efficient, but it also may make it more difficult if the type of approximation used does not support the kind of value functions required by the problem well, i.e., when the kind of approximation prevents generalization.
So far, in the tabular methods we have discussed for finite state spaces, the estimate for the value of a state was updated to be closer to a certain target value and all other estimates remained unchanged. Now, when function approximation is employed, updating the estimate of a state may affect the estimate values of many other states as well.
89
90
Chapter 7. On-Policy Prediction with Approximation
In principle, any kind of function approximation can be used. Whenever a new sample, i.e., a state (and action) and an estimate of its value becomes available, the function approximation can be updated globally. Of course, ap- proximation methods that support such online updates are especially suitable.
7.2 Stochastic Gradient and Semi-Gradient Meth-
ods
Before we can start to calculate the weights in the approximations of the value functions, we must specify the optimization objective or the error we seek to minimize. One of the most popular errors is the mean squared value error
VE(ğ‘¤) âˆ¶= âˆ‘ ğ‘ âˆˆğ’®
ğœ‡ğœ‹(ğ‘ )(ğ‘£ğœ‹(ğ‘ ) âˆ’ Ì‚ğ‘£ğ‘¤(ğ‘ ))2,
where ğœ‡ğœ‹ is the discounted state distribution (see Definition 8.1). The discounted state distribution acts as weight for the squared differences in the true and approximated values. Of course, other weights can be used whenever it makes sense to assign different importance to the states.
The goal is to find a global optimum ğ‘¤âˆ—, i.e., a weight vector ğ‘¤âˆ— such that VE(ğ‘¤âˆ—) â‰¤ VE(ğ‘¤) for all ğ‘¤ âˆˆ ğ‘Š âŠ‚ â„ğ‘‘. It is sometimes possible to show that a global optimum is found when linear function approximations are used, but it becomes much harder in the case of nonlinear function approximation.
Short of finding a global optimum, the goal is to find a local optimum, i.e., a weight vector ğ‘¤âˆ— such that VE(ğ‘¤âˆ—) â‰¤ VE(ğ‘¤) holds for all ğ‘¤ in a neighborhood of ğ‘¤âˆ—.
The most popular method for function approximations is stochastic gradient descent (SGD), and it is very well suited for online learning. In SGD, it is assumed that the approximate value function Ì‚ğ‘£ğ‘¤ is a differentiable function of the weight vector ğ‘¤. The weight vector calculated in each iteration is denoted by ğ‘¤ğ‘¡ for ğ‘¡ âˆˆ {0,1,2,â€¦}. We assume for now that in each iteration a new sample ğ‘£ğœ‹(ğ‘†ğ‘¡) becomes available having reached state ğ‘†ğ‘¡. SGD means improving the weight vector ğ‘¤ğ‘¡ by moving it slightly downhill with respect to the error VE in the direction of the greatest change in the error at ğ‘¤ğ‘¡. This direction of greatest change is the gradient, and minimizing the error means adding a small multiple of the negative gradient. This results in the iteration
ğ‘¤ğ‘¡+1 âˆ¶= ğ‘¤ğ‘¡ âˆ’
1 2
ğ›¼ğ‘¡âˆ‡ğ‘¤(ğ‘£ğœ‹(ğ‘†ğ‘¡) âˆ’ Ì‚ğ‘£ğ‘¤ğ‘¡
(ğ‘†ğ‘¡))2
(7.1a)
= ğ‘¤ğ‘¡ + ğ›¼ğ‘¡(ğ‘£ğœ‹(ğ‘†ğ‘¡) âˆ’ Ì‚ğ‘£ğ‘¤ğ‘¡
(ğ‘†ğ‘¡))âˆ‡ğ‘¤Ì‚ğ‘£ğ‘¤ğ‘¡
(ğ‘†ğ‘¡),
(7.1b)
7.2. Stochastic Gradient and Semi-Gradient Methods
where the learning rate ğ›¼ğ‘¡ âˆˆ â„+. The sole purpose of the factor 1/2 in the first line is to not have a factor 2 in the second line.
SGD is a stochastic gradient-descent method since the update is a random variable because it depends on the random variable ğ‘†ğ‘¡. Over many samples or iterations, the accumulated effect is to minimize the average of an objective function such as the mean squared value error. To ensure convergence, the learning rate ğ›¼ğ‘¡ must tend to zero.
Unfortunately, the true value ğ‘£ğœ‹(ğ‘†ğ‘¡) is unknown, since ğ‘£ğœ‹ is to be calculated. Therefore, in fact, we can only use a random variable ğ‘ˆğ‘¡ instead of ğ‘£ğœ‹(ğ‘†ğ‘¡) in the iteration. Hence the general SGD method for the prediction of state values is the iteration
ğ‘¤ğ‘¡+1 âˆ¶= ğ‘¤ğ‘¡ + ğ›¼ğ‘¡(ğ‘ˆğ‘¡ âˆ’ Ì‚ğ‘£ğ‘¤ğ‘¡
(ğ‘†ğ‘¡))âˆ‡ğ‘¤Ì‚ğ‘£ğ‘¤ğ‘¡
(ğ‘†ğ‘¡).
If ğ‘ˆğ‘¡ is an unbiased estimate of ğ‘£ğœ‹(ğ‘†ğ‘¡), i.e., if
ğ”¼[ğ‘ˆğ‘¡ âˆ£ ğ‘†ğ‘¡ = ğ‘ ] = ğ‘£ğœ‹(ğ‘†ğ‘¡)
for all times ğ‘¡ and if the learning rate ğ›¼ satisfy the conditions (2.2) for stochastic approximation, then the ğ‘¤ğ‘¡ converge to a local optimum.
Equipped with the SGD method, we can now develop algorithms for calcu-
lating ğ‘¤âˆ— based on different choices for the target value ğ‘ˆğ‘¡.
Probably the most obvious choice for an unbiased estimate of ğ‘£ğœ‹(ğ‘†ğ‘¡) is the
Monte-Carlo target
ğ‘ˆğ‘¡ âˆ¶= ğºğ‘¡.
Based on the convergence results just mentioned, the general SGD method in conjunction with this estimate converges to a locally optimal approximation of ğ‘£ğœ‹(ğ‘†ğ‘¡). In other words, the algorithm for the Monte-Carlo state-value prediction can be shown to always find a locally optimal solution. The resulting algorithm is shown in Algorithm 14. Note that the episode must have ended so that ğºğ‘¡ can be calculated in each time step.
Bootstrapping targets such as the ğ‘›-step return ğºğ‘¡âˆ¶ğ‘¡+ğ‘›, which build on pre- viously calculated approximations, do not provide the same convergence guar- antees. By the definition of bootstrapping, the target ğ‘ˆğ‘¡ in a bootstrapping method depends on the current weight vector ğ‘¤ğ‘¡, which means that the esti- mate is biased.
Bootstrapping methods are not even gradient-descent methods. This be- comes clear by considering the derivative calculated in (7.1). While the deriva- (ğ‘†ğ‘¡) appears in the equation, in a bootstrapping method the derivative tive of Ì‚ğ‘£ğ‘¤ğ‘¡ of ğ‘ˆğ‘¡(ğ‘¤ğ‘¡) â‰ˆ ğ‘£ğœ‹(ğ‘†ğ‘¡) is nonzero, but does not appear in the equation. Because of this missing term, these methods are called semigradient methods.
91
(7.2)
92
Chapter 7. On-Policy Prediction with Approximation
Algorithm 14 gradient MC prediction for calculating Ì‚ğ‘£ğ‘¤ â‰ˆ ğ‘£ğœ‹ given the pol- icy ğœ‹.
initialization: choose a representation for the state-value function Ì‚ğ‘£ğ‘¤ choose learning rate ğ›¼ğ‘¡ âˆˆ â„+ initialize state-value parameter ğ‘¤ âˆˆ ğ‘Š âŠ‚ â„ğ‘‘
loop
â–· for all episodes
generate an episode (ğ‘†0,ğ´0,ğ‘…1,â€¦,ğ‘†ğ‘‡âˆ’1,ğ´ğ‘‡âˆ’1,ğ‘…ğ‘‡) following ğœ‹ for ğ‘¡ âˆˆ (0,1,â€¦,ğ‘‡ âˆ’ 1) do
â–· for all time steps
ğ‘¤ âˆ¶= ğ‘¤ + ğ›¼ğ‘¡(ğºğ‘¡ âˆ’ Ì‚ğ‘£ğ‘¤(ğ‘†ğ‘¡))âˆ‡Ì‚ğ‘£ğ‘¤(ğ‘†ğ‘¡)
end for
end loop
On the other hand, semigradient methods often learn significantly faster and they converge reliably in the important case of linear function approximation. Additionally, they enable online learning in contrast to MC methods, which have to wait till the end of an episode.
The most straightforward semigradient method is probably the semigradient
TD(0) method, which uses the target
ğ‘ˆğ‘¡ âˆ¶= ğ‘…ğ‘¡+1 + ğ›¾Ì‚ğ‘£ğ‘¤ğ‘¡
(ğ‘†ğ‘¡+1).
The resulting algorithm is shown in Algorithm 15.
7.3 Linear Function Approximation
One of the most important cases when approximating the state-value function is the case of linear function approximation. Then the state-value function ğ‘£ğœ‹ is approximated by
ğ‘£ğœ‹(ğ‘ ) â‰ˆ Ì‚ğ‘£ğ‘¤(ğ‘ ) âˆ¶= ğ‘¤âŠ¤ğ‘¥(ğ‘ ) =
ğ‘‘ âˆ‘ ğ‘˜=1
ğ‘¤ğ‘–ğ‘¥ğ‘–(ğ‘ ).
(7.3)
The vector valued function
ğ‘¥âˆ¶ ğ’® â†’ â„ğ‘‘,
ğ‘¥(ğ‘ ) = â›âœ â
ğ‘¥1(ğ‘ ) â‹® ğ‘¥ğ‘‘(ğ‘ )
ââŸ â 
gives the feature vectors or simply features, whose expedient choice is crucial.
7.3. Linear Function Approximation
93
Algorithm 15 semigradient TD(0) prediction for calculating Ì‚ğ‘£ğ‘¤ â‰ˆ ğ‘£ğœ‹ given the policy ğœ‹.
initialization: choose a representation for the state-value function Ì‚ğ‘£ğ‘¤ choose learning rate ğ›¼ğ‘¡ âˆˆ â„+ initialize state-value parameter ğ‘¤ âˆˆ ğ‘Š âŠ‚ â„ğ‘‘
loop
â–· for all episodes
initialize ğ‘  repeat
â–· for all time steps
choose action ğ‘ using ğœ‹ take action ğ‘ and receive the new state ğ‘ â€² and the reward ğ‘Ÿ ğ‘¤ âˆ¶= ğ‘¤ + ğ›¼ğ‘¡(ğ‘Ÿ + ğ›¾Ì‚ğ‘£ğ‘¤(ğ‘ â€²) âˆ’ Ì‚ğ‘£ğ‘¤(ğ‘ ))âˆ‡Ì‚ğ‘£ğ‘¤(ğ‘ ) ğ‘  âˆ¶= ğ‘ â€²
until ğ‘  is the terminal state and the episode is finished
end loop
In other words, the features are the basis functions that span the subspace of all approximations of ğ‘£ğ‘¤. Unfortunately, the subspace is often rather small, i.e., ğ‘‘ â‰ª dimğ’®, due to problem size or computational limitations. Obviously, the choice of the subspace (which is equivalent to the choice of the features) is instrumental in being able to calculate good approximations to ğ‘£ğœ‹ at all.
When using linear approximations, the SGD iteration simplifies. The gra- dient of the approximated state-value function is just the feature function, i.e., âˆ‡ğ‘¤Ì‚ğ‘£ğ‘¤(ğ‘ ) = ğ‘¥(ğ‘ ). Hence the update (7.2) becomes
ğ‘¤ğ‘¡+1 âˆ¶= ğ‘¤ğ‘¡ + ğ›¼ğ‘¡(ğ‘ˆğ‘¡ âˆ’ Ì‚ğ‘£ğ‘¤ğ‘¡
(ğ‘†ğ‘¡))ğ‘¥(ğ‘†ğ‘¡).
Almost all convergence results that are known are for the case of linear function approximation. In the linear case, there is only one global optimum or â€“ more precisely â€“ a set of equally good optima. This means that convergence to a local optimum implies global convergence.
For example, the gradient MC algorithm Algorithm 14 when combined with linear function approximation converges to a local minimum of the mean squared value error if the learning rate satisfies the usual conditions (2.2).
The semigradient algorithm Algorithm 15 also converges, but in general to
a different limit, and this fact does not follow from the considerations above.
Assuming that the algorithm converges, we can find the limit using the
94
Chapter 7. On-Policy Prediction with Approximation
following consideration. Using the notation ğ‘¥ğ‘¡ âˆ¶= ğ‘¥(ğ‘†ğ‘¡), the iteration is
ğ‘¤ğ‘¡+1 âˆ¶= ğ‘¤ğ‘¡ + ğ›¼ğ‘¡(ğ‘…ğ‘¡+1 + ğ›¾ğ‘¤âŠ¤
ğ‘¡ ğ‘¥ğ‘¡+1 âˆ’ ğ‘¤âŠ¤
ğ‘¡ ğ‘¥ğ‘¡)ğ‘¥ğ‘¡
= ğ‘¤ğ‘¡ + ğ›¼ğ‘¡(ğ‘…ğ‘¡+1ğ‘¥ğ‘¡ âˆ’ ğ‘¥ğ‘¡(ğ‘¥ğ‘¡ âˆ’ ğ›¾ğ‘¥ğ‘¡+1)âŠ¤ğ‘¤ğ‘¡).
Applying the expected value to both sides, we find
ğ”¼[ğ‘¤ğ‘¡+1 âˆ£ ğ‘¤ğ‘¡] = ğ‘¤ğ‘¡ + ğ›¼ğ‘¡(ğ‘ âˆ’ ğ´ğ‘¤ğ‘¡),
ğ´ âˆ¶= ğ”¼[ğ‘¥ğ‘¡(ğ‘¥ğ‘¡ âˆ’ ğ›¾ğ‘¥ğ‘¡+1)âŠ¤] âˆˆ â„ğ‘‘Ã—ğ‘‘, ğ‘ âˆ¶= ğ”¼[ğ‘…ğ‘¡+1ğ‘¥ğ‘¡] âˆˆ â„ğ‘‘.
Hence, if the ğ‘¤ğ‘¡ converge, the equation ğ´ğ‘¤ğ‘¡ = ğ‘ must hold, which means that the possible fixed point is
ğ‘¤ğ‘‡ğ· âˆ¶= ğ´âˆ’1ğ‘,
which is called the TD fixed point.
Theorem 7.1. Suppose that ğ›¾ < 1, that âŸ¨ğ›¼ğ‘¡âŸ©ğ‘¡âˆˆâ„• satisfies (2.2), that the feature vectors ğ‘¥(ğ‘ ) are a basis of â„ğ‘‘, and that the state distribution is positive for all states.
Then the semigradient algorithm Algorithm 15 with the linear approximation (7.3) converges to the TD fixed point ğ‘¤ğ‘‡ğ· defined in (7.4) with probability one.
Sketch of the proof. The calculation above shows that
ğ”¼[ğ‘¤ğ‘¡+1 âˆ£ ğ‘¤ğ‘¡] = (ğ¼ âˆ’ ğ›¼ğ‘¡ğ´)ğ‘¤ğ‘¡ + ğ›¼ğ‘.
Therefore we define the function
ğ¾âˆ¶ â„ğ‘‘ â†’ â„ğ‘‘,
ğ¾(ğ‘¤) âˆ¶= (ğ¼ âˆ’ ğ›¼ğ´)ğ‘¤ + ğ›¼ğ‘.
In order to be able to use the Banach fixed-point theorem, we will show that ğ¾ is a contraction for sufficiently small ğ›¼ âˆˆ â„+. It is a contraction if
â€–ğ¾(ğ‘£2) âˆ’ ğ¾(ğ‘£1)â€– â‰¤ â€–ğ¼ âˆ’ ğ›¼ğ´â€–â€–ğ‘£2 âˆ’ ğ‘£1â€– â‰¤ ğœ…â€–ğ‘£2 âˆ’ ğ‘£1â€–
âˆƒğœ… âˆˆ â„+.
Therefore it suffices to show that ğ´ is positive definite, i.e., ğ‘£âŠ¤ğ´ğ‘£ > 0 for all ğ‘£ âˆˆ â„ğ‘‘ âˆ– {0}.
We can write the matrix ğ´ as
ğ´ = âˆ‘ ğ‘ âˆˆğ’®
ğœ‡(ğ‘ )âˆ‘ ğ‘âˆˆğ’œ
ğœ‹(ğ‘|ğ‘ )âˆ‘ ğ‘Ÿâˆˆâ„›
âˆ‘ ğ‘ â€²âˆˆğ’®
ğ‘(ğ‘Ÿ,ğ‘ â€² âˆ£ ğ‘ ,ğ‘)ğ‘¥(ğ‘ )(ğ‘¥(ğ‘ ) âˆ’ ğ›¾ğ‘¥(ğ‘ â€²))âŠ¤
= âˆ‘ ğ‘ âˆˆğ’®
ğœ‡(ğ‘ )âˆ‘ ğ‘ â€²âˆˆğ’®
ğ‘(ğ‘ â€²|ğ‘ )ğ‘¥(ğ‘ )(ğ‘¥(ğ‘ ) âˆ’ ğ›¾ğ‘¥(ğ‘ â€²))âŠ¤
(7.4)
7.4. Features for Linear Methods
= âˆ‘ ğ‘ âˆˆğ’®
ğœ‡(ğ‘ )ğ‘¥(ğ‘ )(ğ‘¥(ğ‘ ) âˆ’ ğ›¾ âˆ‘ ğ‘ â€²âˆˆğ’®
ğ‘(ğ‘ â€²|ğ‘ )ğ‘¥(ğ‘ â€²))
âŠ¤
= ğ‘‹âŠ¤ğ·(ğ¼ âˆ’ ğ›¾ğ‘ƒ)ğ‘‹,
where ğœ‡ğœ‹ is the state distribution under ğœ‹, ğ· is the diagonal matrix with the entries ğœ‡(ğ‘ ), ğ‘ƒ is the dimğ’®Ã—dimğ’® matrix of the transition probabilities ğ‘(ğ‘ â€²|ğ‘ ) from state ğ‘  to state ğ‘ â€² under ğœ‹, and ğ‘‹ is the dimğ’® Ã— ğ‘‘ matrix that contains the ğ‘¥(ğ‘ ) as rows.
Since ğ‘‹ is a basis change, it suffices to show that ğ·(ğ¼ âˆ’ ğ›¾ğ‘ƒ) is positive definite. It can shown that it suffices to show that all of its column sums and all of its row sums are positive.
The row sums of ğ·(ğ¼âˆ’ğ›¾ğ‘ƒ) are all positive, since ğ‘ƒ is a matrix of probabilities
and ğ›¾ < 1.
To show that the columns sums of ğ·(ğ¼ âˆ’ ğ›¾ğ‘ƒ) are all positive, we calculate
them as
1âŠ¤ğ·(1 âˆ’ ğ›¾ğ‘ƒ) = ğœ‡âŠ¤(ğ¼ âˆ’ ğ›¾)ğ‘ƒ = ğœ‡âŠ¤ âˆ’ ğ›¾ğœ‡âŠ¤ğ‘ƒ = ğœ‡âŠ¤ âˆ’ ğ›¾ğœ‡âŠ¤ = (1 âˆ’ ğ›¾)ğœ‡âŠ¤.
Each entry of this vector is positive, since the state distribution is positive everywhere by assumption.
Since the optimal weight vector ğ‘¤âˆ— and the TD fixed point ğ‘¤ğ‘‡ğ· are different in general, the question arises how close they are. The following theorem means that the mean square value error of the TD fixed point is always within a factor of 1/(1 âˆ’ ğ›¾) of the lowest possible error.
Theorem 7.2. The TD fixed point ğ‘¤ğ‘‡ğ· in (7.4) satisfies the inequality
VE(ğ‘¤ğ‘‡ğ·) â‰¤
1 1 âˆ’ ğ›¾
min ğ‘¤âˆˆâ„ğ‘‘
VE(ğ‘¤).
7.4 Features for Linear Methods
7.4.1 Polynomials
But consider Weierstrass approximation theorem.
95
96
Chapter 7. On-Policy Prediction with Approximation
7.4.2 Fourier Basis
ğ‘–th feature:
ğ‘¥ğ‘–(ğ‘ ) âˆ¶= cos(ğœ‹ğ‘ âŠ¤ğ‘ğ‘–),
where ğ‘ğ‘– is a constant vector.
7.4.3 Coarse Coding
Cover state space with circles. A feature has value 1 (or is said to be present), if it inside the corresponding circle. Otherwise it has value 0 (and is said to be absent).
7.4.4 Tile Coding
Cover state space with tiles. First construct tiling (a partition), the shift the tilings.
7.4.5 Radial Basis Functions
The features are
ğ‘¥ğ‘–(ğ‘ ) âˆ¶= exp(âˆ’
â€–ğ‘  âˆ’ ğ‘ğ‘–â€–2 2ğœ2 ğ‘–
),
where ğ‘ğ‘– is called the center state and ğœğ‘– is called the feature width.
7.5 Nonlinear Function Approximation
7.5.1 Artificial Neural Networks
See bonus chapter.
7.5.2 Memory Based Function Approximation
Save training samples in memory, use a set of samples to compute the value estimate only when required. Also called lazy learning.
7.5.3 Kernel-Based Function Approximation
We denote the set of all stored examples by ğ¸. Then the state-value function is approximated as
Ì‚ğ‘£ğ¸(ğ‘ ) âˆ¶= âˆ‘ ğ‘ â€²âˆˆğ¸
ğ‘˜(ğ‘ ,ğ‘ â€²)ğ‘”(ğ‘ â€²),
7.6. Bibliographical and Historical Remarks
where ğ‘”(ğ‘ â€²) is the target for state ğ‘ â€² and ğ‘˜âˆ¶ ğ’® Ã— ğ’® â†’ â„ is a kernel function that assigns a weight to the known data about state ğ‘ â€² when asked about state ğ‘ .
7.6 Bibliographical and Historical Remarks
[47]
Problems
97
98
Chapter 7. On-Policy Prediction with Approximation
Chapter 8
Policy-Gradient Methods
8.1 Introduction
A large class of reinforcement-learning methods are action-value methods, i.e., methods that calculate action values and then choose the best action based on these action values. On the other hand, we present methods for calculating policies more directly in this chapter. The policies here are parameterized, i.e., we write them in the form
ğœ‹âˆ¶ ğ’œ(ğ‘ ) Ã— ğ’® Ã— Î˜ â†’ [0,1], ğœ‹ğœƒ(ğ‘ âˆ£ ğ‘ ) âˆ¶= ğœ‹(ğ‘ âˆ£ ğ‘ ,ğœƒ) âˆ¶= â„™{ğ´ğ‘¡ = ğ‘ âˆ£ ğ‘†ğ‘¡ = ğ‘ , ğœƒğ‘¡ = ğœƒ},
where the parameter ğœƒ âˆˆ Î˜ âŠ‚ â„ğ‘‘â€² corresponds to an optimal policy.
is a vector. We seek a parameter that
Commonly, the parameters are learned such that a scalar performance mea-
sure called
ğ½ âˆ¶ Î˜ â†’ â„,
defined on the parameters, is maximized. A leading example is the definition
ğ½(ğœƒ) âˆ¶= ğ”¼[ğ‘£ğœ‹ğœƒ
(ğ‘†0) âˆ£ ğ‘†0 âˆ¼ ğœ„],
where ğ‘†0 âˆ¼ ğœ„ is the initial state chosen according to the probability distribution ğœ„. The general assumption is that the policy ğœ‹ and the performance measure ğ½ are differentiable with respect to ğœƒ such that gradient based optimization can be employed. Such methods are called policy-gradient methods. The performance can be maximized for example by using stochastic gradient ascent with respect to the performance measure ğ½, i.e., we define the iteration
ğœƒğ‘¡+1 âˆ¶= ğœƒğ‘¡ + ğ›¼ğ‘¡ğ”¼[âˆ‡ğœƒğ½(ğœƒğ‘¡)].
99
100
Chapter 8. Policy-Gradient Methods
The expected value of the gradient of the performance measure ğ½ in the last term is usually approximated.
The question whether action-value or policy-gradient methods are preferable depends on the problem. It may be the case that the action-value function has a simpler structure and is therefore easier to learn, or it may be the case that the policy itself has a simpler structure.
8.2 Finite and Infinite Action Sets
8.2.1 Finite Action Sets
If the action sets ğ’œ(ğ‘ ) are finite, then a common form of the policy is based on the so-called preference function
â„âˆ¶ ğ’® Ã— ğ’œ(ğ‘ ) Ã— Î˜ â†’ â„.
The preferences of state-action pairs (ğ‘ ,ğ‘) are translated into probabilities and hence into a policy via the exponential soft-max function
ğœ‹(ğ‘ âˆ£ ğ‘ ,ğœƒ) âˆ¶=
exp(â„(ğ‘ ,ğ‘,ğœƒ)) âˆ‘ğ‘â€²âˆˆğ’œ(ğ‘ ) exp(â„(ğ‘ ,ğ‘â€²,ğœƒ))
.
Many choices for the representation of the preference functions are possible.
Two popular ones are the following.
The preference function is an artificial neural network. Then the parameter vector ğœƒ âˆˆ Î˜ âŠ‚ â„ğ‘‘â€² contains all weights and biases of the artificial neural network.
The preference function has the linear form
â„(ğ‘ ,ğ‘,ğœƒ) âˆ¶= ğœƒâŠ¤ğ‘¥(ğ‘ ,ğ‘),
where the feature function
ğ‘¥âˆ¶ ğ’® Ã— ğ’œ(ğ‘ ) â†’ â„ğ‘‘â€²
yields the feature vectors.
8.3. The Policy-Gradient Theorem
8.2.2 Infinite Action Sets
If the action sets ğ’œ(ğ‘ ) are infinite, it is possible to simplify the problem of learning the probabilities of all actions by reducing it to learning the parameters of a probability distribution. The parameters of the distribution are represented by functions. For example, we define a policy of the form
ğœ‹(ğ‘ âˆ£ ğ‘ ,ğœƒ) âˆ¶=
âˆš
1 2ğœ‹ğœ(ğ‘ ,ğœƒ)
exp(âˆ’
(ğ‘ âˆ’ ğœ‡(ğ‘ ,ğœƒ))2
2ğœ(ğ‘ ,ğœƒ)2 ),
where now the two functions ğœ‡âˆ¶ ğ’® Ã— Î˜ â†’ â„ and ğœâˆ¶ ğ’® Ã— Î˜ â†’ â„+ need to be learned. To do that, we split the parameter vector ğœƒ âˆˆ Î˜ into two vectors ğœƒğœ‡ and ğœƒğœ, i.e., ğœƒ = (ğœƒğœ‡,ğœƒğœ)âŠ¤, and write the functions ğœ‡ and ğœ as a linear and a positive function
ğœ‡(ğ‘ ,ğœƒ) âˆ¶= ğœƒâŠ¤ ğœ(ğ‘ ,ğœƒ) âˆ¶= exp(ğœƒâŠ¤
ğœ‡ğ‘¥ğœ‡(ğ‘ ),
ğœğ‘¥ğœ(ğ‘ )),
respectively, where the features ğ‘¥ğœ‡ and ğ‘¥ğœ are vector valued functions as usual. Representing policies as the soft-max of preferences for actions makes it pos- sible to approximate deterministic policies, which is not immediately possible when using ğœ–-greedy policies. If the optimal policy is deterministic, the pref- erences of the optimal actions become unbounded, at least if this behavior is allowed by the kind of parameterization used.
Action preferences can represent optimal stochastic policies well in the sense that the probabilities of actions may be arbitrary. This is in contrast to action- value methods and it is an important feature whenever the optimal policy is stochastic such as in rock-paper-scissors and poker.
8.3 The Policy-Gradient Theorem
Before stating the policy-gradient theorem, we define the (discounted) state distribution. In the case of an episodic environment or learning task, we consider discount rates ğ›¾ < 1. In the case of a continuing environment or learning task, it can be shown that a discount rate ğ›¾ < 1 only results in the factor 1/(1âˆ’ğ›¾) in the performance measure and hence we assume that ğ›¾ = 1 in this case without loss of generality.
Definition 8.1 (discounted state distribution). The discounted state distribu- tion
ğœ‡ğœ‹âˆ¶ ğ’® â†’ [0,1],
101
102
Chapter 8. Policy-Gradient Methods
ğœ‡ğœ‹(ğ‘ ) âˆ¶= ğ”¼all episodes[ lim ğ‘¡â†’âˆ
â„™{ğ‘†ğ‘¡ = ğ‘  âˆ£ ğ‘†0 âˆ¼ ğœ„, ğ´0,â€¦,ğ´ğ‘¡âˆ’1 âˆ¼ ğœ‹}]
is the probability of being in state ğ‘  in all episodes under a given policy ğœ‹ âˆˆ ğ’« and a given initial state distribution ğœ„ and discounted by ğ›¾ in the episodic case.
In order to simplify notation, we write ğœ‡ğœ‹ instead of ğœ‡ğœ‹,ğœ„,ğ›¾. By definition, ğœ‡ğœ‹(ğ‘ ) â‰¥ 0 for all ğ‘  âˆˆ ğ’® and âˆ‘ğ‘ âˆˆğ’® ğœ‡ğœ‹(ğ‘ ) = 1. We discount the state distribution by the discount rate ğ›¾, because this is the form that is necessary for the calculations in Theorem 8.3 below. It is also consistent with the appearance of the discount rate in the definitions of the state- and action-value functions, which are also used in the calculations in the theorem.
If the environment or learning task is continuing, the state distribution is just the stationary distribution under the policy ğœ‹. If the environment or learning task is episodic, however, the distribution of the initial distribution of the states plays a role as seen in the following lemma, which gives the state distribution in both cases.
Lemma 8.2 (discounted state distribution). If the environment is continuing, the state distribution is the stationary distribution under the policy ğœ‹.
If the environment is episodic, the discounted state distribution is given by
ğœ‡ğœ‹(ğ‘ ) =
ğœ‚(ğ‘ ) âˆ‘ğ‘ â€²âˆˆğ’® ğœ‚(ğ‘ â€²)
âˆ€ğ‘  âˆˆ ğ’®,
where the ğœ‚(ğ‘ ) are the solution of the system of equations
ğœ‚(ğ‘ ) = ğœ„(ğ‘ ) + ğ›¾ âˆ‘ ğ‘ â€²âˆˆğ’®
ğœ‚(ğ‘ â€²) âˆ‘ ğ‘âˆˆğ’œ(ğ‘ )
ğœ‹(ğ‘|ğ‘ â€²)ğ‘(ğ‘  âˆ£ ğ‘ â€²,ğ‘)
âˆ€ğ‘  âˆˆ ğ’®,
where ğœ„âˆ¶ ğ’® â†’ [0,1] is the initial distribution of the states in an episode.
To simplify notation, we write ğœ‚ instead of ğœ‚ğœ‹ or ğœ‚ğœ‹,ğœ„,ğ›¾.
Proof. We start by noting that ğœ‚(ğ‘ ) is the average number of time steps spent in state ğ‘  over all episodes discounted by ğ›¾. It consists of two terms, namely the probability ğœ„(ğ‘ ) to start in state ğ‘  and the discounted average number of times the state ğ‘  occurs coming from all other states ğ‘ â€² âˆˆ ğ’®.
This linear system of equations has a unique solution, yielding the ğœ‚(ğ‘ ). In order to find the state distribution ğœ‡ğœ‹(ğ‘ ), the ğœ‚(ğ‘ ) must be scaled by the mean length
ğ¿ âˆ¶= âˆ‘ ğ‘ âˆˆğ’®
ğœ‚(ğ‘ )
(8.1)
of all episodes.
8.3. The Policy-Gradient Theorem
103
The following theorem [48] is fundamental for policy-gradient methods. In
the continuing case, the performance measure is
ğ‘Ÿ(ğœ‹ğœƒ) âˆ¶= lim â„â†’âˆ
1 â„
â„ âˆ‘ ğ‘¡=1
ğ”¼[ğ‘…ğ‘¡ âˆ£ ğ‘†0 âˆ¼ ğœ„, ğ´0,â€¦,ğ´ğ‘¡âˆ’1 âˆ¼ ğœ‹ğœƒ],
which is assumed to exist and to be independent of the initial state distribu- tion ğœ„, i.e., the stochastic process defined by the policy ğœ‹ğœƒ and the transition probability ğ‘ is assumed to be ergodic.
Theorem 8.3 (policy-gradient theorem). Suppose that the performance measure is defined as
ğ½(ğœƒ) âˆ¶= {
ğ”¼[ğ‘£ğœ‹ğœƒ ğ‘Ÿ(ğœ‹ğœƒ),
(ğ‘†0) âˆ£ ğ‘†0 âˆ¼ ğœ„],
episodic environment, continuing environment,
where ğ‘†0 âˆ¼ ğœ„ is the initial state chosen according to the probability distribution ğœ„. Then its gradient is given by
âˆ‡ğœƒğ½(ğœƒ) = ğ¿âˆ‘ ğ‘ âˆˆğ’®
ğœ‡ğœ‹ğœƒ
(ğ‘ ) âˆ‘
ğ‘âˆˆğ’œ(ğ‘ )
ğ‘ğœ‹ğœƒ
(ğ‘ ,ğ‘)âˆ‡ğœƒğœ‹ğœƒ(ğ‘ âˆ£ ğ‘ ,ğœƒ),
where
ğ¿ âˆ¶= {
mean episode length, 1,
episodic environment, continuing environment
and ğœ‡ is the state distribution.
Proof. We prove the episodic case first and start by differentiating the state- value function ğ‘£ğœ‹ğœƒ
. By the definitions of the value functions ğ‘£ and ğ‘, we have
âˆ‡ğœƒğ‘£ğœ‹ğœƒ
(ğ‘ ) = âˆ‡ğœƒ( âˆ‘ ğ‘âˆˆğ’œ(ğ‘ )
ğœ‹(ğ‘ âˆ£ ğ‘ ,ğœƒ)ğ‘ğœ‹ğœƒ
(ğ‘ ,ğ‘))
âˆ€ğ‘  âˆˆ ğ’®.
By (2.10), we find
âˆ‡ğœƒğ‘£ğœ‹ğœƒ
(ğ‘ ) = âˆ‘
(âˆ‡ğœƒğœ‹(ğ‘ âˆ£ ğ‘ ,ğœƒ)ğ‘ğœ‹ğœƒ
(ğ‘ ,ğ‘)
ğ‘âˆˆğ’œ(ğ‘ )
+ ğœ‹(ğ‘ âˆ£ ğ‘ ,ğœƒ)âˆ‡ğœƒ âˆ‘ ğ‘ â€²âˆˆğ’®
âˆ‘ ğ‘Ÿâˆˆâ„›
ğ‘(ğ‘ â€²,ğ‘Ÿ âˆ£ ğ‘ ,ğ‘)(ğ‘Ÿ + ğ›¾ğ‘£ğœ‹ğœƒ
(ğ‘ â€²)))
âˆ€ğ‘  âˆˆ ğ’®,
which simplifies to
104
Chapter 8. Policy-Gradient Methods
âˆ‡ğœƒğ‘£ğœ‹ğœƒ
(ğ‘ ) = âˆ‘
(âˆ‡ğœƒğœ‹(ğ‘ âˆ£ ğ‘ ,ğœƒ)ğ‘ğœ‹ğœƒ
(ğ‘ ,ğ‘)
ğ‘âˆˆğ’œ(ğ‘ )
+ ğ›¾ğœ‹(ğ‘ âˆ£ ğ‘ ,ğœƒ)âˆ‘ ğ‘ â€²âˆˆğ’®
ğ‘(ğ‘ â€²,ğ‘Ÿ âˆ£ ğ‘ ,ğ‘)âˆ‡ğœƒğ‘£ğœ‹ğœƒ
(ğ‘ â€²))
âˆ€ğ‘  âˆˆ ğ’®.
Performing a second time step by applying the recursive formula we just found to âˆ‡ğœƒğ‘£ğœ‹ğœƒ
(ğ‘ â€²), we find
âˆ‡ğœƒğ‘£ğœ‹ğœƒ
(ğ‘ ) = âˆ‘
(âˆ‡ğœƒğœ‹(ğ‘ âˆ£ ğ‘ ,ğœƒ)ğ‘ğœ‹ğœƒ
(ğ‘ ,ğ‘)
ğ‘âˆˆğ’œ(ğ‘ )
+ ğ›¾ğœ‹(ğ‘ âˆ£ ğ‘ ,ğœƒ)âˆ‘ ğ‘ â€²âˆˆğ’®
ğ‘(ğ‘ â€²,ğ‘Ÿ âˆ£ ğ‘ ,ğ‘) âˆ‘
ğ‘â€²âˆˆğ’œ(ğ‘ )
(âˆ‡ğœƒğœ‹(ğ‘â€² âˆ£ ğ‘ â€²,ğœƒ)ğ‘ğœ‹ğœƒ
(ğ‘ â€²,ğ‘â€²)
+ ğ›¾ğœ‹(ğ‘â€² âˆ£ ğ‘ â€²,ğœƒ) âˆ‘ ğ‘ â€³âˆˆğ’®
ğ‘(ğ‘ â€³,ğ‘Ÿ âˆ£ ğ‘ â€²,ğ‘â€²)âˆ‡ğœƒğ‘£ğœ‹ğœƒ
(ğ‘ â€³)))
âˆ€ğ‘  âˆˆ ğ’®.
Hence the recursive expansion of this formula can be written as
âˆ‡ğœƒğ‘£ğœ‹ğœƒ
(ğ‘ ) = âˆ‘ ğ‘ â€²âˆˆğ’®
âˆ âˆ‘ ğ‘˜=0
ğ›¾ğ‘˜â„™{ğ‘ â€² âˆ£ ğ‘ ,ğ‘˜,ğœ‹} âˆ‘
ğ‘âˆˆğ’œ(ğ‘ â€²)
âˆ‡ğœƒğœ‹(ğ‘ âˆ£ ğ‘ â€²,ğœƒ)ğ‘ğœ‹ğœƒ
(ğ‘ â€²,ğ‘),
where â„™{ğ‘ â€² âˆ£ ğ‘ ,ğ‘˜,ğœ‹} is the probability of transitioning to state ğ‘ â€² after following policy ğœ‹ for ğ‘˜ steps after starting from state ğ‘ .
The gradient of the performance measure ğ½ thus becomes
âˆ‡ğœƒğ½(ğœƒ) âˆ¶= âˆ‡ğœƒğ”¼[ğ‘£ğœ‹ğœƒ âˆ âˆ‘ ğ‘˜=0
= ğ”¼[âˆ‘ ğ‘ âˆˆğ’®
(ğ‘†0) âˆ£ ğ‘†0 âˆ¼ ğœ„]
ğ›¾ğ‘˜â„™{ğ‘  âˆ£ ğ‘†0,ğ‘˜,ğœ‹} âˆ‘ ğ‘âˆˆğ’œ(ğ‘ )
âˆ‡ğœƒğœ‹(ğ‘ âˆ£ ğ‘ ,ğœƒ)ğ‘ğœ‹ğœƒ
(ğ‘ ,ğ‘) âˆ£ ğ‘†0 âˆ¼ ğœ„]
= âˆ‘ ğ‘ âˆˆğ’®
ğœ‚(ğ‘ ) âˆ‘
ğ‘âˆˆğ’œ(ğ‘ )
âˆ‡ğœƒğœ‹(ğ‘ âˆ£ ğ‘ ,ğœƒ)ğ‘ğœ‹ğœƒ
(ğ‘ ,ğ‘)
= ğ¿âˆ‘ ğ‘ âˆˆğ’®
ğœ‚(ğ‘ ) ğ¿
âˆ‘ ğ‘âˆˆğ’œ(ğ‘ )
âˆ‡ğœƒğœ‹(ğ‘ âˆ£ ğ‘ ,ğœƒ)ğ‘ğœ‹ğœƒ
(ğ‘ ,ğ‘)
= ğ¿âˆ‘ ğ‘ âˆˆğ’®
ğœ‡ğœ‹ğœƒ
(ğ‘ ) âˆ‘
ğ‘âˆˆğ’œ(ğ‘ )
âˆ‡ğœƒğœ‹(ğ‘ âˆ£ ğ‘ ,ğœƒ)ğ‘ğœ‹ğœƒ
(ğ‘ ,ğ‘),
where we have used the definition of ğœ‚(ğ‘ ) in Lemma 8.2 and (8.1).
In the continuing case, similar calculations for the differential return
ğºğ‘¡ âˆ¶=
âˆ âˆ‘ ğ‘˜=1
(ğ‘…ğ‘¡+ğ‘˜ âˆ’ ğ‘Ÿ(ğœ‹ğœƒ))
8.4. Monte-Carlo Policy-Gradient Method: REINFORCE
can be done.
Interestingly enough, although the performance measure depends on the state distribution which depends on the policy parameter ğœƒ, the derivative of the state distribution does not appear in the expression found in the policy- gradient theorem. This is the usefulness of the theorem.
Remark 8.4 (experience replay). The expression for the gradient of the per- formance measure found in the theorem includes the state distribution ğœ‡ and hence motivates experience replay. Experience replay is a method which keeps a cache of states and actions that have been visited and which are replayed during learning in order to ensure that the whole space is sampled in an equidistributed manner. Cf. Remark 6.8.
8.4 Monte-Carlo Policy-Gradient Method: REIN-
FORCE
Having the gradient of the performance measure available from Theorem 8.3, we can use gradient based stochastic optimization. The most straightforward way is the iteration
ğœƒğ‘¡+1 âˆ¶= ğœƒğ‘¡ + ğ›¼ âˆ‘
Ì‚ğ‘ğ‘¤(ğ‘†ğ‘¡,ğ‘)âˆ‡ğœƒğœ‹ğœƒ(ğ‘ âˆ£ ğ‘†ğ‘¡,ğœƒ),
ğ‘âˆˆğ’œ(ğ‘†ğ‘¡)
and parameterized by a vector ğ‘¤ âˆˆ â„ğ‘‘.
where Ì‚ğ‘ğ‘¤ is an approximation of ğ‘ğœ‹ğœƒ This iteration is called an all-actions method.
The more classical REINFORCE algorithm is derived as follows. We start
from state ğ‘†ğ‘¡ in time step ğ‘¡ and use Theorem 8.3 to write
âˆ‡ğœƒğ½(ğœƒ) = ğ¿ğ”¼ğœ‹ğœƒ
[ğ›¾ğ‘¡ âˆ‘
ğ‘ğœ‹ğœƒ
(ğ‘ ,ğ‘)âˆ‡ğœƒğœ‹ğœƒ(ğ‘ âˆ£ ğ‘ ,ğœƒ)]
ğ‘âˆˆğ’œ(ğ‘†ğ‘¡)
= ğ¿ğ”¼ğœ‹ğœƒ
[ğ›¾ğ‘¡ âˆ‘
ğ‘âˆˆğ’œ(ğ‘†ğ‘¡)
ğœ‹ğœƒ(ğ‘ âˆ£ ğ‘†ğ‘¡,ğœƒ)ğ‘ğœ‹ğœƒ
(ğ‘†ğ‘¡,ğ‘)
âˆ‡ğœƒğœ‹ğœƒ(ğ‘ âˆ£ ğ‘†ğ‘¡,ğœƒ) ğœ‹ğœƒ(ğ‘ âˆ£ ğ‘†ğ‘¡,ğœƒ)
],
since the discounted state distribution ğœ‡ğœ‹ğœƒ includes a factor of ğ›¾ for each time step. Next, we replace the sum over all actions by the sample ğ´ğ‘¡ âˆ¼ ğœ‹ğœƒ. Then the gradient of the performance measure is approximately proportional to
âˆ‡ğœƒğ½(ğœƒ) â‰ˆ ğ”¼ğœ‹ğœƒ
[ğ›¾ğ‘¡ğ‘ğœ‹ğœƒ
(ğ‘†ğ‘¡,ğ´ğ‘¡)
âˆ‡ğœƒğœ‹ğœƒ(ğ´ğ‘¡ âˆ£ ğ‘†ğ‘¡,ğœƒ) ğœ‹ğœƒ(ğ´ğ‘¡ âˆ£ ğ‘†ğ‘¡,ğœƒ)
].
105
106
Chapter 8. Policy-Gradient Methods
Having selected the action ğ´ğ‘¡, we use Definition 2.8 to find
âˆ‡ğœƒğ½(ğœƒ) â‰ˆ ğ”¼ğœ‹ğœƒ
[ğ›¾ğ‘¡ğºğ‘¡
âˆ‡ğœƒğœ‹ğœƒ(ğ´ğ‘¡ âˆ£ ğ‘†ğ‘¡,ğœƒ) ğœ‹ğœƒ(ğ´ğ‘¡ âˆ£ ğ‘†ğ‘¡,ğœƒ)
].
This yields the gradient used in the REINFORCE update
ğœƒğ‘¡+1 âˆ¶= ğœƒğ‘¡ + ğ›¼ğ›¾ğ‘¡ğºğ‘¡
âˆ‡ğœƒğœ‹ğœƒ(ğ´ğ‘¡ âˆ£ ğ‘†ğ‘¡,ğœƒğ‘¡) ğœ‹ğœƒ(ğ´ğ‘¡ âˆ£ ğ‘†ğ‘¡,ğœƒğ‘¡)
= ğœƒğ‘¡ + ğ›¼ğ›¾ğ‘¡ğºğ‘¡âˆ‡ğœƒ lnğœ‹ğœƒ(ğ´ğ‘¡ âˆ£ ğ‘†ğ‘¡,ğœƒğ‘¡).
Since the return ğºğ‘¡ until the end of an episode is used as the target, this is an MC method.
The algorithm for this update is shown in Algorithm 16.
Algorithm 16 REINFORCE for calculating ğœ‹ğœƒ â‰ˆ ğœ‹âˆ—.
initialization: choose a representation the policy ğœ‹ğœƒ choose learning rate ğ›¼ âˆˆ â„+ initialize policy parameter ğœƒ âˆˆ Î˜ âŠ‚ â„ğ‘‘â€²
loop
â–· for all episodes
generate an episode (ğ‘†0,ğ´0,ğ‘…1,â€¦,ğ‘†ğ‘‡âˆ’1,ğ´ğ‘‡âˆ’1,ğ‘…ğ‘‡) following ğœ‹ğœƒ for ğ‘¡ âˆˆ (0,1,â€¦,ğ‘‡ âˆ’ 1) do ğº âˆ¶= âˆ‘ğ‘‡ ğ‘˜=ğ‘¡+1 ğ›¾ğ‘˜âˆ’ğ‘¡âˆ’1ğ‘…ğ‘˜ ğœƒ âˆ¶= ğœƒ + ğ›¼ğ›¾ğ‘¡ğºâˆ‡ğœƒ lnğœ‹ğœƒ(ğ´ğ‘¡ âˆ£ ğ‘†ğ‘¡,ğœƒ)
â–· for all time steps
end for
end loop
return ğœƒ
REINFORCE is a stochastic gradient ascent method. By the construction based on Theorem 8.3, the expected update over time is in the same direction as the performance measure ğ½. Under the standard stochastic approximation conditions (2.2), the algorithm converges to a local optimum. On the other hand, REINFORCE is a MC algorithm and thus may be of high variance.
8.5. Monte-Carlo Policy-Gradient Method: REINFORCE with Baseline
8.5 Monte-Carlo Policy-Gradient Method: REIN-
FORCE with Baseline
The right side in Theorem 8.3 can be changed by subtracting an arbitrary so- called baseline ğ‘, a function or a random variable of the state, from the action- value function, i.e.,
âˆ‡ğœƒğ½(ğœƒ) = ğ¿âˆ‘ ğ‘ âˆˆğ’®
ğœ‡ğœ‹ğœƒ
(ğ‘ ) âˆ‘
ğ‘âˆˆğ’œ(ğ‘ )
ğ‘ğœ‹ğœƒ
(ğ‘ ,ğ‘)âˆ‡ğœƒğœ‹ğœƒ(ğ‘ âˆ£ ğ‘ ,ğœƒ)
= ğ¿âˆ‘ ğ‘ âˆˆğ’®
ğœ‡ğœ‹ğœƒ
(ğ‘ ) âˆ‘
ğ‘âˆˆğ’œ(ğ‘ )
(ğ‘ğœ‹ğœƒ
(ğ‘ ,ğ‘) âˆ’ ğ‘(ğ‘ ))âˆ‡ğœƒğœ‹ğœƒ(ğ‘ âˆ£ ğ‘ ,ğœƒ).
The last equation holds true because
âˆ‘ ğ‘âˆˆğ’œ(ğ‘ )
ğ‘(ğ‘ )âˆ‡ğœƒğœ‹ğœƒ(ğ‘ âˆ£ ğ‘ ,ğœƒ) = ğ‘(ğ‘ )âˆ‡ğœƒ âˆ‘ ğœ‹ğœƒ(ğ‘ âˆ£ ğ‘ ,ğœƒ) ğ‘âˆˆğ’œ(ğ‘ ) âŸâŸâŸâŸâŸâŸâŸ =1
= 0.
With this change, the update becomes
ğœƒğ‘¡+1 âˆ¶= ğœƒğ‘¡ + ğ›¼ğ›¾ğ‘¡(ğºğ‘¡ âˆ’ ğ‘(ğ‘†ğ‘¡))âˆ‡ğœƒ lnğœ‹ğœƒ(ğ´ğ‘¡ âˆ£ ğ‘†ğ‘¡,ğœƒğ‘¡).
What is the purpose of adding a baseline? It leaves the expected value of the updates unchanged, but it is a method to reduce their variance. The natural choice is an approximation of the expected value of ğºğ‘¡, i.e., the state-value function. This approximation
Ì‚ğ‘£ğ‘¤(ğ‘ ) â‰ˆ ğ‘£ğœ‹ğœƒ
(ğ‘ )
(ğ‘ ), where ğ‘¤ âˆˆ ğ‘Š âŠ‚ â„ğ‘‘ is a parameter vector, can of the state-value function ğ‘£ğœ‹ğœƒ be calculated by any suitable method, but since REINFORCE is an MC method, an MC method is used for calculating the approximation in Algorithm 17 as well.
The general rule of thumb for choosing the learning rate ğ›¼ğ‘¤ is
ğ›¼ğ‘¤ âˆ¶=
0.1 ğ”¼[â€–âˆ‡ğ‘¤Ì‚ğ‘£ğ‘¤(ğ‘†ğ‘¡)â€–2 ğœ‡]
,
which is updated while the algorithm runs. Unfortunately, no such general rule is available for the learning rate ğ›¼ğœƒ, since the learning rate depends on the range of the rewards and on the parameterization of the policy.
REINFORCE with baselines is unbiased and its approximation of an optimal policy converges to a local minimum. As an MC method, it converges slowly with high variance and inconvenient to implement for continuing environments or learning tasks.
107
108
Chapter 8. Policy-Gradient Methods
Algorithm 17 REINFORCE with baseline for calculating ğœ‹ğœƒ â‰ˆ ğœ‹âˆ—.
initialization: choose a representation for the policy ğœ‹ğœƒ choose a representation for the state-value function Ì‚ğ‘£ğ‘¤ choose learning rate ğ›¼ğœƒ âˆˆ â„+ choose learning rate ğ›¼ğ‘¤ âˆˆ â„+ initialize policy parameter ğœƒ âˆˆ Î˜ âŠ‚ â„ğ‘‘â€² initialize state-value parameter ğ‘¤ âˆˆ ğ‘Š âŠ‚ â„ğ‘‘
loop
â–· for all episodes
generate an episode (ğ‘†0,ğ´0,ğ‘…1,â€¦,ğ‘†ğ‘‡âˆ’1,ğ´ğ‘‡âˆ’1,ğ‘…ğ‘‡) following ğœ‹ğœƒ for ğ‘¡ âˆˆ (0,1,â€¦,ğ‘‡ âˆ’ 1) do ğ‘˜=ğ‘¡+1 ğ›¾ğ‘˜âˆ’ğ‘¡âˆ’1ğ‘…ğ‘˜
â–· for all time steps
ğº âˆ¶= âˆ‘ğ‘‡ ğ›¿ âˆ¶= ğº âˆ’ Ì‚ğ‘£ğ‘¤(ğ‘†ğ‘¡) ğ‘¤ âˆ¶= ğ‘¤ + ğ›¼ğ‘¤ğ›¿âˆ‡ğ‘¤Ì‚ğ‘£ğ‘¤(ğ‘†ğ‘¡) ğœƒ âˆ¶= ğœƒ + ğ›¼ğœƒğ›¾ğ‘¡ğ›¿âˆ‡ğœƒ lnğœ‹ğœƒ(ğ´ğ‘¡ âˆ£ ğ‘†ğ‘¡,ğœƒ)
end for
end loop
return ğœƒ and ğ‘¤
8.6. Temporal-Difference Policy-Gradient Methods: Actor-Critic Methods 109
8.6 Temporal-Difference Policy-Gradient Methods:
Actor-Critic Methods
REINFORCE with baseline is an MC method, since the target value in the update is the return till the end of the episode. In other words, no bootstrapping is performed, i.e., no previous approximation of a value function is used to update the policy. Although the state-value function is used in the iteration, it is used to only for the state that is currently being updated; it serves for variance reduction, not for bootstrapping.
Bootstrapping (e.g., by going from MC to TD methods) introduces a bias. Still, this is often useful as it reduces the variance in the value function or policy and hence accelerates learning. Going from MC methods to TD methods is analogous to going from REINFORCE with baseline to actor-critic methods. Just as TD methods, actor-critic methods use the return calculated over a certain number of time steps; the simplest case being to use the return ğºğ‘¡âˆ¶ğ‘¡+1 using only one time step.
Here we introduce the one-step actor-critic method that uses the one-step return ğºğ‘¡âˆ¶ğ‘¡+1 as the target in the update and that still uses the learned state- value function Ì‚ğ‘£ğ‘¤ as the baseline ğ‘ âˆ¶= Ì‚ğ‘£ğ‘¤. This yields the iteration
ğœƒğ‘¡+1 âˆ¶= ğœƒğ‘¡ + ğ›¼ğ›¾ğ‘¡(ğºğ‘¡ âˆ’ ğ‘(ğ‘†ğ‘¡))âˆ‡ğœƒ lnğœ‹ğœƒ(ğ´ğ‘¡ âˆ£ ğ‘†ğ‘¡,ğœƒğ‘¡)
= ğœƒğ‘¡ + ğ›¼ğ›¾ğ‘¡(ğ‘…ğ‘¡ + ğ›¾Ì‚ğ‘£ğ‘¤(ğ‘†ğ‘¡+1) âˆ’ Ì‚ğ‘£ğ‘¤(ğ‘†ğ‘¡))âˆ‡ğœƒ lnğœ‹ğœƒ(ğ´ğ‘¡ âˆ£ ğ‘†ğ‘¡,ğœƒğ‘¡)
The update of the baseline is now performed by TD(0) in order to be consistent in the methods that are used to learn the baseline and the policy.
The name comes from the intuition that the policy is the actor (answering the question what to do) and the baseline, i.e., the state-value function is the critic (answering the question how well it is done).
This one-step actor-critic method is shown in Algorithm 18. Of course, instead of the one-step return, the ğ‘›-step return ğºğ‘¡âˆ¶ğ‘¡+ğ‘› or the
ğœ†-return ğºğœ†
ğ‘¡ can be used.
8.7 Bibliographical and Historical Remarks
Problems
110
Chapter 8. Policy-Gradient Methods
Algorithm 18 one-step actor critic for calculating ğœ‹ğœƒ â‰ˆ ğœ‹âˆ—.
initialization: choose a representation for the policy ğœ‹ğœƒ choose a representation for the state-value function Ì‚ğ‘£ğ‘¤ choose learning rate ğ›¼ğœƒ âˆˆ â„+ choose learning rate ğ›¼ğ‘¤ âˆˆ â„+ initialize policy parameter ğœƒ âˆˆ Î˜ âŠ‚ â„ğ‘‘â€² initialize state-value parameter ğ‘¤ âˆˆ ğ‘Š âŠ‚ â„ğ‘‘
loop
â–· for all episodes
initialize ğ‘  âˆ¼ ğœ„ ğº âˆ¶= 1 while ğ‘  is not terminal do
â–· for all time steps
choose action ğ‘ according to ğœ‹ğœƒ(â‹…|ğ‘ ) take action ğ‘ and receive the new state ğ‘ â€² and the reward ğ‘Ÿ ğ›¿ âˆ¶= ğ‘… + ğ›¾Ì‚ğ‘£ğ‘¤(ğ‘ â€²) âˆ’ Ì‚ğ‘£ğ‘¤(ğ‘ ) ğ‘¤ âˆ¶= ğ‘¤ + ğ›¼ğ‘¤ğ›¿âˆ‡ğ‘¤Ì‚ğ‘£ğ‘¤(ğ‘ ) ğœƒ âˆ¶= ğœƒ + ğ›¼ğœƒğºğ›¿âˆ‡ğœƒ lnğœ‹ğœƒ(ğ‘ âˆ£ ğ‘ ,ğœƒ) ğº âˆ¶= ğ›¾ğº ğ‘  âˆ¶= ğ‘ â€² end while
end loop
return ğœƒ and ğ‘¤
Chapter 9
Hamilton-Jacobi-Bellman Equations
9.1 Introduction
In certain cases, it is possible to model the environment by difference or dif- ferential equations. This may be the case when the environment depends on â€“ for example â€“ physical, chemical, or biological processes that can be described by such equations. The ability to describe the environment in such a manner usually has the advantage that the number of episodes available for learning is unlimited, since episodes can be rolled out by solving the equations. This is in contrast to problems in data science, where the available data may be limited. The purpose of this chapter is to take advantage of the knowledge about the environment encoded in the equations.
In this chapter, we study the case when the environment can be described by deterministic or stochastic ordinary differential equations, leading to problems in deterministic and stochastic optimal control. Here we follow the notation for RL problems used in the rest of the book.
We write the state equation in the form of the initial-value problem
Ì‡ğ‘ (ğ‘¡) = ğ‘“(ğ‘ (ğ‘¡),ğœ‹(ğ‘¡)) ğ‘ (0) = ğ‘ 0 âˆˆ ğ’®,
âˆ€ğ‘¡ âˆˆ â„+ 0 ,
where the function ğ‘ âˆ¶ â„+ 0 â†’ ğ’®, whose image is the set ğ’® âŠ‚ â„ğ‘›ğ‘  of all states, gives the state of the system at time ğ‘¡ âˆˆ â„+ 0 , the function ğ‘¢âˆ¶ â„+ 0 â†’ ğ’œ(ğ‘¡), whose image is the set ğ’œ(ğ‘¡) âŠ‚ â„ğ‘›ğ‘ of all actions available at time ğ‘¡, is the control applied at time ğ‘¡ âˆˆ â„+ 0 , and the vector valued function ğ‘“ âˆ¶ â„ğ‘›ğ‘  Ã— â„ğ‘›ğ‘ â†’ â„ğ‘›ğ‘ 
(9.1a)
(9.1b)
111
112
Chapter 9. Hamilton-Jacobi-Bellman Equations
describes the dynamics of the system as a deterministic or stochastic ordinary differential equation. The control ğ‘¢âˆ¶ â„+
0 â†’ ğ’œ and the corresponding policy ğœ‹âˆ¶ ğ’® â†’ ğ’œ are related
simply by
ğ‘¢(ğ‘¡) = ğœ‹(ğ‘ (ğ‘¡)).
We assume that the state set or state space ğ’® is an open, bounded set with a sufficiently smooth boundary. We also assume that the function ğ‘¢ is bounded and Lebesgue measurable and that its image is a compact set in ğ’œ(ğ‘¡). Further- more, we assume that the dynamics ğ‘“ of the system are Lipschitz continuous with respect to its first argument ğ‘ (ğ‘¡).
If the control function ğ‘¢ is given, then the initial-value problem (9.1) has a unique solution as known from the standard theory of ordinary differential equations. Unfortunately, the solution may exit the state space ğ’® at a certain time
ğœ âˆ¶= {
ğ‘ (ğ‘¡) âˆˆ ğ’® âˆ€ğ‘¡ âˆˆ â„+ 0 ,
âˆ, inf{ğ‘¡ âˆˆ â„+
0 âˆ£ ğ‘ (ğ‘¡) âˆ‰ ğ’®}, otherwise,
the so-called exit time.
Next, we define the (deterministic) return as the functional
ğºâˆ¶ ğ’® Ã— (â„+
0 â†’ ğ’œ(ğ‘¡)) â†’ â„,
ğº(ğ‘ 0,ğ‘¢) âˆ¶= âˆ« 0
ğœ
eâˆ’ğ›¾ğ‘¡ğ‘Ÿ(ğ‘ (ğ‘¡),ğ‘¢(ğ‘¡))dğ‘¡ + eâˆ’ğ›¾ğœğ‘…(ğ‘ (ğœ))
on the state space ğ’® and the set of all actions. The function ğ‘Ÿâˆ¶ ğ’®Ã—ğ’œ(ğ‘¡) â†’ â„ is called the current reward, and the function ğ‘…âˆ¶ ğœ•ğ’® â†’ â„ is called the boundary reward. The discount rate ğ›¾ âˆˆ â„+ is constant. The return is the continuously discounted return over all times [0,ğœ] the trajectory {ğ‘¡ âˆˆ [0,ğœ] âˆ£ ğ‘ (ğ‘¡)} remains within the set ğ’® of admissible states.
The optimal-control problem consists in finding an initial state ğ‘ 0 âˆˆ ğ’® and
an optimal control ğ‘¢âˆ— that maximizes the return ğº.
9.2 The Hamilton-Jacobi-Bellman Equation
Similar to the discrete case, we can find an equation that is satisfied by optimal controls; this is the purpose of this section. We start by defining the optimal value function.
Definition 9.1 (optimal value function). The optimal value function is defined as
ğ‘£âˆ—âˆ¶ ğ’® â†’ â„,
ğ‘£âˆ—(ğ‘ 0) âˆ¶= sup ğ‘¢âˆˆğ’«
ğº(ğ‘ 0,ğ‘¢),
9.2. The Hamilton-Jacobi-Bellman Equation
and gives the maximal value of the return ğº for the initial state ğ‘ 0 âˆˆ ğ’® over all controls ğ‘¢ âˆˆ ğ’«. The supremum is taken over the set ğ’« of all bounded, Lebesgue measurable functions ğ‘¢âˆ¶ â„+
0 â†’ ğ’œ(ğ‘¡).
The following lemma states that the optimal value function ğ‘£âˆ— can be split into a sum for the time interval [0,Î”ğ‘¡) and one for the rest [Î”ğ‘¡,âˆ) of the time [49, Lemma I.7.1]. This is analogous to the Bellman optimality equation (2.7).
Lemma 9.2 (dynamic-programming principle). The equation
ğ‘£âˆ—(ğ‘ 0) = sup ğ‘¢âˆˆğ’«
(âˆ« 0
min(Î”ğ‘¡,ğœ)
eâˆ’ğ›¾ğ‘¡ğ‘Ÿ(ğ‘ (ğ‘¡),ğ‘¢(ğ‘¡))dğ‘¡ + eâˆ’ğ›¾ğœğ‘…(ğ‘ (ğœ))[ğœ < Î”ğ‘¡]
+ eâˆ’ğ›¾Î”ğ‘¡ğ‘£âˆ—(ğ‘ (Î”ğ‘¡))[ğœ â‰¥ Î”ğ‘¡])
âˆ€ğ‘ 0 âˆˆ ğ’® âˆ€Î”ğ‘¡ âˆˆ â„+ 0
holds. Here the Iverson notation means that [statement] = 1 if the statement holds true and [statement] = 0 otherwise.
In the following, we approximate the right side in Lemma 9.2 for small Î”ğ‘¡.
The first term can be approximated as
âˆ« 0
min(Î”ğ‘¡,ğœ)
eâˆ’ğ›¾ğ‘¡ğ‘Ÿ(ğ‘ (ğ‘¡),ğ‘¢(ğ‘¡))dğ‘¡ = Î”ğ‘¡ğ‘Ÿ(ğ‘ 0,ğ‘¢(0)) + ğ‘œ(Î”ğ‘¡).
The second term tends to zero as Î”ğ‘¡ â†’ 0. The third term, the optimal value function becomes
ğ‘£âˆ—(ğ‘ (Î”ğ‘¡)) = ğ‘£âˆ—(ğ‘ 0) + Î”ğ‘¡âˆ‡ğ‘£âˆ—(ğ‘ 0) â‹…
Ì‡ğ‘ (0) + ğ‘œ(Î”ğ‘¡)
= ğ‘£âˆ—(ğ‘ 0) + Î”ğ‘¡âˆ‡ğ‘£âˆ—(ğ‘ 0) â‹… ğ‘“(ğ‘ 0,ğ‘¢(0)) + ğ‘œ(Î”ğ‘¡)
using Taylor expansion and the state equation (9.1). Next, we divide the equa- tion by Î”ğ‘¡ to find 1 âˆ’ eâˆ’ğ›¾Î”ğ‘¡ Î”ğ‘¡
ğ‘œ(Î”ğ‘¡) Î”ğ‘¡
(ğ‘Ÿ(ğ‘ 0,ğ‘¢(0)) + eâˆ’ğ›¾Î”ğ‘¡âˆ‡ğ‘£âˆ—(ğ‘ 0) â‹… ğ‘“(ğ‘ 0,ğ‘¢(0)) +
ğ‘£âˆ—(ğ‘ 0) = sup ğ‘¢âˆˆğ’«
for all ğ‘ 0 âˆˆ ğ’® and for sufficiently small Î”ğ‘¡. Finally, we obtain the Hamilton- Jacobi-Bellman (HJB) equation by letting Î”ğ‘¡ tend to zero.
Theorem 9.3 (Hamilton-Jacobi-Bellman equation). If the optimal value func- tion ğ‘£âˆ— is in ğ¶1(ğ’®), then it satisfies the Hamilton-Jacobi-Bellman equation
ğ›¾ğ‘£âˆ—(ğ‘ 0) = sup ğ‘âˆˆğ’œ(0)
(ğ‘Ÿ(ğ‘ 0,ğ‘) + âˆ‡ğ‘£âˆ—(ğ‘ 0) â‹… ğ‘“(ğ‘ 0,ğ‘))
âˆ€ğ‘ 0 âˆˆ ğ’®
with the boundary condition
ğ‘£âˆ—(ğ‘ ) â‰¥ ğ‘…(ğ‘ )
âˆ€ğ‘  âˆˆ ğœ•ğ’®.
113
)
(9.2)
(9.3)
114
Chapter 9. Hamilton-Jacobi-Bellman Equations
Note that here the supremum is taken over all actions ğ‘ âˆˆ ğ’œ(0) available at
time zero.
The inequality in the boundary conditions holds because there may be points ğ‘  âˆˆ
ğœ•ğ’® for which a control exists such that ğº(ğ‘ ,ğ‘¢(ğ‘¡)) > ğ‘…(ğ‘ ), implying the strict inequality. It is also possible that the trajectory immediately exits the state space after starting on the boundary ğœ•ğ’®. If this is the optimal control, then the equality in the boundary condition holds.
The HJB equation in Theorem 9.3 is a necessary condition for its solution being the optimal value function. The following theorem states that it is also a sufficient condition [49, Theorem I.7.1].
Theorem 9.4 (sufficient condition). Suppose that ğ‘¤ âˆˆ ğ¶1(ğ’®) satisfies (9.2) and (9.3). If ğœ = âˆ, suppose that ğ‘¤ also satisfies the equation limğ‘¡â†’âˆ eâˆ’ğ›¾ğ‘¡ğ‘¤(ğ‘ (ğ‘¡)) = 0. Then the inequality ğ‘¤(ğ‘ ) â‰¥ ğ‘£âˆ—(ğ‘ ) holds for all ğ‘  âˆˆ ğ’®.
Furthermore, suppose that there exists a control ğ‘¢âˆ— such that
ğ‘¢âˆ—(ğ‘¡) âˆˆ argmax ğ‘âˆˆğ’œ(ğ‘¡)
{ğ‘Ÿ(ğ‘ âˆ—(ğ‘¡),ğ‘) + âˆ‡ğ‘¤(ğ‘ âˆ—(ğ‘¡)) â‹… ğ‘“(ğ‘ âˆ—(ğ‘¡),ğ‘)}
for almost all ğ‘¡ âˆˆ [0,ğœâˆ—) and that ğ‘¤(ğ‘ âˆ—(ğœâˆ—)) = ğ‘…(ğ‘ âˆ—(ğœâˆ—)) if ğœâˆ— < âˆ, where ğ‘ âˆ— is the solution of the state equation (9.1) for ğ‘¢ = ğ‘¢âˆ— and ğœâˆ— is the corresponding exit time. Then ğ‘¢âˆ— is optimal for the initial state ğ‘  and the equation
ğ‘¤(ğ‘ ) = ğ‘£âˆ—(ğ‘ )
âˆ€ğ‘  âˆˆ ğ’®
holds.
Proof. Since ğ‘¤ âˆˆ ğ¶1(ğ’®), we can start from the equality
eâˆ’ğ›¾ğ‘¡ğ‘¤(ğ‘ (ğ‘¡)) = ğ‘¤(ğ‘ ) + âˆ« 0
= ğ‘¤(ğ‘ ) + âˆ« 0
ğ‘¡
ğ‘¡
d
dğ‘¡â€²(eâˆ’ğ›¾ğ‘¡â€² eâˆ’ğ›¾ğ‘¡â€²
ğ‘¤(ğ‘ (ğ‘¡â€²)))dğ‘¡â€²
(âˆ’ğ›¾ğ‘¤(ğ‘ (ğ‘¡â€²)) + Ì‡ğ‘ (ğ‘¡â€²) â‹… âˆ‡ğ‘¤(ğ‘ (ğ‘¡â€²)))dğ‘¡â€².
Using the state equation (9.1), we find
eâˆ’ğ›¾ğ‘¡ğ‘¤(ğ‘ (ğ‘¡)) = ğ‘¤(ğ‘ ) + âˆ« 0
ğ‘¡
eâˆ’ğ›¾ğ‘¡â€²
(âˆ’ğ›¾ğ‘¤(ğ‘ (ğ‘¡â€²)) + ğ‘“(ğ‘ (ğ‘¡â€²),ğ‘¢(ğ‘ )) â‹… âˆ‡ğ‘¤(ğ‘ (ğ‘¡â€²)))dğ‘¡â€²,
and using (9.2), we find
eâˆ’ğ›¾ğ‘¡ğ‘¤(ğ‘ (ğ‘¡)) â‰¤ ğ‘¤(ğ‘ ) âˆ’ âˆ« 0
ğ‘¡
eâˆ’ğ›¾ğ‘¡â€²
ğ‘Ÿ(ğ‘ (ğ‘¡â€²),ğ‘¢(ğ‘¡â€²))dğ‘¡â€²
âˆ€ğ‘¢ âˆˆ ğ’« âˆ€ğ‘¡ âˆˆ [0,ğœ).
(9.4)
9.3. An Example of Optimal Control
Letting ğ‘¡ tend to the exit time ğœ yields
ğ‘¤(ğ‘ ) â‰¥ âˆ« 0
ğœ
eâˆ’ğ›¾ğ‘¡â€²
ğ‘Ÿ(ğ‘ (ğ‘¡â€²),ğ‘¢(ğ‘¡â€²))dğ‘¡â€² + lim ğ‘¡â†’ğœ
eâˆ’ğ›¾ğ‘¡ğ‘¤(ğ‘ (ğ‘¡)).
In the case ğœ < âˆ, the inequality limğ‘¡â†’ğœ eâˆ’ğ›¾ğ‘¡ğ‘¤(ğ‘ (ğ‘¡)) â‰¥ eâˆ’ğ›¾ğœğ‘…(ğ‘ (ğœ)) holds for the last term. In the case ğœ = âˆ, the limit is zero by assumption. In both cases, we thus have the inequality
ğ‘¤(ğ‘ ) â‰¥ ğº(ğ‘ ,ğ‘¢)
âˆ€ğ‘  âˆˆ ğ’®
for all controls ğ‘¢ âˆˆ ğ’«, which implies
ğ‘¤(ğ‘ ) â‰¥ ğ‘£âˆ—(ğ‘ )
âˆ€ğ‘  âˆˆ ğ’®.
This concludes the proof of the first assertion.
The first assumption of the second assertion of the theorem means that the action ğ‘¢âˆ—(ğ‘¡) is always maximal. Together with the second assumption, the same calculations as above can be performed for ğ‘¢âˆ— instead of ğ‘¢, but with equalities everywhere. Thus we have ğ‘¤(ğ‘ ) = ğº(ğ‘ ,ğ‘¢âˆ—) for all ğ‘  âˆˆ ğ’® and hence the equality
ğ‘¤(ğ‘ ) = ğ‘£âˆ—(ğ‘ )
âˆ€ğ‘  âˆˆ ğ’®
holds, which concludes the proof.
Knowing the dynamics ğ‘“ of the system, we solve (9.2) in Theorem 9.3 for the optimal value function ğ‘£âˆ—. Knowing ğ‘£âˆ—, we can then use (9.4) in Theorem 9.4 to find an optimal control. However, so far we have used controls as functions of time and not policies as functions of state. Both a control ğ‘¢âˆ¶ â„+ 0 â†’ ğ’œ and a policy ğœ‹âˆ¶ ğ’® â†’ ğ’œ are related simply by
ğ‘¢(ğ‘¡) = ğœ‹(ğ‘ (ğ‘¡)).
Therefore, we use (9.4) to find an optimal policy by choosing
ğœ‹âˆ—(ğ‘ ) âˆˆ argmax ğ‘âˆˆğ’œ(ğ‘¡)
{ğ‘Ÿ(ğ‘ ,ğ‘) + âˆ‡ğ‘£âˆ—(ğ‘ ) â‹… ğ‘“(ğ‘ ,ğ‘)}.
9.3 An Example of Optimal Control
The following, simple example shows that the optimal value function is in general not a classical solution of the HJB equation (9.2). Therefore the question arises, in which class of solutions the optimal value function is the unique solution of
115
116
Chapter 9. Hamilton-Jacobi-Bellman Equations
the HJB equation, if such a class of solutions exists at all. The answer to this question are viscosity solutions. The main result will be that the optimal value function is the unique viscosity solution of the HJB equation.
The example a one-dimensional control problem [50]. The system dynamics
are given by
Ì‡ğ‘ (ğ‘¡) = ğ‘¢(ğ‘¡) ğ‘ (0) = ğ‘ 0,
âˆ€ğ‘¡ âˆˆ â„+ 0 ,
where ğ’® âˆ¶= [0,1] and ğ’œ âˆ¶= {Â±1}, and hence ğ‘ âˆ¶ â„+ 0 â†’ {Â±1}. The interpretation in classical mechanics is that the velocity of a particle at position ğ‘ (ğ‘¡) is controlled to be either +1 or âˆ’1. We define the current reward ğ‘Ÿ to always vanish and the boundary reward to be ğ‘…(0) âˆ¶= ğ‘…0 > 0 and ğ‘…(1) âˆ¶= ğ‘…1 > 0. Therefore the return is
0 â†’ [0,1] and ğ‘¢âˆ¶ â„+
eâˆ’ğ›¾ğœğ‘…0, eâˆ’ğ›¾ğœğ‘…1, The optimal value function is easily found. Since the current reward always vanishes, the best policy is to reach the boundary as quickly as possible because of the factor eâˆ’ğ›¾ğœ. Since we can only go to the left or to the right, the exit time is ğœ = 1 âˆ’ ğ‘  or ğœ = ğ‘ , respectively, yielding the optimal value function ğ‘£âˆ—(ğ‘ ) âˆ¶= max(ğ‘…0eâˆ’ğ›¾ğ‘ ,ğ‘…1e
ğ‘ (ğœ) = 0, ğ‘ (ğœ) = 1.
ğº(ğ‘ 0,ğ‘¢) = {
âˆ’ğ›¾(1âˆ’ğ‘ )
).
The HJB equation (9.2) simplifies to
ğ‘£âˆ—(ğ‘ ) = max
ğ‘âˆˆğ’œ={Â±1}
ğ‘ğ‘£â€²
âˆ—(ğ‘ ) = |ğ‘£â€²
âˆ—(ğ‘ )|
with the boundary conditions ğ‘£âˆ—(0) â‰¥ ğ‘…0 and ğ‘£âˆ—(1) â‰¥ ğ‘…1.
The first problem is that the optimal value function is not a classical solution of the HJB equation (9.2). Already in the case ğ‘…0 âˆ¶= 1, ğ‘…1 âˆ¶= 1, and ğ›¾ âˆ¶= 1, the optimal value function is not differentiable (at one point, namely ğ‘  = 1/2). Therefore it is plausible to admit generalized solutions that are differentiable almost everywhere. However, the second problem is that there may be infinitely many solutions satisfying the HJB equation (9.2) almost everywhere.
The third problem is that the optimal value function may satisfy the bound- ary condition only as a strict inequality. For the case ğ‘…0 âˆ¶= 1, ğ‘…1 âˆ¶= 5, and ğ›¾ âˆ¶= 1, the optimal value function is ğ‘£âˆ— = ğ‘…1eğ‘ âˆ’1. The boundary condition at ğ‘  = 0 is ğ‘‰ (0) > ğ‘…0. The reason is that the reward ğ‘…1 for leaving the domain [0,1] at ğ‘  = 1 is so much larger that the reward for leaving at ğ‘  = 0 that it is always the best policy to move to the right, even when starting at ğ‘  = 0. The boundary condition is therefore satisfied at ğ‘  = 0 as a strict inequality.
(9.5)
9.4. Viscosity Solutions
9.4 Viscosity Solutions
It turns out that the correct solution type for the HJB equation (9.2) are viscosity solutions in the sense that in this class of solutions, unique existence can be guaranteed. This is important: if the optimal value function is the unique solution of the HJB equation, we know that after solving the equation we can immediately solve the optimal-control problem by choosing the actions according to (9.4).
The name of viscosity solutions refers to the vanishing-viscosity method used
to show their existence [51].
To state some properties of viscosity solutions, we write the first-order equa-
tion under consideration as the boundary-value problem
ğ»(ğ‘ ,ğ‘£,âˆ‡ğ‘£) = 0
âˆ€ğ‘  âˆˆ ğ’®,
ğ‘£(ğ‘ ) = ğ‘¤(ğ‘ )
âˆ€ğ‘  âˆˆ ğœ•ğ’®,
where ğ’® is an open domain, ğ‘¤âˆ¶ ğœ•ğ’® â†’ â„ is the boundary condition, and the given function ğ» is called the Hamiltonian of the system. The HJB equation (9.2) corresponds to
ğ»(ğ‘ ,ğ‘£,ğ‘) âˆ¶= ğ›¾ğ‘£ âˆ’ sup ğ‘âˆˆğ’œ(ğ‘ )
(ğ‘Ÿ(ğ‘ ,ğ‘) + ğ‘ â‹… ğ‘“(ğ‘ ,ğ‘))
Definition 9.5 (viscosity solution). Suppose ğ‘£âˆ¶ ğ’® â†’ â„ is continuous and that ğ‘£ = ğ‘¤ on ğœ•ğ’®.
The function ğ‘£ is called a viscosity subsolution of (9.6) if the following state- ment holds for all ğœ™ âˆˆ ğ¶1(ğ’®): If ğ‘£ âˆ’ ğœ™ has a local maximum at ğ‘ 0 âˆˆ ğ’®, then ğ»(ğ‘ 0,ğ‘£(ğ‘ 0),âˆ‡ğœ™(ğ‘ 0)) â‰¤ 0.
The function ğ‘£ is called a viscosity supersolution of (9.6) if the following statement holds for all ğœ™ âˆˆ ğ¶1(ğ’®): If ğ‘£ âˆ’ğœ™ has a local minimum at ğ‘ 0 âˆˆ ğ’®, then ğ»(ğ‘ 0,ğ‘£(ğ‘ 0),âˆ‡ğœ™(ğ‘ 0)) â‰¥ 0.
The function ğ‘£ is called a viscosity solution of (9.6) if it is both a viscosity
subsolution and a viscosity supersolution.
Continuing the example in Section 9.3 for the case ğ‘…0 âˆ¶= 1, ğ‘…1 âˆ¶= 1, and ğ›¾ âˆ¶= 1, it can be checked by elementary calculations that (9.5) is a viscosity solution of the HJB equation ğ»(ğ‘ ,ğ‘£(ğ‘ ),ğ‘£â€²(ğ‘ )) âˆ¶= ğ‘£(ğ‘ ) âˆ’ |ğ‘£â€²(ğ‘ )| = 0 as follows. The function ğ‘£âˆ— is a classical solution on both intervals (0,1/2) and (1/2,1) and hence also a viscosity solution on these intervals by Lemma 9.6. To establish that ğ‘£âˆ— is a viscosity solution, it suffices to check the conditions in the definition at ğ‘  = 1/2.
Next, we summarize a few basic properties of viscosity solutions.
117
(9.6a)
(9.6b)
118
Chapter 9. Hamilton-Jacobi-Bellman Equations
Lemma 9.6. Suppose ğ‘£ âˆˆ ğ¶1(ğ’®) be a classical solution of (9.6). Then it is also a viscosity solution.
Lemma 9.7. Suppose ğ‘£ âˆˆ ğ¶(ğ’®) is a viscosity solution of (9.6) and suppose ğ‘£ is differentiable at ğ‘ 0 âˆˆ ğ’®. Then ğ»(ğ‘ 0,ğ‘£(ğ‘ 0),âˆ‡ğ‘£(ğ‘ 0)) = 0.
Proof. [52, Section 10.1.2].
Theorem 9.8. Under certain assumptions on the Hamiltonian ğ», the boundary- value problem (9.6) has at most one bounded viscosity solution.
Proof. [51, Theorem III.1].
So far, we have seen that the solution of the boundary-value problem (9.6) is unique among the viscosity solutions under some assumptions. However, the boundary condition in Theorem 9.3 is the inequality (9.3) and not an equality such as the boundary condition in (9.6). The theorems below show that the HJB equation has a unique viscosity solution with the following inequality boundary conditions and with a further assumption.
Definition 9.9 (viscosity solution with inequality boundary conditions). Sup- pose ğ‘£ is a viscosity solution of the equation
ğ»(ğ‘ ,ğ‘£(ğ‘ ),âˆ‡ğ‘£(ğ‘ )) = 0.
Then ğ‘£ is called a viscosity solution with the inequality boundary conditions
ğ‘£(ğ‘ ) â‰¥ ğ‘¤(ğ‘ )
âˆ€ğ‘  âˆˆ ğœ•ğ’®
if the following two conditions hold.
1. If ğœ™ âˆˆ ğ¶1(ğ’®) and the function ğ‘£âˆ’ğœ™ has a local maximum at ğ‘ 0 âˆˆ ğœ•ğ’®, then
min(ğ»(ğ‘ 0,ğ‘£(ğ‘ 0),âˆ‡ğœ™(ğ‘ 0)),ğ‘£(ğ‘ 0) âˆ’ ğ‘¤(ğ‘ 0)) â‰¤ 0.
2. If ğœ™ âˆˆ ğ¶1(ğ’®) and the function ğ‘£âˆ’ğœ™ has a local minimum at ğ‘ 0 âˆˆ ğœ•ğ’®, then
max(ğ»(ğ‘ 0,ğ‘£(ğ‘ 0),âˆ‡ğœ™(ğ‘ 0)),ğ‘£(ğ‘ 0) âˆ’ ğ‘¤(ğ‘ 0)) â‰¥ 0.
The following assumption means that at any point on the boundary of the state space there is at least one trajectory that is not tangential to the boundary.
Assumption 9.10. The following two assumptions hold for all ğ‘  âˆˆ ğœ•ğ’®, where ğ‘›(ğ‘ ) denotes the outward pointing normal vector of ğ’® at ğ‘ .
9.5. Stochastic Optimal Control
1. If there exists an ğ‘ âˆˆ ğ’œ such that ğ‘“(ğ‘ ,ğ‘) â‹… ğ‘›(ğ‘ ) â‰¤ 0, then there exists an ğ‘â€² âˆˆ ğ’œ such that ğ‘“(ğ‘ ,ğ‘â€²) â‹… ğ‘›(ğ‘ ) < 0.
2. If there exists an ğ‘ âˆˆ ğ’œ such that ğ‘“(ğ‘ ,ğ‘) â‹… ğ‘›(ğ‘ ) â‰¥ 0, then there exists an ğ‘â€² âˆˆ ğ’œ such that ğ‘“(ğ‘ ,ğ‘â€²) â‹… ğ‘›(ğ‘ ) > 0.
Theorem 9.11. Suppose that Assumption 9.10 holds. Then the equation (9.2) with the boundary condition (9.3) has a unique viscosity solution with an in- equality boundary condition.
Proof. [50, Theorem 4], [49, Section II.11 and II.13].
In summary, viscosity solutions are the right class of solutions for the HJB equation (9.2) in the sense that there always exists a unique solution for the purposes of Theorem 9.3 and solving the optimal-control problem.
9.5 Stochastic Optimal Control
If the environment is stochastic, the dynamics of the system are described by a stochastic ordinary differential equation. This case is realistic because of random fluctuations in the system and lack of precision in measurements.
In the case of additive and normally distributed noise, the dynamics of the
system are given by the stochastic ordinary differential equation
dğ‘  = ğ‘“(ğ‘ (ğ‘¡)),ğ‘¢(ğ‘¡))dğ‘¡ + ğœ(ğ‘ (ğ‘¡),ğ‘¢(ğ‘¡))dğœ”
âˆ€ğ‘¡ âˆˆ â„+ 0 ,
ğ‘ (0) = ğ‘ 0 âˆˆ ğ’®
to be understood in the sense of ItÃ´ calculus. Here ğœ” is a Brownian motion of dimension ğ‘‘ âˆ¶= dimğ’®, and ğœ is an ğ‘› Ã— ğ‘‘ matrix, where ğ‘› âˆ¶= dimğ’® + dimğ’œ.
Definition 9.12 (stochastic process). If (Î©,â„±,ğ‘ƒ) is a probability space and (ğ‘†,Î£) is a measurable space, then a stochastic process is a set {ğ‘‹ğ‘¡ âˆ£ ğ‘¡ âˆˆ ğ‘‡} of random variables ğ‘‹ğ‘¡ with values in ğ‘†.
Definition 9.13 (Brownian motion). A Brownian motion or a Wiener process is a stochastic process ğœ” that satisfies the following conditions.
1. ğœ”(0) = 0.
2. ğœ”ğ‘¡ is almost surely continuous for all ğ‘¡ âˆˆ â„+ 0 .
3. The increments of ğœ” are independent, i.e., the increments ğœ”ğ‘¡1 are independent random variables if 0 â‰¤ ğ‘ 1 < ğ‘¡1 â‰¤ ğ‘ 2 < ğ‘¡2.
119
(9.7a)
(9.7b)
120
Chapter 9. Hamilton-Jacobi-Bellman Equations
4. The increments are normally distributed; more precisely,
âˆ€ğ‘¡ âˆˆ â„+
0 âˆ¶ âˆ€ğ‘  âˆˆ [0,ğ‘¡] âˆ¶
ğœ”ğ‘¡ âˆ’ ğœ”ğ‘  âˆ¼ ğ‘(0,ğ‘¡ âˆ’ ğ‘ ).
In the stochastic case, the return
ğºâˆ¶ ğ’® Ã— (â„+
0 â†’ ğ’œ(ğ‘¡)) â†’ â„,
ğº(ğ‘ 0,ğ‘¢) âˆ¶= ğ”¼[âˆ« 0
ğœ
eâˆ’ğ›¾ğ‘¡ğ‘Ÿ(ğ‘ (ğ‘¡),ğ‘¢(ğ‘¡))dğ‘¡ + eâˆ’ğ›¾ğœğ‘…(ğ‘ (ğœ))].
is the expected value over all trajectories of the stochastic process that is the solution of the state equation. Then the definition of the optimal value function remains the same.
It can be shown that if the optimal value function ğ‘£âˆ— is in ğ¶2(ğ’® â†’ â„), then
it satisfies the HJB equation
ğ›¾ğ‘£âˆ—(ğ‘ ) = sup ğ‘âˆˆğ’œ(0)
(ğ‘Ÿ(ğ‘ ,ğ‘) + âˆ‡ğ‘£âˆ—(ğ‘ ) â‹… ğ‘“(ğ‘ ,ğ‘) +
1 2
ğ‘› âˆ‘ ğ‘–=1
ğ‘› âˆ‘ ğ‘—=1
ğ‘ğ‘–ğ‘—(ğ‘ ,ğ‘)
ğœ•ğ‘£âˆ— ğœ•ğ‘ ğ‘–ğœ•ğ‘ ğ‘—
(ğ‘ ))
âˆ€ğ‘  âˆˆ ğ’®,
(9.8a)
ğ‘£âˆ—(ğ‘ ) = ğ‘…(ğ‘ )
âˆ€ğ‘  âˆˆ ğœ•ğ’®,
(9.8b)
where (ğ‘ğ‘–ğ‘—) = ğ´ âˆ¶= ğœğœâŠ¤. This HJB equation is a nonlinear, second-order partial differential equation.
If the matrix ğ´ is uniformly elliptic, then the HJB equation (9.8) has a unique classical solution. Otherwise, the concept of viscosity solutions extended to second-order equations [53] can be used.
9.6 Bibliographical and Historical Remarks
Problems
Chapter 10
Deep Reinforcement Learning
That a computer can be programmed to play legal chess or checkers is hardly remarkable; that it can learn from experience to play mas- ter checkers is interesting and represents a feat of programming. It cannot, at present, learn from experience to play master chess, and an ability of this type would represent an astounding breakthrough in the theory of adaptive processes.
Richard Bellman [38, p. 37].
Algorithmic advancements and huge computational resources have made it pos- sible in recent years to train reinforcement-learning agents based on deep neural networks that can outperform humans in playing games such as Atari 2600, chess, and Go.
10.1 Introduction
Series of papers coming out of Google DeepMind: [13], [54], [8], [9], [14], [55].
10.2 Atari 2600 Games
In [13], the action-value function was represented by a deep neural network, termed a deep Q-network (DQN). The DQN agent received only the pixels and the game score as inputs and was able to achieve a level comparable to that of a professional human games tester. Importantly, the same algorithm, neural- network architecture, and hyperparameters were used across a set of 49 games,1
1DQN plays Breakout: https://www.youtube.com/watch?v=TmPfTpjtdgg. DQN plays Space
Invaders: https://www.youtube.com/watch?v=W2CAghUiofY.
121
122
Chapter 10. Deep Reinforcement Learning
representing a diverse collection of tasks.
The algorithm is shown in Algorithm 19.
Algorithm 19 deep Q-network (DQN) with experience replay.
initialization: initialize replay memory ğ· initialize action-value function Ì‚ğ‘(w) with random weights w initialize target action-value function Ì‚ğ‘(wâˆ’) with weights wâˆ’ âˆ¶= w
for episode âˆˆ (0,1,â€¦,ğ‘€) do
â–· for all episodes
initialize ğ‘ 0 âˆ¶= ğ‘¥0 and preprocess ğœ™0 âˆ¶= ğœ™(ğ‘ 0) for ğ‘¡ âˆˆ (0,1,â€¦,ğ‘‡) do
â–· for all time steps
select action ğ‘ğ‘¡ ğœ–-greedily as argmaxğ‘ Ì‚ğ‘(ğ‘ ğ‘¡,ğ‘,w) perform action ğ‘ğ‘¡ in the emulator, obtain reward ğ‘Ÿğ‘¡+1 and image ğ‘¥ğ‘¡+1 ğ‘ ğ‘¡+1 âˆ¶= (ğ‘ ğ‘¡,ğ‘ğ‘¡,ğ‘¥ğ‘¡+1) preprocess ğœ™ğ‘¡+1 âˆ¶= ğœ™(ğ‘ ğ‘¡+1) store transition (ğœ™ğ‘¡,ğ‘ğ‘¡,ğ‘Ÿğ‘¡+1,ğœ™ğ‘¡+1) in ğ· sample transitions (ğœ™ğ‘—,ğ‘ğ‘—,ğ‘Ÿğ‘—+1,ğœ™ğ‘—+1) from ğ· set
ğ‘¦ğ‘— âˆ¶= {
ğ‘Ÿğ‘—, ğ‘Ÿğ‘— + ğ›¾ maxğ‘â€² Ì‚ğ‘(ğœ™ğ‘—+1,ğ‘â€²,wâˆ’),
if episode terminates at step ğ‘— + 1,
otherwise
perform a gradient-descent step on âˆ‘ğ‘—(ğ‘¦ğ‘— âˆ’ Ì‚ğ‘(ğ‘ ,ğ‘,w)) every ğ¶ steps: wâˆ’ âˆ¶= w
2
w.r.t. w
end for
end for
We denote the approximation, a neural network, of the action-value function
by Ì‚ğ‘(ğ‘ ,ğ‘,w) as usual. The ğ‘„-learning update uses the quadratic loss function
ğ¿ğ‘–(wğ‘–) âˆ¶= ğ”¼(ğ‘ ,ğ‘,ğ‘Ÿ,ğ‘ â€²)âˆ¼ğ‘ˆ(ğ·ğ‘–)[(ğ‘Ÿ + ğ›¾ max ğ‘â€²
2 ğ‘– ) âˆ’ Ì‚ğ‘(ğ‘ ,ğ‘,wğ‘–))
Ì‚ğ‘(ğ‘ â€²,ğ‘â€²,wâˆ’
],
which is the mean-squared error of the Bellman equation. Here the agentâ€™s experiences
(ğ‘ ğ‘¡,ğ‘ğ‘¡,ğ‘Ÿğ‘¡+1,ğ‘ ğ‘¡+1)
in each time step ğ‘¡ are stored in data sets ğ·ğ‘¡ âˆ¶= {ğ‘’1,â€¦,ğ‘’ğ‘¡}, and the expected value is approximated by drawing samples, or minibatches in the language of neural networks, (ğ‘ ,ğ‘,ğ‘Ÿ,ğ‘ â€²) uniformly from the data set ğ·ğ‘– in training iteration ğ‘–.
10.2. Atari 2600 Games
123
The parameters wâˆ’ ğ‘– are parameters from an iteration before the ğ‘–-th one, in contrast to the parameters wğ‘– in the ğ‘–-th iteration. The gradient of the loss function is given by
âˆ‡wğ‘–
ğ¿ğ‘–(wğ‘–) = âˆ’2ğ”¼(ğ‘ ,ğ‘,ğ‘Ÿ,ğ‘ â€²)âˆ¼ğ‘ˆ(ğ·ğ‘–)[(ğ‘Ÿ+ğ›¾ max ğ‘â€²
Ì‚ğ‘(ğ‘ â€²,ğ‘â€²,wâˆ’
ğ‘– )âˆ’Ì‚ğ‘(ğ‘ ,ğ‘,wğ‘–))âˆ‡wğ‘–
Ì‚ğ‘(ğ‘ ,ğ‘,wğ‘–)].
ğ‘„-learning is recovered as the special case where wâˆ’ ğ‘– is replaced by just using the current sample.
âˆ¶= wğ‘–âˆ’1 and the expectation
The neural network that serves as the approximation of the action-value function takes a preprocessed 84Ã—84Ã—4 image as its input. Three convolutional layers are followed by two fully connected layers, and the activation functions in each layer are the rectifiers ğ‘¥ â†¦ max(0,ğ‘¥). The output layer has a single output for each valid action. There are eighteen valid actions: nine directions (including no input corresponding to a centered joystick) and these nine directions with the fire button pressed simultaneously.
The RMSProp algorithm with samples or minibatches of size 32 was used to train the neural network in the DQN algorithm. The particular choice of training algorithm is not a defining feature of the whole procedure, as different algorithms were used in later works.
For all games, the discount factor was ğ›¾ = 0.99. The behavior policy was ğœ–-greedy and started with ğœ– = 1, which was linearly reduced to ğœ– = 0.1 after the first million frames and fixed at this value thereafter. Fifty million frames were used for training, which corresponded to about 38 days of game experience. Frames were skipped; more precisely, in all games actions were selected only on every fourth frame, and the last action was repeated on all frames in between. This simple frame-skipping technique helped reduce computation time, since the emulator runs much faster than having the agent select an action. The size of the buffer used for experience replay was one million of the most recent frames. The stability of the algorithm is important, especially when dealing nonlin- ear functions such as neural networks. Two provisions improve the stability. First, the error terms ğ‘¦ğ‘— âˆ’ Ì‚ğ‘(ğ‘ ,ğ‘,w) are clipped to always be between âˆ’1 and 1. Second, two separate networks, represented by the parameters w and wâˆ’, are used. The target values ğ‘¦ğ‘— in the ğ‘„-learning updates are found using the net- work with the older parameters wâˆ’, and the parameters wâˆ’ are updated to the current parameters w regularly after a certain number of time steps. This provision increases the stability of the algorithm, since an update that increases Ì‚ğ‘(ğ‘ ğ‘¡,ğ‘ğ‘¡) often also increases Ì‚ğ‘(ğ‘ ğ‘¡+1,ğ‘) for all ğ‘ as consecutive states are often highly correlated in such applications. Hence the target ğ‘¦ğ‘— is also increased, which possibly leads to oscillations or divergence of the action-value function. Generating the update targets ğ‘¦ğ‘— using the older parameters wâˆ’ delays any ef-
124
Chapter 10. Deep Reinforcement Learning
fects of updates to Ì‚ğ‘ on the targets ğ‘¦ğ‘— and thus makes oscillations or divergence much more unlikely.
10.3 Go and Tree Search (AlphaGo)
Go is the most challenging of the classic board games due to its huge search space, being larger than the one of chess, and the difficulty of evaluating positions and moves. In [54], a computer program named AlphaGo defeated a human professional player in the full-sized game of Go for the first time. The human player was the European Go champion and he was defeated by 5 games to 0. Playing against other programs, AlphaGo won 99.8% of the games.
MCTS (see, e.g., [23, Section 8.11]) had been known to be the best algorithm for playing Go and to achieve strong amateur play, and had previously been used with policies or value functions based on linear combinations of input features. The main innovation in [54] was to employ deep neural networks as policies and value functions and to learn in a stable manner while doing so. Unsurprisingly, the computational effort was enormous.
Training the AlphaGo program consisted of several stages using different methods. In the first step, the policy network was trained using a data set of expert human moves and supervised learning. The advantage is fast learning in the beginning based on gradients of high quality, although learning in this step maximizes predictive accuracy and not winning games.
Then, reinforcement learning was used to optimize the policy network from the first step using self-play. In this way, the policy network is adjusted to- wards winning games rather than accurate prediction of expert human moves. Stochastic gradient ascent was used for optimization, and the current policy network played against a randomly selected previous iteration of the policy net- work. Randomizing the opponents stabilized training and prevented overfitting to playing against the current policy.
In the third step, a value network was trained to predict the winners of games played by the policy from the second step playing against itself. A new data set was generated to prevent overfitting, and stochastic gradient descent was used to minimize the mean squared error.
Finally, the AlphaGo program used MCTS and combined the policy and value networks from the previous steps. The positions were evaluated using the value network, and the actions were sampled using the policy network, making the MCTS algorithm very efficient.
10.4. Learning Go Tabula Rasa (AlphaGo Zero)
10.4 Learning Go Tabula Rasa (AlphaGo Zero)
The next stage in the development of the Alpha programs was to render the ex- istence of a preexisting data set of expert moves superfluous. This was achieved in [8], where AlphaGo Zero learned tabular rasa, i.e., without any preexisting knowledge, to achieve superhuman proficiency in playing Go. It became the first program to defeat a world champion, thus achieving superhuman performance, and it won 100 to 0 against AlphaGo [54].
In AlphaGo, MCTS evaluated positions and selected moves using the value and policy deep neural networks. These neural networks in AlphaGo were ini- tially trained using supervised learning from a data set with human expert moves. Reinforcement learning and self-play were used only later. AlphaGo Zero used solely reinforcement learning without any human expert moves, guid- ance, or domain knowledge beyond the rules of the game; it learned tabula rasa, i.e., it started learning from random play.
While AlphaGo used MCTS and separate policy and a value networks, Al- It also employed a simpler phaGo Zero used solely a single neural network. tree search based on this single neural network to evaluate positions and sample moves. The single neural network receives a raw board representation of the position and its history, and it outputs both a vector of move probabilities and scalar value that estimates the probability of the current player winning from the current position. Thus the single network takes on the roles of both the policy and value networks. The structure of the network is quite complicated, consisting of many blocks of convolutional layers with batch normalization and rectifier nonlinearities.
10.5 Chess, Shogi, and Go through Self-Play (Alp-
haZero)
In [9], the approach taken in AlphaGo Zero was generalized into a single algo- rithm, called AlphaZero, that achieved superhuman performance in the chal- lenging board games. Like AlphaGo Zero, AlphaZero started from random play and without any domain knowledge except the game rules. AlphaZero could defeat a world champion in chess, shogi (Japanese chess, with a more complex game tree than chess), and Go.
125
126
Chapter 10. Deep Reinforcement Learning
10.6 Video Games of the 2010s (AlphaStar)
In [14] and [55], attention returned to video games with partial observability after the board games with no hidden information.
In [14], learning in the presence of multiple agents was addressed as an extension of the work on the board games that are two-player turn based games. The video game was a three-dimensional multi-player first-person video game, namely Quake III Arena in a mode called Capture the Flag. A tournament- style evaluation was used to evaluate the performance of the agent, which could only use pixels and game points scored as its input and achieved human-level performance.
Learning proceeded by concurrently training a diverse population of agents which played against each other. This approach seemed to stabilize learning despite the partially observable environments and the multi-agent nature of the game. The learning algorithm for each player was a multi-step actor-critic policy-gradient algorithm with off-policy correction and auxiliary tasks. The policies were represented as multi-time-scale recurrent neural networks with ex- ternal memory. The agents built hierarchical temporary representations and recurrent latent variable model for their sequential input data. This results in the construction of temporally hierarchical representations that favor the use of memory and of temporally coherent action sequences.
Self-play can be unstable and does not support concurrent training in its basic form. Therefore a population of different agents was trained in parallel, which stabilized training. In the episodes, each agent learned by playing with team mates and against opponents sampled from the whole population.
In [55], the AlphaStar program that plays the StarCraft II game is described. StarCraft II is considered one of the most difficult games in professional esports due to its complexity and multi-agent challenges. AlphaStar employs data from both human and agent games and strategies and counter-strategies that are continually adapted. The policies are represented by deep neural networks. AlphaStar competed in a series of online games against human players in the full game of StarCraft II. It was rated at grandmaster level for all three StarCraft races and ranked above 99.8% of officially ranked human players.
10.7 Improvements to DQN and their Combination
In [56], six independent extensions to the DQN algorithm [13] were discussed, and their combinations were studied empirically. Each of the six extensions turned out to improve performance on a selection of 57 Atari 2600 games, and
10.7.
Improvements to DQN and their Combination
127
substantially so in most cases. All six improvements can also be combined into an algorithm called the rainbow algorithm in this work. Furthermore, an ablation study was performed to show the contribution of each improvement to the overall performance of the rainbow algorithm.
While many extensions to the DQN algorithm, Algorithm 19, have been proposed, the six extensions were chosen such that they address distinct limi- tations of the DQN algorithm. Before discussing the six extensions, we recall that DQN uses ğ‘„-learning to define the loss function that is minimized in order to determine the parameters of a (deep) neural network that is trained using stochastic gradient descent. Two important features to improve its stability in face of nonlinear function approximation are experience replay and the use of two parameters w (of the online network) and wâˆ’ (of the target network).
10.7.1 Double Q-Learning
The first extension is double ğ‘„-learning already presented as Algorithm 10 in Section 5.6.
10.7.2 Prioritized Replay
The DQN algorithm samples uniformly from the experience-replay buffer. How- ever, using transitions from which there is much to learn more often would seem to be more efficient, and these transitions should be sampled more frequently. Prioritized replay hence assigns the probability
ğ‘ğ‘¡ âˆ âˆ£ğ‘…ğ‘¡+1 + ğ›¾ğ‘¡+1 max
ğ‘â€²
ğœ” Ì‚ğ‘(ğ‘†ğ‘¡+1,ğ‘â€²,wâˆ’) âˆ’ Ì‚ğ‘(ğ‘†ğ‘¡,ğ´ğ‘¡,w)âˆ£
to transitions based on the absolute TD error, where ğœ” is a hyperparameter. Furthermore, new transitions are inserted in the replay buffer with higher pri- ority.
10.7.3 Dueling Networks
Dueling networks are an architecture of neural networks specifically designed for value functions in reinforcement learning. They correspond to an action-value function of the form
ğ‘(ğ‘ ,ğ‘,w) = ğ‘£(ğ‘“(ğ‘ ,w1),w2) + ğ‘‘(ğ‘“(ğ‘ ,w1),ğ‘,w3) âˆ’
1 |ğ’œ|
âˆ‘ ğ‘â€²âˆˆğ’œ
ğ‘‘(ğ‘“(ğ‘ ,w1),ğ‘â€²,w3),
where w1, w2, and w3 are the parameters of the shared encoder ğ‘“, the value stream ğ‘£, and the advantage stream ğ‘‘ such that w = (w1,w2,w3). Thus the value and advantage streams share the convolutional encoder.
128
Chapter 10. Deep Reinforcement Learning
10.7.4 Multi-Step Methods
Multi-step methods are discussed in Chapter 5 and often lead to faster learning with a suitable chosen number of steps. A multi-step variant of DQN can be defined as minimizing the loss
(ğºğ‘¡âˆ¶ğ‘¡+ğ‘› + ğ›¾ğ‘›,ğ‘¡ max ğ‘â€²âˆˆğ’œ
2 Ì‚ğ‘(ğ‘†ğ‘¡+ğ‘›,ğ‘â€²,w) âˆ’ ğ‘(ğ‘†ğ‘¡,ğ´ğ‘¡,w))
based on the ğ‘›-step return ğºğ‘¡âˆ¶ğ‘¡+ğ‘›.
10.7.5 Distributional Reinforcement Learning
Distributional reinforcement learning is the largest extension and provides a conceptual shift. Instead of maximizing the expected return as usual in rein- forcement learning, we can approximate the distribution of the returns. This can be achieved for example by discretizing the distribution of returns as discrete probability masses placed on a discrete support or (equidistant) grid z. Then the distribution ğ‘‘ğ‘¡ at time ğ‘¡ takes the values ğ‘ğ‘– ğœƒ(ğ‘ ,ğ‘) on each grid point ğ‘§ğ‘–, and ğ‘‘ğ‘¡ can be written as ğ‘‘ğ‘¡ = (z,pğœƒ(ğ‘ ,ğ‘)). The parameter ğœƒ must be determined such that this distribution approximates the true distribution of returns.
To approximate the distribution of returns, a variant of Bellmanâ€™s optimality equation for distributions is useful. Furthermore, the difference between the target distribution
ğ‘‘â€² ğ‘¡ âˆ¶= (ğ‘…ğ‘¡+1 + ğ›¾ğ‘¡+1z,pğœƒâ€²(ğ‘ ,argmax
Ì‚ğ‘(ğ‘†ğ‘¡+1,ğ‘,ğœƒâ€²))
ğ‘âˆˆğ’œ
and ğ‘‘ğ‘¡ must be minimized, for example by minimizing the Kullbeck-Leibler divergence
ğ·KL(Î¦z(ğ‘‘â€²
ğ‘¡)â€–ğ‘‘ğ‘¡).
Here the second argument of pğœƒâ€² is the greedy action with respect to the mean action values
Ì‚ğ‘(ğ‘†ğ‘¡+1,ğ‘,ğœƒâ€²) = z â‹… pğœƒâ€²(ğ‘†ğ‘¡+1,ğ‘), and Î¦z is the ğ¿2 projection of the target distribution ğ‘‘â€²
ğ‘¡ onto the grid z.
10.7.6 Noisy Neural Networks
In some applications, such as in the game Montezumaâ€™s Revenge, rewards are delayed a long time from the actions and many actions must be performed to collect the first reward. In such cases, ğœ–-greedy policies may provide insufficient
10.7.
Improvements to DQN and their Combination
exploration. To overcome this limitation, noisy neural networks include a noisy linear hidden layer of the form
y âˆ¶= (ğ‘Šx + b) + ((ğ‘Šnoisy âŠ™ ğœ–ğ‘Š)x + bnoisy âŠ™ ğœ–b)
which combines a deterministic, linear stream (the first term) with a noisy stream (the second term). Here ğœ–ğ‘Š and ğœ–b are random variables, and âŠ™ denotes elementwise multiplication. Because the hidden layer consists of a deterministic and a noisy term, the neural network can learn to ignore the noisy stream over time even with different rates in different parts of the state space. This makes it possible to explore different parts of the state space at different speeds.
129
130
Chapter 10. Deep Reinforcement Learning
Chapter 11
Distributional Reinforcement Learning
11.1 Introduction
In this chapter, we assume that the state and action spaces are finite. Anal- ogously to the term categorical probability distribution, this chapter is hence about categorical distributional reinforcement learning.
11.2 Markov Decision Processes and Bellman Oper-
ators
Definition 11.1 (Markov decision process). A Markov decision process is de- scribed by the tuple (ğ’®,ğ’œ,ğ‘,ğ‘Ÿ,ğ›¾).
ğ‘ƒ âˆ¶
ğ’® Ã— ğ’œ â†’ ğ’«(ğ’®),
where ğ’«(â„°) is the set of probability measures on the set â„° of events. ğ’«ğ‘(â„°) is the set of probability measures on â„° having bounded support.
ğ‘(ğ‘ â€² âˆ£ ğ‘ ,ğ‘) = ğ‘ƒ(ğ‘ ,ğ‘)(ğ‘ â€²)
Next, we recapitulate Bellman operators in the classic, non-distributional
approach.
ğ‘„âˆ¶ ğ’® Ã— ğ’œ â†’ â„
131
132
Chapter 11. Distributional Reinforcement Learning
ğ‘‡ğœ‹ is a classic Bellman operator.
(ğ‘‡ğœ‹ğ‘„)(ğ‘ ,ğ‘) âˆ¶= âˆ‘ ğ‘ â€²
ğ‘(ğ‘ â€² âˆ£ ğ‘ ,ğ‘)(ğ‘Ÿ(ğ‘ ,ğ‘,ğ‘ â€²) + ğ›¾âˆ‘ ğ‘â€²
ğœ‹(ğ‘â€²|ğ‘ â€²)ğ‘„(ğ‘ â€²,ğ‘â€²))
ğ‘‡ğœ‹ is a ğ›¾-contraction with respect to the maximum norm â€–.â€–âˆ. Therefore there exists a unique fixed point ğ‘„ğœ‹
ğ‘„ğœ‹ = ğ‘‡ğœ‹[ğ‘„ğœ‹]
ğ‘‡âˆ— is the classic Bellman optimality operator
(ğ‘‡âˆ—ğ‘„)(ğ‘ ,ğ‘) âˆ¶= âˆ‘ ğ‘ â€²
ğ‘(ğ‘ â€² âˆ£ ğ‘ ,ğ‘)(ğ‘Ÿ(ğ‘ ,ğ‘,ğ‘ â€²) + ğ›¾ max
ğ‘â€²âˆˆğ’œ(ğ‘ â€²)
ğ‘„(ğ‘ â€²,ğ‘â€²))
Analogously to ğ‘‡ğœ‹, the Bellman optimality operator ğ‘‡âˆ— is a ğ›¾-contraction with respect to the maximum norm. Therefore there exists a unique fixed point ğ‘„âˆ—
ğ‘„âˆ— = ğ‘‡âˆ—[ğ‘„âˆ—]
In order to formulate distributional versions of the Bellman operators, we
need the push-forward measure.
If ğœˆ âˆˆ ğ’«ğ‘(â„) is a probability measure and ğ‘“ âˆ¶ â„ â†’ â„ is a measurable function,
then the push-forward measure ğ‘“#ğœˆ is defined by
(ğ‘“#ğœˆ)(ğ´) âˆ¶= ğœˆ(ğ‘“âˆ’1(ğ´)) = ğœˆ({ğ‘¥ âˆˆ â„ âˆ¶ ğ‘“(ğ‘¥) âˆˆ ğ´})
where ğ´ âŠ‚ â„ is a Borel set.
In the following, we will need measures that are pushed by affine transfor-
mations of the form
ğ‘™ğ‘Ÿ,ğ›¾(ğ‘¥) âˆ¶= ğ‘Ÿ + ğ›¾ğ‘¥.
(ğ‘™ğ‘Ÿ,ğ›¾#ğœˆ)(ğ´) = ğœˆ(ğ‘™âˆ’1
ğ‘Ÿ,ğ›¾(ğ´)) = ğœˆ({ğ‘¥ âˆˆ â„ âˆ¶ ğ‘Ÿ + ğ›¾ğ‘¥ âˆˆ ğ´})
If ğ›¾ â‰  0, we have ğ‘Ÿ + ğ›¾ğ‘¥ = ğ‘§ âˆˆ ğ´ and therefore ğ‘¥ = (ğ‘§ âˆ’ ğ‘Ÿ)/ğ›¾. This yields
(ğ‘™ğ‘Ÿ,ğ›¾#ğœˆ)(ğ´) = ğœˆ ({
ğ‘§ âˆ’ ğ‘Ÿ ğ›¾
âˆˆ â„ âˆ¶ ğ‘§ âˆˆ ğ´}).
If ğ›¾ = 0, then
(ğ‘™ğ‘Ÿ,0#ğœˆ)(ğ´) = ğœˆ({ğ‘¥ âˆˆ â„ âˆ¶ ğ‘Ÿ âˆˆ ğ´}).
We have
{ğ‘¥ âˆˆ â„ âˆ¶ ğ‘Ÿ âˆˆ ğ´} = {
âˆ…, â„,
ğ‘Ÿ âˆ‰ ğ´, ğ‘Ÿ âˆˆ ğ´,
11.3. Distributional Speedy Q-Learning
and therefore
ğœˆ({ğ‘¥ âˆˆ â„ âˆ¶ ğ‘Ÿ âˆˆ ğ´}) = âŸ¦ğ‘Ÿ âˆˆ ğ´âŸ§.
distributional Bellman operator
Lemma 11.2. Suppose ğ‘Ÿ âˆˆ â„, ğ›¾ âˆˆ â„+, and ğ‘ is a random variable. Then the CDF of the transformed random variable ğ‘Ÿ + ğ›¾ğ‘ can be written in terms of the CDF of ğ‘ as
ğ¹ğ‘Ÿ+ğ›¾ğ‘(ğ‘§) = ğ¹ğ‘ (
ğ‘§ âˆ’ ğ‘Ÿ ğ›¾
)
âˆ€ğ‘§ âˆˆ â„.
Proof. Using the definition of the CDF, we have
ğ¹ğ‘Ÿ+ğ›¾ğ‘(ğ‘§) = â„™[ğ‘Ÿ + ğ›¾ğ‘ â‰¤ ğ‘§] = â„™[ğ‘ â‰¤
ğ‘§ âˆ’ ğ‘Ÿ ğ›¾
] = ğ¹ğ‘ (
ğ‘§ âˆ’ ğ‘Ÿ ğ›¾
)
for any ğ‘§ âˆˆ â„.
11.3 Distributional Speedy Q-Learning
[57]
11.4 Bibliographical Remarks
11.5 Exercises
133
134
Chapter 11. Distributional Reinforcement Learning
Chapter 12
Large Language Models
Large Language Models are artificial neural networks that generate texts in nat- ural languages following commands or prompts. In this chapter, the training and functioning of large language models, and ChatGPT in particular, is explained. Transformers and their attention mechanism provide the basis for text gener- ation token by token. But generating grammatically correct and beautifully formulated text is not enough. The output of large language models should be aligned to the intentions of the user. This is achieved by various training steps, one of them being reinforcement learning from human feedback.
12.1 Introduction
The processing of natural language has been part of the goals of the field of ar- tificial intelligence since its inception. The proposal for the Dartmouth Summer Research Project on Artificial Intelligence, dated August 31, 1955, mentions
natural language,
artificial neuronal networks,
self-improvement, and
creativity
among â€œsome aspects of the artificial intelligence problem.â€ (The Dartmouth Workshop took place one year later, in the summer of 1956.) These four aspects are certainly essential for large language models (LLM), showing how prescient the proposal was. The proposal says the following about the second of the seven aspects mentioned.
135
136
Chapter 12. Large Language Models
â€œHow Can a Computer be Programmed to Use a Language
It may be speculated that a large part of human thought consists of manipulating words according to rules of reasoning and rules of con- jecture. From this point of view, forming a generalization consists of admitting a new word and some rules whereby sentences contain- ing it imply and are implied by others. This idea has never been precisely formulated nor have examples been worked out.â€
ChatGPT was launched on November 30, 2022. It revolutionized natural- language processing and has achieved at least some of the goals broadly outlined in the proposal for the Dartmouth Workshop. To which extent is ChatGPT intelligent and creative? The answer to this question is in the eye of the beholder, but there can be no doubt that ChatGPT shows intelligence and creativity.
Why did it take nearly seventy years until computers could be programmed to use natural language at a level comparable to humans or even at a superhuman level? There are two complications.
The first is that any program that can usefully deal with natural language requires a world model or common sense, thus knowing how the world works and how the subjects and objects in a text may interact. This information must be conveyed to the program somehow. Common sense is also required to be able to deal with ambiguities.
The second complication is that natural language is full of references, some- In order to resolve references correctly, times over large distances in a text. context must be understood. A common example are personal pronouns that reference nouns.
LLM such as ChatGPT address the first complication by being training on huge, high-quality text corpora. This is an example of self-improvement: the learning algorithm extracts information (including grammar) from the text cor- pora and stores it in a new form, i.e., the neural network, for later use. No gram- matical rules or facts are programmed into an LLM directly. Self-improvement and the use of huge text corpora necessitate a probabilistic approach to learning and to generating textual output in order to deal with contradictions that are most likely present in large text corpora. The probabilistic approach to gener- ating output is linked to the so-called temperature parameter, which in turn is linked to creativity.
The second challenge is addressed by the attention mechanism of transform-
ers.
12.2. Transformers
12.2 Transformers
Transformers are a certain kind of deep neural network. Their defining feature is their sole use of the so-called attention mechanism, which makes it possible to learn references within a text or a time series, in a rather simple architecture. Transformers were introduced in [58] for use in machine translation in 2017. Before this publication, the dominant sequence transduction models for use in machine translation were recurrent or convolutional neural networks, but already used attention mechanisms. The advantages of transformers reported in [58] are superior quality, more parallelizable models, and shorter training times.
The following discussion of transformers is based on [58] and pertains mostly to the features that set them apart from other neural-network architectures, i.e., scaled dot-product attention, multi-head attention, and positional encoding. A simplified version of transformers for machine translation are the basis of ChatGPT, discussed later. Much of the design of neural-network architectures is empirical; after the success of transformers in machine translation, their use has expanded to many other application areas, and many variations have been proposed and implemented.
A schematic diagram of the original transformer architecture for machine translation is shown in Figure 12.1. We start with an overview, and go into the details later. The left part is the encoder stack, and the right part is the decoder stack. The input text in the source language is converted into a vector of so- called tokens by the input embedding. Positional encoding is added. The light gray box on the left side in the figure contains the encoder and consists of six identical layers. In each layer, there are two sublayers, i.e., multi-head attention and feed forward. At the end of each sublayer, the two indicated vectors are added, and their sum is normalized.
The input to the encoder is a window, i.e., a vector of tokens with fixed length, that slides along the whole input text. The length of this vector is called the model dimension and has the value ğ‘‘model = 512 in [58]. To facilitate all connections, all embeddings and sublayers have the same length, i.e., the model dimension.
In the first iteration, the vector containing the previous output in the tar- get language shown at the bottom right contains only padding. The decoder shown on the right is similar to the encoder shown on the left. Its layers are also repeated six times, but it contains three sublayers, i.e., masked multi-head attention, multi-head attention, and feed forward. The masking in the first sub- layers prevents positions from attending to subsequent positions, and hence the predictions only depend on known output tokens. The output of the encoder is passed to the second sublayers, i.e., the multi-head attention layers, of the six
137
138
Chapter 12. Large Language Models
Figure 12.1: Transformer architecture for machine translation. Source: Figure 1].
decoder layers, and conveys all meaning from the input to the output text.
Finally, a linear transformation is applied, and a softmax layer generates a single output token in a probabilistic manner depending on the temperature parameter ğ‘‡ by assigning the probability
softmax(ğ‘¥ğ‘–) âˆ¶=
ğ‘¥ğ‘–/ğ‘‡ e
âˆ‘ğ‘– e
ğ‘¥ğ‘–/ğ‘‡
to the ğ‘–-th element of the real-valued input vector x to the softmax layer.
[58,
12.2. Transformers
Figure 12.2: Scaled dot-product attention (left) and multi-head attention (right). Source: [58, Figure 2].
In this manner, a single token is generated in each iteration, which is an evalu- ation of the transformer, and appended to the output sequence. The transformer model is auto-regressive, as the previously generated tokens serve as additional input when generating the next token.
All textual inputs to a transformer are converted by embeddings to vectors of tokens, and the output of the transformer is another token. Why are tokens more useful than characters or words? Experience has shown that characters as the smallest textual units do not carry enough information. On the other hand, using words as the smallest units results in large vocabularies that contain tens of thousands of entries and is inflexible. Tokens are a useful compromise, and very good mental model of tokens are syllables.
The process of converting a text to tokens is called tokenization and is per- formed by a tokenizer. Each token has an ID, and the set of all tokens is called the vocabulary. A tokenizer can parse, i.e., convert a string to a list of char- acters; it can encode, i.e., convert a vector of tokens to their IDs; and it can decode, i.e., convert a vector of token IDs to a string.
Embeddings convert tokens to real valued vectors of dimension ğ‘‘model. This makes it possible, for example, that tokens of similar meaning or use are con- verted to points that are close in the real vector space. to In [58], the embeddings are learned linear transformation, although other choices exist [59].
Next, we turn our attention toâ€¦ attention. Figure 12.2 shows schematic
139
140
Chapter 12. Large Language Models
diagrams for scaled dot-product attention and multi-head attention.
In the mechanism of scaled dot-production attention, queries and keys of dimension ğ‘‘ğ‘˜ as well as values of dimension ğ‘‘ğ‘£ are used. In practice, the attention function works on a set of queries simultaneously, and hence the queries, keys, and values are stored in matrices ğ‘„, ğ¾, and ğ‘‰ . Scaled dot-product attention is defined as
attention(ğ‘„,ğ¾,ğ‘‰ ) âˆ¶= softmax(
ğ‘„ğ¾âŠ¤ âˆšğ‘‘ğ‘˜
)ğ‘‰ .
Elements of the inner product ğ‘„ğ¾âŠ¤ are large whenever queries and keys point into the same direction and vanish whenever they are orthogonal. Thus, when- ever the queries and keys are aligned, the values ğ‘‰ are the result of the attention, otherwise the attention is (close to) zero. Therefore this mechanism enables the learning of grammar. For example, queries and keys make it possible to match nouns and pronouns.
All operations in this definition are standard functions and can be imple- mented using highly optimized code. The scaling factor âˆšğ‘‘ğ‘˜ in the denomi- nator is important, because the inner product in the argument to the softmax function may become large in magnitude for large values of ğ‘‘ğ‘˜. The value of the scaling factor is motivated by the following fact. Suppose the elements of the two vectors q âˆˆ â„ğ‘‘ğ‘˜ and k âˆˆ â„ğ‘‘ğ‘˜ are independent random variables with expectation 0 and variance 1. Then their inner product q â‹… k has mean 0 and variance ğ‘‘ğ‘˜, which means that after scaling by 1/âˆšğ‘‘ğ‘˜ the variance of the inner product is 1.
In the mechanism of multi-head attention [58], the ğ‘‘model-dimensional keys, values, and queries are projected â„ times using learned linear projections to ğ‘‘ğ‘˜, ğ‘‘ğ‘˜, and ğ‘‘ğ‘£ dimensions, respectively. The attention function is applied to the projected versions in parallel, yielding ğ‘‘ğ‘£-dimensional vectors. These are concatenated and projected again, i.e.,
headğ‘– âˆ¶= attention(ğ‘„ğ‘Š ğ‘„ ,ğ‘‰ ğ‘Š ğ‘‰ multiHead(ğ‘„,ğ¾,ğ‘‰ ) âˆ¶= concat(head1,â€¦,headâ„)ğ‘Š ğ‘‚
,ğ¾ğ‘Š ğ¾
ğ‘–
ğ‘–
ğ‘– ) âˆˆ â„ğ‘‘ğ‘£,
with the parameter matrices ğ‘Š ğ‘„ ğ‘– âˆˆ â„ğ‘‘modelÃ—ğ‘‘ğ‘£, and ğ‘Š ğ‘‚ ğ‘– âˆˆ â„â„ğ‘‘ğ‘£Ã—ğ‘‘model. In [58], â„ = 8 and ğ‘‘ğ‘˜ = ğ‘‘ğ‘£ = ğ‘‘model/â„ = 64 was used. Since each of the multiple heads is smaller, the total computational cost is similar to that of single-head attention with full dimensionality.
ğ‘– âˆˆ â„ğ‘‘modelÃ—ğ‘‘ğ‘˜, ğ‘Š ğ¾
ğ‘– âˆˆ â„ğ‘‘modelÃ—ğ‘‘ğ‘˜, ğ‘Š ğ‘‰
The advantage of multi-head attention is that the model can pay attention to information from different representation subspaces at different positions. This is inhibited by averaging if a single attention head is used.
12.3.
InstructGPT and ChatGPT
There are three applications of attention in [58]. Firstly, in the encoder- decoder attention layers, the queries come from the previous decoder layer, while the keys and the values come the output of the encoder. In this manner, all positions in the decoder can attend to all positions in the input sequence.
Secondly, the attention layers in the encoder are self-attention layers. In the encoder, all keys, values, and queries come from the output of the previous encoder layer. Hence, all positions in the encoder can attend to all positions in the previous layer in the encoder.
Thirdly, analogously to the attention layers in the encoder, the attention layers in the decoder are also self-attention layers, but with the slight difference that all positions in the decoder attend to all positions in the decoder up to and including that position. Otherwise, information could flow leftward and the auto-regressive property would be violated.
Finally, we discuss how positional information is encoded in transformers [58]. It is advantageous for the neural network to be able to use the order of the sequence and to know the distances between tokens. Such information about the relative and absolute positions of tokens is made available by positional encoding, which is added to the input embeddings at the bottoms of the encoder and decoder stacks. Both the embeddings and the positional embeddings have dimension ğ‘‘model and are added.
Many approaches to positional embeddings exist. In [58], the sine and cosine
functions
PE(ğ‘,2ğ‘–) âˆ¶= sin(ğ‘/100002ğ‘–/ğ‘‘model), PE(ğ‘,2ğ‘– + 1) âˆ¶= cos(ğ‘/100002ğ‘–/ğ‘‘model)
of different frequencies, where ğ‘ is the position and ğ‘– the dimension, were used. The wavelengths are a geometric progression from 2ğœ‹ to 10000 â‹… 2ğœ‹.
Such a positional encoding has two advantages. Firstly, all values of PE are between âˆ’1 and +1, which is advantageous for training neural networks. Secondly, attention to relative positions is easily learned, since PE(ğ‘ + ğ‘˜,ğ‘–) is a linear function of PE(ğ‘,ğ‘–) for any fixed ğ‘˜.
The authors compared this sinusoidal positional encoding with learned posi- tional encodings, finding that both yielded nearly identical results, and eventu- ally chose the sinusoidal positional encoding, because its behavior for sequence lengths longer than the ones encountered during training is more predictable.
12.3 InstructGPT and ChatGPT
G: generative, P: pretrained, T: transformer.
141
142
Chapter 12. Large Language Models
Figure 12.3: The three steps used in training InstructGPT [60, Figure 2].
The predecessor of ChatGPT is called InstructGPT [60]. A model is called aligned if it is
helpful,
honest, and
harmless.
Figure 12.3 shows the three steps used in training InstructGPT. Table 12.1 shows the criteria given to the labelers.
12.4 Proximal Policy Optimization (PPO)
The third step in training InstructGPT and ChatGPT uses reinforcement learn- ing based on the reward model found in the second step (see Figure 12.3). More precisely, proximal policy optimization (PPO), which is a policy-gradient method, is used.
Whenever local optima of smooth functions are sought, the gradient of the objective function is instrumental in devising efficient optimization algorithms [61, Chapter 12].
12.4. Proximal Policy Optimization (PPO)
Metadata Overall quality Fails to follow the correct instruction/task Inappropriate for customer assistant Hallucination Satisfies constraint provided in the instruction Contains sexual content Contains violent content Encourages or fails to discourage
Scale Likert scale, 1â€“7 Binary Binary Binary Binary Binary Binary
violence/abuse/terrorism/self-harm
Denigrates a protected class Gives harmful advice Expresses opinion Expresses moral judgment
Binary Binary Binary Binary Binary
Table 12.1: The criteria the human labelers used to judge model output [60, Table 3].
In reinforcement learning, gradient methods can be applied to the action- value function or to the policy directly or to both. Policy-gradient methods are based on the policy-gradient theorem (see Section 8.3).
PPO is a family of policy-gradient methods[62]. The algorithms alternate between sampling epochs of minibatches from the environment and optimizing a surrogate objective function using stochastic gradient ascent. According to [62], PPO methods share some of the benefits of trust-region policy optimiza- tion (TRPO), are simpler to implement, and empirically have better sample complexity.
Policy-gradient methods maximize the performance
ğ½PG(ğœƒ) = Ì‚ğ”¼ğ‘¡[logğœ‹ğœƒ(ğ‘ğ‘¡|ğ‘ ğ‘¡) Ì‚ğ´ğ‘¡],
where Ì‚ğ´ğ‘¡(ğ‘ ,ğ‘) is the estimate of the advantage function
ğ´ğ‘¡(ğ‘ ,ğ‘) âˆ¶= ğ‘„ğ‘¡(ğ‘ ,ğ‘) âˆ’ ğ‘‰ğ‘¡(ğ‘ ).
The approximation Ì‚ğ”¼ of the expectation is the sample mean based on a finite batch of samples. This objective function, i.e., the performance, leads to the estimate
Ì‚ğ‘”(ğœƒ) = Ì‚ğ”¼ğ‘¡[âˆ‡ğœƒ logğœ‹ğœƒ(ğ‘ğ‘¡|ğ‘ ğ‘¡) Ì‚ğ´ğ‘¡],
of the gradient.
143
144
Chapter 12. Large Language Models
It is not well-justified to perform multiple optimization steps on the perfor- mance ğ½PG using the same trajectory and empirically it often leads to destruc- tively large updates [62].
TRPO is a trust-region method [63]. It uses the surrogate objective function
maximizeğœƒ
Ì‚ğ”¼ğ‘¡ [
subject to
Ì‚ğ”¼ğ‘¡[KL[ğœ‹ğœƒold
ğœ‹ğœƒ(ğ‘ğ‘¡|ğ‘ ğ‘¡) (ğ‘ğ‘¡|ğ‘ ğ‘¡) ğœ‹ğœƒold (â‹…|ğ‘ ğ‘¡),ğœ‹ğœƒ(â‹…|ğ‘ ğ‘¡)]] â‰¤ ğ›¿,
Ì‚ğ´ğ‘¡]
which is maximized subject to the constraint on the size of the update, i.e., the difference between the old and new policies. Here KL denotes the Kullback- Leibler divergence
KL(ğ‘ƒ,ğ‘„) âˆ¶= âˆ«ğ‘(ğ‘¥)log(
ğ‘(ğ‘¥) ğ‘(ğ‘¥)
)dğ‘¥
of two continuous random variables ğ‘ƒ and ğ‘„, where ğ‘ and ğ‘ are their densities. Alternatively, instead of a constraint, a penalty can be used, which results
in the unconstrained optimization problem
maximizeğœƒ
Ì‚ğ”¼ğ‘¡ [
ğœ‹ğœƒ(ğ‘ğ‘¡|ğ‘ ğ‘¡) (ğ‘ğ‘¡|ğ‘ ğ‘¡) ğœ‹ğœƒold
Ì‚ğ´ğ‘¡ âˆ’ ğ›½ KL[ğœ‹ğœƒold
(â‹…|ğ‘ ğ‘¡),ğœ‹ğœƒ(â‹…|ğ‘ ğ‘¡)]],
(12.1)
where ğ›½ is a non-negative coefficient. Unfortunately, reasonable choices of ğ›½ vary over the course of learning and among different learning problems.
Next, we define the clipped surrogate objective [62]. We note the probability
ratio between the old and the new policies by
ğ‘Ÿğ‘¡(ğœƒ) âˆ¶=
ğœ‹ğœƒ(ğ‘ğ‘¡|ğ‘ ğ‘¡) (ğ‘ğ‘¡|ğ‘ ğ‘¡) ğœ‹ğœƒold
.
TRPO maximizes the objective
ğ½CPI(ğœƒ) âˆ¶= Ì‚ğ”¼ğ‘¡ [
ğœ‹ğœƒ(ğ‘ğ‘¡|ğ‘ ğ‘¡) (ğ‘ğ‘¡|ğ‘ ğ‘¡) ğœ‹ğœƒold
Ì‚ğ´ğ‘¡] = Ì‚ğ”¼ğ‘¡[ğ‘Ÿğ‘¡(ğœƒ) Ì‚ğ´ğ‘¡].
The index refers to conservative policy iteration1.
1[S. Kakade and J. Langford. â€œApproximately optimal approximate reinforcement learningâ€.
In: ICML. Vol. 2. 2002, pp. 267â€“274]
12.4. Proximal Policy Optimization (PPO)
In order to limit the size of policy updates, the objective function is now modified such that changes in ğ‘Ÿğ‘¡(ğœƒ) away from ğ‘Ÿ(ğœƒold) = 1 are penalized. In [62], the performance
ğ½CLIP(ğœƒ) âˆ¶= Ì‚ğ¸ğ‘¡ [min(ğ‘Ÿğ‘¡(ğœƒ) Ì‚ğ´ğ‘¡,clip(ğ‘Ÿğ‘¡(ğœƒ),1 âˆ’ ğœ–,1 + ğœ–) Ì‚ğ´ğ‘¡)] is hence proposed as the main objective function. The clip function ensures that the value ğ‘Ÿğ‘¡(ğœƒ) remains in the interval [1 âˆ’ ğœ–,1 + ğœ–]. Taking the minimum of the clipped and unclipped values ensures that the performance is a lower or pessimistic bound on the unclipped value. In other words, changes in the probability ratio ğ‘Ÿğ‘¡(ğœƒ) are included when they make the objective worse and are ignored when they would improve the objective.
Next, we discuss how the hyperparameter ğ›½ in the penalized, unconstrained optimization problem (12.1) can be adapted dynamically [62]. The main idea is to adapt the penalty coefficient ğ›½ so that a prescribed target value ğ‘‘targ of the KL divergence is achieved.
We denote the expectation in (12.1) by ğ½KLPEN(ğœƒ). It is maximized using
several epochs of minibatches. Then the difference
ğ‘‘ âˆ¶= Ì‚ğ”¼ğ‘¡[KL[ğœ‹ğœƒold
(â‹…|ğ‘ ğ‘¡),ğœ‹ğœƒ(â‹…|ğ‘ ğ‘¡)]]
between the old and new policies is computed, and the parameter ğ›½ may be updated according to its value: if ğ‘‘ < ğ‘‘targ/1.5, then the updated penalty parameter is ğ›½ â† ğ›½/2; if ğ‘‘ > ğ‘‘targ â‹… 1.5, then it becomes ğ›½ â† ğ›½ â‹… 2. The new value of ğ›½ is used for the next policy updated, and so on. The parameters 1.5 and 2 are chosen heuristically.
It was found in [62] that ğ½KLPEN performs worse than ğ½CLIP. Finally, we can formulate the main PPO algorithm. It combines three terms
in its performance
ğ½CLIP+VS+S(ğœƒ) âˆ¶= Ì‚ğ”¼ğ‘¡ [ğ½CLIP âˆ’ ğ‘1ğ½VF(ğœƒ) + ğ‘2ğ‘†[ğœ‹ğœƒ](ğ‘ ğ‘¡)] to be maximized, where ğ‘1 and ğ‘2 are non-negative coefficients. The second term is the squared-error loss
ğ½VF âˆ¶= (ğ‘‰ğœƒ(ğ‘ ğ‘¡) âˆ’ ğ‘‰ target
ğ‘¡
)
2
,
and the third term ğ‘† is the entropy. The entropy is added as a reward in order to ensure sufficient exploration.
The policy-gradient implementation is often based on segments of episodes 2, which is also well-suited for use with recurrent neural networks. The policy
2[V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. P. Lillicrap, T. Harley, D. Silver, and K. In: arXiv preprint
Kavukcuoglu. â€œAsynchronous methods for deep reinforcement learningâ€. arXiv:1602.01783 (2016).]
145
146
Chapter 12. Large Language Models
is run for ğ‘‡ time steps, where ğ‘‡ is much less than the episode length, and this truncated segment of an episode is used as the sample for the policy-gradient update. This approach means that the estimator of the advantage function only uses ğ‘‡ time steps. A very general choice is
Ì‚ğ´ğ‘¡ âˆ¶= ğ›¿ğ‘¡ + (ğ›¾ğœ†)ğ›¿ğ‘¡+1 + â€¦ + (ğ›¾ğœ†)ğ‘‡âˆ’ğ‘¡+1ğ›¿ğ‘‡âˆ’1, ğ›¿ğ‘¡ âˆ¶= ğ‘Ÿğ‘¡ + ğ›¾ğ‘‰ (ğ‘ ğ‘¡+1) âˆ’ ğ‘‰ (ğ‘ ğ‘¡).
The proximal policy optimization (PPO) algorithm is shown in Algorithm 20.
Algorithm 20 Proximal policy optimization (PPO) for policy optimization [62].
loop iterations
for all actors from 1 to ğ‘ do
run policy ğœ‹ğœƒold compute advantage estimates Ì‚ğ´1,â€¦, Ì‚ğ´ğ‘‡
for ğ‘‡ time steps
end for optimize surrogate performance/objective ğ½CLIP+VS+S w.r.t. ğœƒ
using ğ¾ epochs and minibatch size ğ‘€ â‰¤ ğ‘ğ‘‡
ğœƒold âˆ¶= ğœƒ
end loop
12.5 Bibliographical and Historical Remarks
Transformers were introduced in [58]. Proximal policy optimization (PPO) was introduced in [62]. InstructGPT was introduced in [60].
12.6 Problems
1. Number of letters. Ask one or preferably more LLM how many letters some words contain and compare the results.
2. Implement a character based tokenizer.
3. Implement an embedding layer.
Appendix A
Analysis
This chapter contains advanced definitions and results from analysis.
A.1 The Riemann-Stieltjes Integral
The Riemann-Stieltjes integral is a generalization of the Riemann integral and a precursor of the Lebesgue integral. It is very useful to uniformly formulate statistical formulae and theorems that apply both to discrete and continuous probabilities.
Definition A.1 (partition (of a real interval)). A partition of a set ğ‘‹ is a set of subsets ğ‘† âŠ‚ ğ‘‹ of ğ‘‹ such that each subset ğ‘† â‰  âˆ… is non-empty and that every element ğ‘¥ âˆˆ ğ‘‹ is in exactly one of these subsets.
If the set ğ‘‹ to be partitioned into a partition ğ‘ƒ is a real interval [ğ‘,ğ‘] âŠ‚ â„,
a set of points ğ‘¥ğ‘› âˆˆ [ğ‘,ğ‘], 0 â‰¤ ğ‘› â‰¤ ğ‘(ğ‘ƒ), such that
ğ‘ = ğ‘¥0 < ğ‘¥1 < â‹¯ < ğ‘¥ğ‘(ğ‘ƒ) = ğ‘
gives rise to
ğ‘ƒ = {[ğ‘¥0,ğ‘¥1),[ğ‘¥1,ğ‘¥2),[ğ‘¥2,ğ‘¥3),â€¦,[ğ‘¥ğ‘(ğ‘ƒ)âˆ’1,ğ‘¥ğ‘(ğ‘ƒ)]},
the partition of a real interval. The number of points is ğ‘(ğ‘ƒ)+1, and the mesh |ğ‘ƒ| of a partition ğ‘ƒ of this form is defined to be the length
|ğ‘ƒ| âˆ¶=
ğ‘(ğ‘ƒ) max ğ‘›=1
|ğ‘¥ğ‘› âˆ’ ğ‘¥ğ‘›âˆ’1|
of its longest subinterval.
147
148
Appendix A. Analysis
Definition A.2 (total variation). Suppose [ğ‘,ğ‘] âŠ‚ â„ and ğ‘“ âˆ¶ [ğ‘,ğ‘] â†’ â„ is a real valued function. Then the quantity
ğ‘‰ ğ‘ ğ‘ (ğ‘“) âˆ¶= sup ğ‘ƒâˆˆğ’«
ğ‘(ğ‘ƒ) âˆ‘ ğ‘›=1
|ğ‘“(ğ‘¥ğ‘›) âˆ’ ğ‘“(ğ‘¥ğ‘›âˆ’1)|
is called the total variation of ğ‘“, where the supremum is taken over all parti- tions ğ’« of the real interval [ğ‘,ğ‘].
Definition A.3 (bounded variation). Suppose [ğ‘,ğ‘] âŠ‚ â„ and ğ‘“ âˆ¶ [ğ‘,ğ‘] â†’ â„ is a real valued function. The function ğ‘“ is said to be of bounded variation, i.e., ğ‘“ âˆˆ BV([ğ‘,ğ‘]), if its total variation is finite, i.e., ğ‘‰ ğ‘
ğ‘ (ğ‘“) < âˆ.
It can be shown that if a real valued function ğ‘“ âˆ¶ [ğ‘,ğ‘] â†’ â„ is differentiable and its derivative ğ‘“â€² is Riemann-integrable, then its total variation is given by
ğ‘
ğ‘‰ ğ‘ ğ‘ (ğ‘“) = âˆ« ğ‘
|ğ‘“â€²(ğ‘¥)|dğ‘¥,
which is the vertical component of the arc length of its graph.
Furthermore, the Jordan decomposition of a function means that a real func- tion ğ‘“ âˆ¶ â„ â†’ â„ is of bounded variation in [ğ‘,ğ‘] if and only if it can be written as the difference ğ‘“ = ğ‘“1 âˆ’ ğ‘“2 of two non-decreasing functions real functions ğ‘“1 and ğ‘“2 on [ğ‘,ğ‘]. Definition A.4 (Riemann-Stieltjes integral). Suppose a partition ğ‘ƒ of the real interval [ğ‘,ğ‘] âŠ‚ â„ is given by the points {ğ‘¥0,â€¦,ğ‘¥ğ‘(ğ‘ƒ)}. Then the approximating sum ğ‘† of a Riemann-Stieltjes integral is defined as
ğ‘†(ğ‘ƒ,ğ‘“,ğ‘”) âˆ¶=
ğ‘(ğ‘ƒ) âˆ‘ ğ‘›=1
ğ‘“(ğ‘ğ‘›)(ğ‘”(ğ‘¥ğ‘›) âˆ’ ğ‘”(ğ‘¥ğ‘›âˆ’1)),
where ğ‘ğ‘› âˆˆ [ğ‘¥ğ‘›âˆ’1,ğ‘¥ğ‘›] for all ğ‘› âˆˆ {1,â€¦,ğ‘(ğ‘ƒ)}, and the limit
ğ‘ âˆ«
ğ‘¥=ğ‘
ğ‘“(ğ‘¥)dğ‘”(ğ‘¥) âˆ¶= lim |ğ‘ƒ|â†’0
ğ‘†(ğ‘ƒ,ğ‘“,ğ‘”),
if it exists, is called the Riemann-Stieltjes integral of the integrand ğ‘“ âˆ¶ â„ â†’ â„ on the interval [ğ‘,ğ‘] with respect to the integrator ğ‘”âˆ¶ â„ â†’ â„, which is assumed to be of bounded variation. The limit is defined to be equal to ğ‘† âˆˆ â„, if for every ğœ– âˆˆ â„+ there exists a ğ›¿ âˆˆ â„+ such that for every partition ğ‘ƒ with mesh |ğ‘ƒ| < ğ›¿ and for every choice of points ğ‘ğ‘› âˆˆ [ğ‘¥ğ‘›âˆ’1,ğ‘¥ğ‘›], the inequality
|ğ‘†(ğ‘ƒ,ğ‘“,ğ‘”) âˆ’ ğ‘†| < ğœ–
holds.
A.1. The Riemann-Stieltjes Integral
Theorem A.5 (integration by parts). Suppose the real valued functions ğ‘“ âˆ¶ â„ â†’ â„ and ğ‘”âˆ¶ â„ â†’ â„ are of bounded variation. Then the existence of either Riemann- Stieltjes integral âˆ« ğ‘”(ğ‘¥)dğ‘“(ğ‘¥) implies the existence of the other. If the integrals exist, then the formula
ğ‘
ğ‘ ğ‘“(ğ‘¥)dğ‘”(ğ‘¥) or âˆ«
ğ‘¥=ğ‘
ğ‘¥=ğ‘
ğ‘
ğ‘
âˆ«
ğ‘“(ğ‘¥)dğ‘”(ğ‘¥) = ğ‘“(ğ‘)ğ‘”(ğ‘) âˆ’ ğ‘“(ğ‘)ğ‘”(ğ‘) âˆ’ âˆ«
ğ‘”(ğ‘¥)dğ‘“(ğ‘¥),
ğ‘¥=ğ‘
ğ‘¥=ğ‘
called integration by parts, holds.
If the integrator ğ‘” is smooth enough, then the Riemann-Stieltjes integral
reduces to a Riemann integral.
Theorem A.6 (relation to Riemann integral). Suppose ğ‘“ âˆ¶ â„ â†’ â„ is bounded on the interval [ğ‘,ğ‘] âˆˆ â„, ğ‘”âˆ¶ â„ â†’ â„ increases monotonically, and ğ‘”â€² is Riemann- integrable. Then the Riemann-Stieltjes integral of ğ‘“ with respect to ğ‘” is given by the Riemann integral of ğ‘“ğ‘”â€², i.e.,
ğ‘ âˆ«
ğ‘“(ğ‘¥)dğ‘”(ğ‘¥) = âˆ«
ğ‘
ğ‘“(ğ‘¥)ğ‘”â€²(ğ‘¥)dğ‘¥.
ğ‘¥=ğ‘
ğ‘¥=ğ‘
Suppose the integrator is a unit-step function. Then the Riemann-Stieltjes integral of an integrand (that is continuous at the step) with respect to the step function equals the integrand evaluated at the unit step.
Theorem A.7 (unit-step function as integrator). Suppose ğ‘“ âˆ¶ â„ â†’ â„ is contin- uous at ğœ‰ âˆˆ (ğ‘,ğ‘) âŠ‚ â„ and ğ»ğœ‰ is the unit-step function
ğ»ğœ‰âˆ¶ â„ â†’ â„, ğ»ğœ‰(ğ‘¥) âˆ¶= {
0, ğ‘¥ < ğœ‰, 1, ğ‘¥ â‰¥ ğœ‰.
Then the Riemann-Stieltjes integral of ğ‘“ with respect to ğ»ğœ‰ is
ğ‘ âˆ«
ğ‘“(ğ‘¥)dğ»ğœ‰(ğ‘¥) = ğ‘“(ğœ‰).
ğ‘¥=ğ‘
More generally, the following theorem holds for piecewise constant functions
as integrators.
Definition A.8 (piecewise constant function). A piecewise constant function ğ‘”âˆ¶ â„ â†’ â„ is a function that has finitely many points of discontinuity ğ‘¥1 < â‹¯ < ğ‘¥ğ‘, ğ‘ âˆˆ â„•, and that is constant on the intervals (âˆ’âˆ,ğ‘¥1), (ğ‘¥1,ğ‘¥2), â€¦, (ğ‘¥ğ‘âˆ’1,ğ‘¥ğ‘), (ğ‘¥ğ‘,âˆ).
149
150
Appendix A. Analysis
In this definition, the values of ğ‘” at the points ğ‘¥ğ‘› of discontinuity are arbi-
trary and not necessarily equal to ğ‘”(ğ‘¥ğ‘›âˆ’) or ğ‘”(ğ‘¥ğ‘›+). We write
ğ‘”(ğœ‰âˆ’) âˆ¶= lim
ğ‘”(ğ‘¥),
ğ‘¥âˆˆ(âˆ’âˆ,ğœ‰), ğ‘¥â†’ğœ‰
ğ‘”(ğœ‰+) âˆ¶= lim
ğ‘”(ğ‘¥).
ğ‘¥âˆˆ(ğœ‰,+âˆ), ğ‘¥â†’ğœ‰
Theorem A.9 (piecewise constant function as integrator). Suppose ğ‘”âˆ¶ â„ â†’ â„ is a piecewise constant function, and ğ‘“ âˆ¶ â„ â†’ â„ is continuous at the points of discontinuity ğ‘¥1 < â‹¯ < ğ‘¥ğ‘, ğ‘ âˆˆ â„•. Then the Riemann-Stieltjes integral of ğ‘“ with respect to ğ‘” is
âˆ
âˆ«
ğ‘¥=âˆ’âˆ
ğ‘“(ğ‘¥)dğ‘”(ğ‘¥) =
ğ‘ âˆ‘ ğ‘›=1
ğ‘“(ğ‘¥ğ‘›)(ğ‘”(ğ‘¥ğ‘›+) âˆ’ ğ‘”(ğ‘¥ğ‘›âˆ’)).
Integrators that are piecewise constant functions are the link between con- tinuous and discrete random variables and probability distributions. Here we consider the expectation as the leading example of an integral (in the case of a continuous probability distribution) or a summation (in the case of a discrete probability distribution), but the principle applies in general. The Riemann- Stieltjes integral provides a unified point of view of the integrals arising in the continuous case and the summations arising in the discrete case, where the dis- crete case becomes a special case of the continuous one.
Suppose ğ¹ğ‘‹ is the cumulative probability distribution of a random vari- able ğ‘‹. As such, ğ¹ğ‘‹ is of bounded variation, which means that it can always serve as the integrator of a Riemann-Stieltjes integral. By definition, the expec- tation of â„(ğ‘‹) is
âˆ
ğ”¼[â„(ğ‘‹)] âˆ¶= âˆ«
â„(ğ‘¥)dğ¹ğ‘‹(ğ‘¥).
ğ‘¥=âˆ’âˆ
If the cumulative probability distribution ğ¹ğ‘‹ is continuous differentiable, the
corresponding probability density function is ğ‘“ğ‘‹ âˆ¶= ğ¹ â€²
ğ‘‹, and we have
âˆ
ğ”¼[â„(ğ‘‹)] = âˆ«
â„(ğ‘¥)ğ‘“ğ‘‹(ğ‘¥)dğ‘¥,
ğ‘¥=âˆ’âˆ
which can be viewed as a Riemann or a Lebesgue integral and is often used as the definition of the expectation.
However, if the random variable ğ‘‹ does not have a probability density func- tion with respect to the Lebesgue measure, this formula does not hold. The
A.2. The Banach Fixed-Point Theorem
leading examples are discrete random variables, i.e., random variables whose probabilities are assigned to a finite number of real points.
When using Riemann-Stieltjes integrals, how do discrete random variables and distributions become special cases of continuous ones? Suppose that the sample space is the finite set Î© âˆ¶= {ğ‘¥1,â€¦,ğ‘¥ğ‘}, ğ‘ âˆˆ â„•, such that ğ‘¥1 < â‹¯ < ğ‘¥ğ‘, and that the probabilities of the outcomes of the random variable ğ‘‹ are
â„™[ğ‘‹ = ğ‘¥ğ‘›] âˆ¶= ğ‘ğ‘› âˆˆ [0,1],
ğ‘› âˆˆ {1,â€¦,ğ‘},
such that âˆ‘ğ‘
ğ‘›=1 ğ‘ğ‘› = 1. The cumulative distribution function of ğ‘‹ is
ğ¹ğ‘‹(ğ‘¥) âˆ¶= â„™[ğ‘‹ â‰¤ ğ‘¥] =
â§ { â¨ { â©
0, âˆ‘ğ‘› 1,
ğ‘¥ < ğ‘¥1, ğ‘–=1 ğ‘ğ‘–, ğ‘¥ğ‘› â‰¤ ğ‘¥ < ğ‘¥ğ‘›+1, ğ‘¥ğ‘ â‰¤ ğ‘¥,
where we have only used the general definition of a cumulative distribution function. We consider the expectation of â„(ğ‘‹) as a leading example of an integral and find
âˆ
ğ”¼[â„(ğ‘‹)] âˆ¶= âˆ«
â„(ğ‘¥)dğ¹ğ‘‹(ğ‘¥)
ğ‘¥=âˆ’âˆ
=
ğ‘ âˆ‘ ğ‘›=1
â„(ğ‘¥ğ‘›)(ğ¹ğ‘‹(ğ‘¥ğ‘›+) âˆ’ ğ¹ğ‘‹(ğ‘¥ğ‘›âˆ’))
=
ğ‘ âˆ‘ ğ‘›=1
â„(ğ‘¥ğ‘›)ğ‘ğ‘›
by Theorem A.9. The last expression is commonly used as the special definition of the expectation in the discrete case, but using the Riemann-Stieltjes integral it immediately follows from the general definition.
A.2 The Banach Fixed-Point Theorem
Definition A.10 (contraction, Lipschitz constant). Suppose (ğ‘‹,ğ‘‘) is a metric space. A map ğ‘‡ âˆ¶ ğ‘‹ â†’ ğ‘‹ is called a contraction on ğ‘‹ if
âˆƒğœ† âˆˆ [0,1)âˆ¶
âˆ€(ğ‘¥,ğ‘¦) âˆˆ ğ‘‹ Ã— ğ‘‹âˆ¶
ğ‘‘(ğ‘‡(ğ‘¥),ğ‘‡(ğ‘¦) â‰¤ ğœ†ğ‘‘(ğ‘¥,ğ‘¦).
Any such value of ğœ† is called a Lipschitz constant for ğ‘‡.
Clearly, every contraction is a continuous function.
151
152
Appendix A. Analysis
Theorem A.11 (Banach fixed-point theorem). Suppose (ğ‘‹,ğ‘‘) is a non-empty complete metric space and ğ‘‡ âˆ¶ ğ‘‹ â†’ ğ‘‹ is a contraction with constant ğœ†. Then ğ‘‡ has a unique fixed point ğ‘¥âˆ—, i.e.,
ğ‘¥âˆ— = ğ‘‡(ğ‘¥âˆ—).
Furthermore, define a sequence âŸ¨ğ‘¥ğ‘›âŸ©ğ‘›âˆˆâ„• by
ğ‘¥ğ‘›+1 âˆ¶= ğ‘‡(ğ‘¥ğ‘›),
ğ‘› âˆˆ â„•,
and ğ‘¥0 âˆˆ ğ‘‹ chosen arbitrarily. Then this sequence converges to the unique fixed point, i.e.,
lim ğ‘›â†’âˆ
ğ‘¥ğ‘› = ğ‘¥âˆ—.
Furthermore, the inequalities
ğ‘‘(ğ‘¥âˆ—,ğ‘¥ğ‘›+1) â‰¤ ğœ†ğ‘‘(ğ‘¥âˆ—,ğ‘¥ğ‘›)
âˆ€ğ‘› âˆˆ â„•,
ğ‘‘(ğ‘¥âˆ—,ğ‘¥ğ‘›) â‰¤
ğ‘‘(ğ‘¥âˆ—,ğ‘¥ğ‘›+1) â‰¤
ğœ†ğ‘› 1 âˆ’ ğœ† ğœ† 1 âˆ’ ğœ†
ğ‘‘(ğ‘¥1,ğ‘¥0)
ğ‘‘(ğ‘¥ğ‘›+1,ğ‘¥ğ‘›)
âˆ€ğ‘› âˆˆ â„•,
âˆ€ğ‘› âˆˆ â„•
hold.
The following generalizations can be shown.
Theorem A.12 (generalizations). Suppose (ğ‘‹,ğ‘‘) is a non-empty complete met- ric space and ğ‘‡ âˆ¶ ğ‘‹ â†’ ğ‘‹ is a function.
1. Suppose that an iterate ğ‘‡ ğ‘› of ğ‘‡ is a contraction. Then ğ‘‡ has a unique fixed point.
2. Suppose that for all ğ‘› âˆˆ â„• there exists a ğ‘ğ‘› âˆˆ â„ such that ğ‘‘(ğ‘‡ ğ‘›(ğ‘¥),ğ‘‡ ğ‘›(ğ‘¦)) â‰¤ ğ‘ğ‘›ğ‘‘(ğ‘¥,ğ‘¦) for all ğ‘¥ âˆˆ ğ‘‹ and ğ‘¦ âˆˆ ğ‘‹ and that âˆ‘ğ‘›âˆˆâ„• ğ‘ğ‘› < âˆ. Then ğ‘‡ has a unique fixed point.
3. Suppose ğ‘‘(ğ‘‡(ğ‘¥),ğ‘‡(ğ‘¦)) < ğ‘‘(ğ‘¥,ğ‘¦) for all distinct ğ‘¥ âˆˆ ğ‘‹ and ğ‘¦ âˆˆ ğ‘‹. Then ğ‘‡ has a unique fixed point.
A.3 Exercises
Exercise A.1. Show Theorem A.5.
Exercise A.2. Show Theorem A.6.
A.3. Exercises
Exercise A.3. Show Theorem A.7.
Exercise A.4. Show Theorem A.9.
Exercise A.5. Define all terms used in Section A.2.
Exercise A.6. Prove Theorem A.11.
Exercise A.7 (generalizations). Prove Theorem A.12.
153
154
Appendix A. Analysis
Appendix B
Measure and Probability Theory
This chapter contains definitions and results that serve as the interface between measure and probability theory on the one hand and RL on the other hand.
B.1 Notation
The following notation is used throughout the book. The first definition concerns ranges of integers.
Definition B.1 (range). The set {ğ‘š,â€¦,ğ‘›} of all integers between ğ‘š and ğ‘› is denoted by [ğ‘šâˆ¶ğ‘›].
The second notation assigns one and zero to true and false statements, re-
spectively.
Definition B.2 (Iverson bracket). The Iverson bracket of a statement is defined as
âŸ¦statementâŸ§ âˆ¶= {
1 0
if statement is true, if statement is false.
B.2 Measures and Measure Spaces
In measure and probability theory, it is often convenient to augment the real numbers by +âˆ = âˆ and âˆ’âˆ. In order to save space, the arithmetic operations on and the algebraic properties of the extended real numbers are not discussed here.
155
156
Appendix B. Measure and Probability Theory
Definition B.3 (extended real numbers). The extended real numbers are the set â„ âˆª {âˆ’âˆ,+âˆ} = [âˆ’âˆ,+âˆ].
The concept of a ğœ-algebra is fundamental for the following definitions. In the following, sets of sets and ğœ-algebras in particular are denoted by calligraphic letters.
Definition B.4 (ğœ-algebra). Suppose Î© is a non-empty set. Then a subset â„± âŠ‚ ğ’«(Î©) of its power set ğ’«(Î©) is called a ğœ-algebra over the universal set Î© if it satisfies the following properties:
1. The set â„± contains the universal set Î©, i.e., Î© âˆˆ â„±.
2. The set â„± is closed under complements, i.e., if ğ´ âˆˆ â„±, then also Î©âˆ–ğ´ âˆˆ â„±.
3. The set â„± is closed under countable unions, i.e., if ğ´ğ‘– âˆˆ â„± for all ğ‘– âˆˆ â„•, ğ‘–=1 ğ´ğ‘– âˆˆ â„±.
Since the ğœ-algebra â„± is closed under complements and countable unions, â„± is also closed under countable intersections, i.e., if ğ´ğ‘– âˆˆ â„± for all ğ‘– âˆˆ â„•, then also â‹‚
âˆ ğ‘–=1 ğ´ğ‘– âˆˆ â„±, by De Morganâ€™s law.
Given a universal set Î©, the set {âˆ…,Î©}, also called the trivial ğœ-algebra, is the smallest possible ğœ-algebra. The power set ğ’«(Î©) is the largest possible ğœ-algebra over Î©. The smallest ğœ-algebra that contains a subset ğ´ âŠ‚ Î© is {âˆ…,ğ´,Î©âˆ–ğ´,Î©}. The most common used ğœ-algebra over the real numbers is the Borel ğœ- algebra over the real numbers. In order to define Borel ğœ-algebras, we need the definitions of a topological space and a ğœ-operator.
Definition B.5 (topological space, topology, open set). A topological space is a pair (Î©,ğ’ª) where Î© is a set and the topology ğ’ª is a set of subsets of Î©, called the open sets, that satisfy the following properties:
1. The empty set and the set Î© are elements of the topology ğ’ª, i.e., âˆ… âˆˆ ğ’ª and Î© âˆˆ ğ’ª.
2. Any (finite or infinite) union of elements of the topology ğ’ª is an element of the topology ğ’ª, i.e., any (finite or infinite) union of open sets is again an open set.
3. The intersection of any finite number of elements of the topology ğ’ª is an element of the topology ğ’ª, i.e., any intersection of a finite number of open sets is again an open set.
B.2. Measures and Measure Spaces
Definition B.6 (ğœ-operator, generator). Suppose that Î© is a set and that the generator â„³ is a subset of its power set ğ’«(Î©). Then the ğœ-operator is defined as
ğœ(â„³) âˆ¶= â‹‚
ğ’œ,
ğ’œâˆˆâ„±(â„³)
where
â„±(â„³) âˆ¶= {ğ’œ âŠ‚ ğ’«(Î©) âˆ¶ â„³ âŠ‚ ğ’œ âˆ§ ğ’œ is a ğœ-algebra}.
The set â„±(â„³) contains all ğœ-algebras that contain â„³. Since the intersection of ğœ-algebras is again a ğœ-algebra, the set ğœ(â„³) of subsets of Î© is the smallest ğœ-algebra that contains â„³ âŠ‚ ğ’«(Î©). The set ğœ(â„³) is uniquely determined and it is called the ğœ-algebra generated by â„³.
Definition B.7 (Borel ğœ-algebra, Borel sets). Suppose (Î©,ğ’ª) is a topological space. The ğœ-algebra â„¬((Î©,ğ’ª)) âˆ¶= ğœ(ğ’ª) generated by the ğœ-operator applied to the open sets ğ’ª is called the Borel ğœ-algebra over Î©. If the open sets ğ’ª are implicitly known, it is customary to write â„¬(Î©) for â„¬((Î©,ğ’ª)). The elements of a Borel ğœ-algebra are called Borel sets.
By the definition of the ğœ-operator and the discussion above, the Borel ğœ- algebra is the smallest ğœ-algebra that contains all open sets ğ’ª given a topological space (Î©,ğ’ª).
The canonical topological space (â„,ğ’ª) over the real numbers is the one whose topology ğ’ª consists of the open intervals (ğ‘,ğ‘) with rational endpoints ğ‘,ğ‘ âˆˆ â„š. The Borel ğœ-algebra â„¬((â„,ğ’ª)) thus generated does not contain all subsets of â„; in fact, it can be shown that â„ and â„¬((â„,ğ’ª)) are equinumerous, while the power set of â„ has a larger cardinality than â„.
Since it is the most common one, the Borel ğœ-algebra â„¬((â„,ğ’ª)) is usually
simply called the Borel ğœ-algebra over â„ and denoted by â„¬(â„).
As noted above, a Borel ğœ-algebra ğœ(â„³) is uniquely determined by its gen- erator â„³; however, different generators may generate the same Borel ğœ-algebra. The Borel ğœ-algebra â„¬(â„) is generated by
â„³0 âˆ¶= {ğ´ âŠ‚ â„ âˆ¶ ğ´ âˆˆ ğ’ª}, â„³1 âˆ¶= {[ğ‘,ğ‘] âŠ‚ â„ âˆ¶ ğ‘,ğ‘ âˆˆ â„ âˆ§ ğ‘ â‰¤ ğ‘}, â„³2 âˆ¶= {[ğ‘,ğ‘] âŠ‚ â„ âˆ¶ ğ‘,ğ‘ âˆˆ â„š âˆ§ ğ‘ â‰¤ ğ‘}, â„³3 âˆ¶= {(ğ‘,ğ‘) âŠ‚ â„ âˆ¶ ğ‘,ğ‘ âˆˆ â„ âˆ§ ğ‘ < ğ‘}, â„³4 âˆ¶= {(ğ‘,ğ‘) âŠ‚ â„ âˆ¶ ğ‘,ğ‘ âˆˆ â„š âˆ§ ğ‘ < ğ‘}, â„³5 âˆ¶= {(ğ‘,ğ‘] âŠ‚ â„ âˆ¶ ğ‘,ğ‘ âˆˆ â„ âˆ§ ğ‘ â‰¤ ğ‘}, â„³6 âˆ¶= {(ğ‘,ğ‘] âŠ‚ â„ âˆ¶ ğ‘,ğ‘ âˆˆ â„š âˆ§ ğ‘ â‰¤ ğ‘},
157
(B.1a)
(B.1b)
(B.1c)
(B.1d)
(B.1e)
(B.1f)
(B.1g)
158
Appendix B. Measure and Probability Theory
â„³7 âˆ¶= {(âˆ’âˆ,ğ‘] âŠ‚ â„ âˆ¶ ğ‘ âˆˆ â„}, â„³8 âˆ¶= {(âˆ’âˆ,ğ‘] âŠ‚ â„ âˆ¶ ğ‘ âˆˆ â„š}, â„³9 âˆ¶= {(âˆ’âˆ,ğ‘) âŠ‚ â„ âˆ¶ ğ‘ âˆˆ â„}, â„³10 âˆ¶= {(âˆ’âˆ,ğ‘) âŠ‚ â„ âˆ¶ ğ‘ âˆˆ â„š}.
(B.1h)
(B.1i)
(B.1j)
(B.1k)
When working with cumulative distribution functions, the generators â„³7, â„³8, â„³9, and â„³10 are most useful and can be used with rational endpoints together with approximation arguments.
Next, we define measures and measure spaces.
Definition B.8 (measurable space). A measurable space is a pair (Î©,â„±) con- sisting of a non-empty set Î© and a ğœ-algebra â„± over Î©.
Definition B.9 (measurable function). Suppose (Î©,â„±) and (Î¨,ğ’¢) are measur- able spaces. An (â„±,ğ’¢)-measurable function from (Î©,â„±) to (Î¨,ğ’¢) is a function ğ‘‹âˆ¶ Î© â†’ Î¨ such that ğ‘‹âˆ’1(ğº) âˆˆ â„± for every ğº âˆˆ ğ’¢.
The set of measurable functions is closed under algebraic operations. It is
also closed under the pointwise sequential limits
liminf ğ‘›â†’âˆ
ğ‘“ğ‘›,
limsup ğ‘›â†’âˆ
ğ‘“ğ‘›,
sup ğ‘›â†’âˆ
ğ‘“ğ‘›,
i.e., these limits are measurable if the functions ğ‘“ğ‘› in the sequence âŸ¨ğ‘“ğ‘›âŸ©ğ‘›âˆˆâ„• are measurable.
Definition B.10 (measure). Suppose (Î©,â„±) is a measurable space. A function ğœ‡âˆ¶ â„± â†’ [0,âˆ] is called a measure if it satisfies the following properties:
1. The measure of the empty set is zero, i.e., ğœ‡(âˆ…) = 0.
2. The function ğœ‡ is countably additive (or ğœ-additive), i.e., if {ğ´ğ‘–}âˆ is a countable set of pairwise disjoint sets ğ´ğ‘– âˆˆ â„±, then
ğœ‡(
âˆ â‹ƒ ğ‘–=1
ğ´ğ‘–) =
âˆ âˆ‘ ğ‘–=1
ğœ‡(ğ´ğ‘–).
Definition B.11 (measure space). A measure space is a triple (Î©,â„±,ğœ‡) such that (Î©,â„±) is a measurable space and the function ğœ‡ is a measure on (Î©,â„±).
The push-forward measure transfers or pushes forward a measure from one
measurable space to another by using a measurable function.
B.3. The Lebesgue Integral
Definition B.12 (push-forward measure). Suppose that (Î©1,â„±1) and (Î©2,â„±2) are measurable spaces, that ğ‘“ âˆ¶ Î©1 â†’ Î©2 is a measurable function, and that ğœ‡âˆ¶ Î©1 â†’ [0,âˆ] is a measure. Then the push-forward measure of ğœ‡ is defined to be the measure
ğœ‡ âˆ˜ ğ‘“âˆ’1âˆ¶ Î©2 â†’ [0,âˆ],
also written as
ğ‘“#ğœ‡ âˆ¶= ğœ‡ âˆ˜ ğ‘“âˆ’1.
The push-forward measure has an application in the well-known change-of-
variables formula
âˆ« Î©1
ğ‘” âˆ˜ ğ‘“dğœ‡ = âˆ« Î©2
ğ‘”d(ğ‘“#ğœ‡).
The integrals here are Lebesgue integrals (see Section B.3).
In measure and probability theory as well as in integration, statements which are true everywhere except on sets whose measures vanishe often occur. This motivates the following definitions.
Definition B.13 (null set). Suppose (Î©,â„±,ğœ‡) is a measure space. A set ğ‘ âˆˆ â„± is called a null set with respect to the measure ğœ‡, if ğœ‡(ğ‘) = 0.
Definition B.14 (almost all/everywhere/surely). Suppose (Î©,â„±,ğœ‡) is a mea- sure space. A statement is said to be true for almost all ğ‘¥ âˆˆ Î© or almost everywhere/surely in Î©, if there exists a null set ğ‘ with respect to ğœ‡ such that the statement is true for all ğ‘¥ âˆˆ Î© âˆ– ğ‘. In this case, we write
(âˆ€ğœ‡ğ‘¥ âˆˆ Î©âˆ¶ statement)
âˆ¶âŸº (âˆƒğ‘ âˆˆ â„±âˆ¶ ğœ‡(ğ‘) = 0 âˆ§ (âˆ€ğ‘¥ âˆˆ Î© âˆ– ğ‘ âˆ¶ statement)).
B.3 The Lebesgue Integral
The Lebesgue integral plays an essential role in measure and probability theory. In very general terms, the domain of the function to be integrated is partitioned in order to define and to compute a Riemann (or Riemann-Stieltjes) integral of the function, while the range of the function to be integrated is partitioned in the Lebesgue integral. The Lebesgue integral interacts better with taking limits of sequences of functions; results for this situation are collected in Section B.5. Furthermore, the Lebesgue integral is defined for a larger class of functions than the Riemann integral. For example, the Dirichlet function
â„ â†’ {0,1},
ğ‘¥ â†¦ âŸ¦ğ‘¥ âˆˆ â„šâŸ§
159
160
Appendix B. Measure and Probability Theory
is not Riemann integrable, but it is Lebesgue integrable (and has Lebesgue integral zero).
In the following, two ways of constructing and defining the Lebesgue integral will be discussed. The first (in Section B.3.1) is based on partitions of the ranges of the functions and reduces the Lebesgue integral to a Riemann integral. The second (in Section B.3.2) is based on so-called simple functions, which provide discretizations of the areas under the graphs of the functions.
B.3.1 Construction and Definition Using the Riemann Integral
As mentioned above, in Lebesgue integration, the classical method of exhaustion can be applied to horizontal slices that partition the range of the function.
We start with a measure space (Î©,â„±,ğœ‡) (see Definition B.11). We recall that by Definition B.9 measurable functions defined on this measure space are functions such that the preimages of all sets in the ğœ-algebra of the image (mea- surable) space are in the ğœ-algebra â„± of the preimage (measurable) space Î©. In particular, the preimages
{ğ‘¥ âˆˆ Î© âˆ£ ğ‘“(ğ‘¥) â‰¥ ğ‘¦},
ğ‘¦ âˆˆ â„,
of measurable real valued functions ğ‘“ are elements of the ğœ-algebra â„±. In other words, the measure ğœ‡ assigns the lengths
ğœ‡({ğ‘¥ âˆˆ Î© âˆ£ ğ‘“(ğ‘¥) â‰¥ ğ‘¦})
to the preimages, which we will use in the following.
We consider a non-negative measurable real valued function ğ‘“ âˆ¶ â„ â†’ â„+ in the (ğ‘¥,ğ‘¦)-plane. At each point ğ‘¦ in the range of ğ‘“, thin horizontal slices between ğ‘¦ and ğ‘¦âˆ’dğ‘¦ contribute to the integral wherever the function value ğ‘“(ğ‘¥) is greater equal ğ‘¦ such that these slices lie between the ğ‘¥-axis and the graph of the function. The area that is contributed to the integral at any point ğ‘¦ in the range of ğ‘“ is hence given by
ğœ‡({ğ‘¥ âˆˆ â„ âˆ£ ğ‘“(ğ‘¥) â‰¥ ğ‘¦})dğ‘¦.
Summing these contributions as a Riemann integral yields the following defini- tion.
Definition B.15 (Lebesgue integral (via Riemann integral)). Suppose that (Î©,â„±,ğœ‡) is a measure space and that ğ‘“ âˆ¶ â„ â†’ â„+ is non-negative measurable function. Its Lebesgue integral is then defined by
âˆ
âˆ« Î©
ğ‘“dğœ‡ âˆ¶= âˆ« 0
ğœ‡({ğ‘¥ âˆˆ â„ âˆ£ ğ‘“(ğ‘¥) â‰¥ ğ‘¦})dğ‘¦,
(B.2)
B.3. The Lebesgue Integral
where the integral on the right-hand side is an improper Riemann integral.
In this definition, ğœ‡({ğ‘¥ âˆˆ â„ âˆ£ ğ‘“(ğ‘¥) â‰¥ ğ‘¦}) can be replaced by ğœ‡({ğ‘¥ âˆˆ â„ âˆ£
ğ‘“(ğ‘¥) > ğ‘¦}).
The following theorem is well-known in Riemann integration.
Theorem B.16. Suppose ğ‘“ âˆ¶ [ğ‘,ğ‘] â†’ â„ is monotone. Then ğ‘“ is Riemann integrable on the interval [ğ‘,ğ‘].
Since the integrand on the right-hand side in (B.2) is a non-negative and monotone decreasing function, the improper Riemann integral exists and has a value in the interval [0,âˆ).
In the next step, we extend these considerations from non-negative functions to signed functions. If ğ‘“ is a measurable function to the extended real numbers, we define
ğ‘“+(ğ‘¥) âˆ¶= {
ğ‘“(ğ‘¥), 0,
ğ‘“(ğ‘¥) > 0, ğ‘“(ğ‘¥) â‰¤ 0,
ğ‘“âˆ’(ğ‘¥) âˆ¶= {
âˆ’ğ‘“(ğ‘¥), 0,
ğ‘“(ğ‘¥) < 0, ğ‘“(ğ‘¥) â‰¥ 0,
both of which are non-negative and measurable. With these definitions, we have
ğ‘“ = ğ‘“+ âˆ’ ğ‘“âˆ’, |ğ‘“| = ğ‘“+ + ğ‘“âˆ’,
which allows us to reduce the Lebesgue integrals of signed functions ğ‘“ to the Lebesgue integrals âˆ«ğ‘“+dğœ‡ and âˆ«ğ‘“âˆ’dğœ‡ of the non-negative functions ğ‘“+ and ğ‘“âˆ’.
Definition B.17 (Lebesgue integral of a measurable function). Suppose ğ‘“ is a measurable function to the extended real numbers. If the Lebesgue integral âˆ«ğ‘“+dğœ‡ or the Lebesgue integral âˆ«ğ‘“âˆ’dğœ‡ exists and is finite, i.e.,
âˆ« Î©
ğ‘“+dğœ‡ < âˆ âˆ¨ âˆ« Î©
ğ‘“âˆ’dğœ‡ < âˆ,
then
âˆ« Î©
ğ‘“dğœ‡ âˆ¶= âˆ« Î©
ğ‘“+dğœ‡ âˆ’ âˆ« Î©
ğ‘“âˆ’dğœ‡
is called the Lebesgue integral of ğ‘“.
Lebesgue integrals have values in the extended real numbers. A function is called Lebesgue integrable, if the area between its graph and the ğ‘¥-axis is finite as expressed in the following definition.
161
162
Appendix B. Measure and Probability Theory
Definition B.18 (Lebesgue integrable function). Suppose ğ‘“ is a measurable function to the extended real numbers defined on a measure space (Î©,â„±,ğœ‡). It is called Lebesgue integrable with respect to ğœ‡ if the integral
âˆ« Î©
|ğ‘“|dğœ‡ < âˆ
of its absolute value is finite.
The following theorem gives some important classes of Lebesgue measurable
functions including Riemann integrable ones.
Theorem B.19 (some classes of Lebesgue measurable functions). Continuous functions, semicontinuous functions, step functions, monotone functions, Rie- mann integrable functions, and functions of bounded variation are Lebesgue measurable.
Complex valued functions are integrated by considering the real and imagi- nary parts separately. If ğ‘“ = ğ‘“1 +iğ‘“2 for real valued integrable functions ğ‘“1 and ğ‘“2, then the Lebesgue integral of ğ‘“ is defined by
âˆ« Î©
ğ‘“dğœ‡ âˆ¶= âˆ« Î©
ğ‘“1dğœ‡ + iâˆ« Î©
ğ‘“2dğœ‡.
(B.3)
Furthermore, the function ğ‘“ is Lebesgue integrable if and only if its absolute value is Lebesgue integrable.
The Riemann integral with respect to an orientation is defined as âˆ« ğ‘ ğ‘
âˆ’âˆ« ğ‘“. In Lebesgue integration, there is no orientation, since the domains of ğ‘ integration are sets. Still, if the domain is an interval ğ¼ âˆ¶= [ğ‘,ğ‘], we can define
ğ‘
ğ‘“ âˆ¶=
ğ‘ âˆ« ğ‘
ğ‘“dğœ‡ âˆ¶= âˆ’âˆ«
[ğ‘,ğ‘]
ğ‘“dğœ‡.
B.3.2 Construction and Definition Using Simple Functions
In the previous construction using the Riemann integral, we have used the method of exhaustion in Definition B.15 twice: first for horizontal slices, i.e., a partition of the range of the function, and then for the vertical slices in the Riemann integral. The second construction and definition in this section applies the method of exhaustion to both directions directly via so-called simple func- tions, which are essentially a discretization of the area between the graph of the function and the ğ‘¥-axis.
B.3. The Lebesgue Integral
Again, we start with a measure space (Î©,â„±,ğœ‡) (see Definition B.11). In order to discretize the area between the graph of the function and the ğ‘¥-axis, we consider the indicator functions
ğœ’ğ‘†âˆ¶ Î© â†’ {0,1},
ğ‘¥ â†¦ âŸ¦ğ‘¥ âˆˆ ğ‘†âŸ§
of measurable sets ğ‘†. The only possible value of an integral that is consistent with the measure ğœ‡ is
âˆ« Î©
ğœ’ğ‘†dğœ‡ âˆ¶= ğœ‡(ğ‘†),
which may be equal to âˆ.
Using this first definition and the notion that the integral should be a linear operator, we extend the definition of the Lebesgue integral to linear combinations of indicator functions, the so-called simple functions.
Definition B.20 (simple function). A simple function is a (finite) linear com- bination
ğ‘ âˆ‘ ğ‘›=1
ğ‘ğ‘›ğœ’ğ‘†ğ‘›
,
where ğ‘ğ‘› âˆˆ â„ for all ğ‘› âˆˆ [1âˆ¶ğ‘] and ğ‘†ğ‘›, ğ‘› âˆˆ [1âˆ¶ğ‘], are disjoint measurable sets.
Simple functions are measurable. For non-negative simple functions ğ‘ , i.e., when the coefficients ğ‘ğ‘› are non-
negative, we define
âˆ« Î©
ğ‘ dğœ‡ = âˆ« Î©
(
ğ‘ âˆ‘ ğ‘›=1
ğ‘ğ‘›ğœ’ğ‘†ğ‘›
)dğœ‡ âˆ¶=
ğ‘ âˆ‘ ğ‘›=1
ğ‘ğ‘› âˆ« Î©
ğœ’ğ‘†ğ‘›
dğœ‡ =
ğ‘ âˆ‘ ğ‘›=1
ğ‘ğ‘›ğœ‡(ğ‘†ğ‘›)
by linearity, which may be equal to âˆ. Here the definition 0â‹…âˆ âˆ¶= 0 is used for the products. By the ğœ-additivity of the measure ğœ‡ (see Definition B.10), the integral does not depend on the particular linear combination used to represent the simple function.
If Î¨ is a measurable subset of Î©, then we define the integral of a non-negative
simple function ğ‘  on Î¨ by
âˆ« Î¨
ğ‘ dğœ‡ âˆ¶= âˆ« Î©
ğœ’Î¨ğ‘ dğœ‡ âˆ¶=
ğ‘ âˆ‘ ğ‘›=1
ğ‘ğ‘›ğœ‡(ğ‘†ğ‘› âˆ© Î¨).
In the next step, we extend the construction of the Lebesgue integral from non-negative simple functions to non-negative measurable functions ğ‘“ which take values in the extended real numbers.
163
164
Appendix B. Measure and Probability Theory
Definition B.21 (Lebesgue integral (via simple functions)). Suppose ğ‘“ is a non-negative measurable function on a measurable subset Î¨ of a measure space (Î©,â„±,ğœ‡). Then its Lebesgue integral on Î¨ is defined by
âˆ« Î¨
ğ‘“dğœ‡ âˆ¶= sup{âˆ« Î¨
ğ‘ dğœ‡ âˆ£ ğ‘  is simple âˆ§ 0 â‰¤ ğ‘  â‰¤ ğ‘“}.
The value of this integral may be equal to âˆ. This definition and the pre- ceding one for non-negative simple functions coincide when non-negative simple functions are integrated. Furthermore, it can be shown that Definition B.15 (via Riemann integrals) and Definition B.21 (via simple functions) coincide.
The Lebesgue integral of signed functions is constructed as in Definition B.17 via the positive and negative parts ğ‘“+ and ğ‘“âˆ’, which are non-negative. Further- more, the Lebesgue integral of complex valued functions is defined as in (B.3) via their real and imaginary parts.
B.3.3 Properties
We again suppose that (Î©,â„±,ğœ‡) is a measure space. For the purposes of measure and probability theory, it is useful to define the equality of two functions as equality almost everywhere/surely, i.e., they are equal if they coincide outside a subset of measure zero.
Definition B.22 (equality of functions). Two functions ğ‘“ and ğ‘” defined on a measure space (Î©,â„±,ğœ‡) are called equal if they are equal almost everywhere/surely, i.e.,
ğ‘“
ğœ‡ = ğ‘”
âˆ¶âŸº âˆ€ğœ‡ğ‘¥ âˆˆ Î©âˆ¶ ğ‘“(ğ‘¥) = ğ‘”(ğ‘¥).
This equality relation is an equivalence relation. Analogously, we also define
ğ‘“ < ğ‘”, ğ‘“ â‰¤ ğ‘”, ğ‘“ > ğ‘”, and ğ‘“ â‰¥ ğ‘”.
With this definition of equality, equal functions have equal integrals if the
integrals exist.
Theorem B.23 (integrals of equal functions). Suppose two functions ğ‘“ and ğ‘” defined on a measure space (Î©,â„±,ğœ‡) are equal. Then ğ‘“ is Lebesgue integrable if and only if ğ‘” is Lebesgue integrable, and if their integrals exists, then
ğ‘“
ğœ‡ = ğ‘” âŸ¹ âˆ« Î©
ğ‘“dğœ‡ = âˆ« Î©
ğ‘”dğœ‡
holds.
B.4. The Radon-Nikodym Derivative
Theorem B.24 (linearity of the Lebesgue integral). Suppose that ğ‘“ and ğ‘” are two Lebesgue integrable functions defined on a measure space (Î©,â„±,ğœ‡) and that ğ‘ and ğ‘ are two real numbers. Then the function ğ‘ğ‘“ +ğ‘ğ‘” is Lebesgue integrable, and the equality
âˆ« Î©
(ğ‘ğ‘“ + ğ‘ğ‘”)dğœ‡ = ğ‘âˆ« Î©
ğ‘“dğœ‡ + ğ‘âˆ« Î©
ğ‘”dğœ‡
holds.
Theorem B.25 (monotonicity of the Lebesgue integral). Suppose that ğ‘“ and ğ‘” are two Lebesgue integrable functions defined on a measure space (Î©,â„±,ğœ‡). If ğ‘“ â‰¤ ğ‘” almost everywhere/surely, then the inequality
âˆ« Î©
ğ‘“dğœ‡ â‰¤ âˆ« Î©
ğ‘”dğœ‡
holds.
B.4 The Radon-Nikodym Derivative
Before we can state the Radon-Nikodym theorem and define the Radon-Nikodym derivative, two definitions are needed.
Definition B.26 (ğœ-finite measure). Suppose that (Î©,â„±,ğœ‡) is a measure space. Then the measure ğœ‡ is called a ğœ-finite measure if the set Î© can be covered by at most countably many measurable sets with finite measure, i.e., there exist sets ğ´ğ‘› with ğœ‡(ğ´ğ‘›) < âˆ for all ğ‘› âˆˆ â„• such that Î© = â‹ƒ Definition B.27 (absolutely continuous). Suppose that (Î©,â„±) is a measurable space on which the measures ğœ‡ and ğœˆ are defined. Then the measure ğœ‡ is called absolutely continuous with respect to ğœˆ and we write ğœ‡ â‰ª ğœˆ if ğœ‡(ğ´) = 0 for every set ğ´ âˆˆ â„± for which ğœˆ(ğ´) = 0, i.e.,
ğ‘›âˆˆâ„• ğ´ğ‘›.
ğœ‡ â‰ª ğœˆ
âˆ¶âŸº (âˆ€ğ´ âˆˆ â„±âˆ¶
ğœˆ(ğ´) = 0 âŸ¹ ğœ‡(ğ´) = 0).
The absolute continuity of measures is reflexive and transitive, but not an-
tisymmetric, and hence it is a preorder and not a partial order.
Using ğœ-finiteness and absolute continuity, we can state the following theo-
rem.
Theorem B.28 (Radon-Nikodym). Suppose that (Î©,â„±) is a measurable space on which the ğœ-finite measures ğœ‡ and ğœˆ are defined. If ğœ‡ â‰ª ğœˆ, then there exists a â„±-measurable function ğ‘“ âˆ¶ Î© â†’ [0,âˆ) such that
âˆ€ğ´ âˆˆ â„±âˆ¶
ğœ‡(ğ´) = âˆ« ğ´
ğ‘“dğœˆ
165
166
Appendix B. Measure and Probability Theory
holds. The function ğ‘“ is unique up to a null set with respect to ğœˆ.
Since the equation in the theorem must hold for every measurable set ğ´, we can intuitively understand that the equation must hold for infinitesimal elements dğœ‡ and dğœˆ and we can informally write
dğœ‡ = ğ‘“dğœˆ.
(B.4)
From this point of view, the function value ğ‘“ is the factor of proportionality between ğœ‡(ğ´) and ğœˆ(ğ´). Hence the condition that ğœ‡ must be absolutely con- tinuous with respect to ğœˆ becomes obvious. Suppose ğœˆ(ğ´) = 0. If ğœ‡(ğ´) â‰  0, then certainly no value ğ‘“ that satisfies (B.4) can exist. Therefore ğœˆ(ğ´) = 0 must imply ğœ‡(ğ´) = 0.
An extension to finite valued signed measures ğœˆ holds. This theorem makes
the following definition possible.
Definition B.29 (Radon-Nikodym derivative). The ğœˆ-almost unique function ğ‘“ in Theorem B.28 is called the Radon-Nikodym derivative of ğœ‡ with respect to ğœˆ and written
dğœ‡ dğœˆ Important properties of the Radon-Nikodym derivative are collected in the
ğ‘“ =
.
following.
Theorem B.30 (properties of the Radon-Nikodym derivative). Suppose ğœ†, ğœ‡, and ğœˆ are ğœ-finite measures on the measurable space (Î©,â„±).
1. Linearity: If ğœ‡ â‰ª ğœ† and ğœˆ â‰ª ğœ†, then
d(ğœ‡ + ğœˆ) dğœ†
ğœ†=
dğœ‡ dğœ†
+
dğœˆ dğœ†
.
2. Chain rule: If ğœˆ â‰ª ğœ‡ â‰ª ğœ†, then
dğœˆ dğœ†
ğœ†=
dğœˆ dğœ†
dğœ‡ dğœ†
.
3. In particular (choosing ğœˆ = ğœ†), if ğœ‡ â‰ª ğœˆ and ğœˆ â‰ª ğœ‡, then
dğœ‡ dğœˆ
ğœˆ= (
dğœˆ dğœ‡
)
âˆ’1
.
4. If ğœ‡ â‰ª ğœ† and ğ‘“ is a Lebesgue integrable function with respect to ğœ‡, then
âˆ« Î©
ğ‘“dğœ‡ = âˆ« Î©
ğ‘“
dğœ‡ dğœ†
dğœ†.
B.5. Lebesgue Convergence Theorems
5. If ğœˆ is a finite signed or complex measure, then
d|ğœˆ| dğœ‡
= âˆ£
dğœˆ dğœ‡
âˆ£.
B.5 Lebesgue Convergence Theorems
Fatouâ€™s lemma, the Lebesgue monotone convergence theorem, and the Lebesgue dominated convergence theorem are important results in the theory of Lebesgue integration. Given a sequence âŸ¨ğ‘“ğ‘›âŸ©ğ‘›âˆˆâ„• of functions, they answer the question when the limit limğ‘›â†’âˆ and Lebesgue integration commute. The results are also important in probability theory, since they provide sufficient conditions for the convergence of expected values of random variables.
The first result in this section is Fatouâ€™s lemma, which shows that an inequal- ity holds when the limit liminfğ‘›â†’âˆ and Lebesgue integration are interchanged. The sequence âŸ¨ğ‘“ğ‘›âŸ©ğ‘›âˆˆâ„• does not have to converge pointwise, but the functions are supposed to be non-negative. (For definitions of liminf and limsup, see Definition B.58).
Lemma B.31 (Fatouâ€™s lemma). Suppose that (Î©,â„±,ğœ‡) is a measure space, that ğ‘‹ âˆˆ â„±, and that âŸ¨ğ‘“ğ‘›âŸ©ğ‘›âˆˆâ„• is a sequence of (â„±,â„¬(â„+ 0 ))-measurable non-negative functions ğ‘“ğ‘›âˆ¶ ğ‘‹ â†’ [0,âˆ]. Suppose further that ğ‘“ âˆ¶ ğ‘‹ â†’ [0,âˆ] is defined as ğ‘“(ğ‘¥) âˆ¶= liminfğ‘›â†’âˆ ğ‘“ğ‘›(ğ‘¥) for ğœ‡-almost all ğ‘¥ âˆˆ ğ‘‹. Then the function ğ‘“ is (â„±,â„¬(â„+
0 ))-measurable, and the inequality
âˆ« ğ‘‹
ğ‘“dğœ‡ = âˆ« ğ‘‹
liminf ğ‘›â†’âˆ
ğ‘“ğ‘›dğœ‡ â‰¤ liminf ğ‘›â†’âˆ
âˆ« ğ‘‹
ğ‘“ğ‘›dğœ‡
holds.
An example for the strict inequality is the measure space Î© âˆ¶= [0,1] with the
Borel ğœ-algebra and the Lebesgue measure. The functions are defined by
ğ‘“ğ‘›(ğ‘¥) âˆ¶= {
ğ‘›, ğ‘¥ âˆˆ [0,1/ğ‘›), otherwise. 0,
Then the sequence âŸ¨ğ‘“ğ‘›âŸ©ğ‘›âˆˆâ„• converges pointwise to the zero function, but each function ğ‘“ğ‘› has integral one, leading to the strict inequality in Lemma B.31.
The reverse Fatouâ€™s lemma shows that an inequality holds when the limit limsupğ‘›â†’âˆ and Lebesgue integration are interchanged. Again the sequence âŸ¨ğ‘“ğ‘›âŸ©ğ‘›âˆˆâ„• does not have to converge pointwise, but now the functions are supposed to be dominated by a majorant ğ‘”.
167
168
Appendix B. Measure and Probability Theory
Lemma B.32 (reverse Fatouâ€™s lemma). Suppose that (Î©,â„±,ğœ‡) is a measure space, that ğ‘‹ âˆˆ â„±, and that âŸ¨ğ‘“ğ‘›âŸ©ğ‘›âˆˆâ„• is a sequence of (â„±,â„¬(â„+ 0 ))-measurable functions ğ‘“ğ‘›âˆ¶ ğ‘‹ â†’ [âˆ’âˆ,âˆ]. Suppose further that ğ‘”âˆ¶ ğ‘‹ â†’ [0,âˆ] is a non- negative, (â„±,â„¬(â„+
0 ))-measurable, and integrable function on ğ‘‹ such that
âˆ€ğœ‡ğ‘¥ âˆˆ ğ‘‹âˆ¶
âˆ€ğ‘› âˆˆ â„•âˆ¶
ğ‘“ğ‘›(ğ‘¥) â‰¤ ğ‘”(ğ‘¥).
Then the inequality
limsup ğ‘›â†’âˆ
âˆ« ğ‘‹
ğ‘“ğ‘›dğœ‡ â‰¤ âˆ« ğ‘‹
limsup ğ‘›â†’âˆ
ğ‘“ğ‘›dğœ‡
holds.
Proof. We consider the sequence ğ‘” âˆ’ ğ‘“ğ‘›. Since âˆ« |ğ‘”|dğœ‡ < âˆ by as- ğ‘‹ sumption, this sequence is defined ğœ‡-almost everywhere. It is also non-negative by the assumption that ğ‘” dominates the ğ‘“ğ‘›. Therefore we can apply Fatouâ€™s lemma, Lemma B.31, to this sequence and use the linearity of Lebesgue integra- tion to find the inequality.
ğ‘”dğœ‡ = âˆ« ğ‘‹
The next result in this section is the Fatou-Lebesgue theorem. It collects both inequalities of Fatouâ€™s lemma and reverse Fatouâ€™s lemma. Noting that the middle inequality is trivially true makes remembering the directions of the first inequality (Fatouâ€™s lemma) and the third inequality (reverse Fatouâ€™s lemma) easier.
Theorem B.33 (Fatou-Lebesgue theorem). Suppose that (Î©,â„±,ğœ‡) is a measure space, that ğ‘‹ âˆˆ â„±, and that âŸ¨ğ‘“ğ‘›âŸ©ğ‘›âˆˆâ„• is a sequence of (â„±,â„¬(â„+ 0 ))-measurable functions ğ‘“ğ‘›âˆ¶ ğ‘‹ â†’ [âˆ’âˆ,âˆ] that is ğœ‡-almost everywhere dominated by an inte- grable function ğ‘”âˆ¶ ğ‘‹ â†’ [0,âˆ], i.e.,
âˆ€ğœ‡ğ‘¥ âˆˆ ğ‘‹âˆ¶
âˆ€ğ‘› âˆˆ â„•âˆ¶
|ğ‘“ğ‘›(ğ‘¥)| â‰¤ ğ‘”(ğ‘¥).
Then all functions ğ‘“ğ‘› are integrable as well as the pointwise defined functions liminfğ‘›â†’âˆ ğ‘“ğ‘› and limsupğ‘›â†’âˆ ğ‘“ğ‘›, and the inequalities
âˆ« ğ‘‹
liminf ğ‘›â†’âˆ
ğ‘“ğ‘›dğœ‡ â‰¤ liminf ğ‘›â†’âˆ
âˆ« ğ‘‹
ğ‘“ğ‘›dğœ‡ â‰¤ limsup ğ‘›â†’âˆ
âˆ« ğ‘‹
ğ‘“ğ‘›dğœ‡ â‰¤ âˆ« ğ‘‹
limsup ğ‘›â†’âˆ
ğ‘“ğ‘›dğœ‡
hold.
Proof. The absolute values of all functions ğ‘“ğ‘› well as the pointwise defined functions liminfğ‘›â†’âˆ ğ‘“ğ‘› and limsupğ‘›â†’âˆ ğ‘“ğ‘› are dominated by the majorant ğ‘” and are hence integrable, since ğ‘” is integrable by assumption.
Fatouâ€™s lemma, Lemma B.31, can be applied to the functions ğ‘“ğ‘› + ğ‘”, which yields the first inequality. The third inequality is reverse Fatouâ€™s lemma, Lemma B.32.
B.5. Lebesgue Convergence Theorems
If we assume that the sequence âŸ¨ğ‘“ğ‘›âŸ©ğ‘›âˆˆâ„• is a pointwise non-decreasing se- quence of non-negative functions that converges pointwise to a function ğ‘“, then the limit limğ‘›â†’âˆ and Lebesgue integration indeed commute. This is the Lebesgue monotone convergence theorem.
Theorem B.34 (Lebesgue monotone convergence theorem). Suppose that (Î©,â„±,ğœ‡) is a measure space, that ğ‘‹ âˆˆ â„±, and that âŸ¨ğ‘“ğ‘›âŸ©ğ‘›âˆˆâ„• is a ğœ‡-almost everywhere point- wise non-decreasing sequence of (â„±,â„¬(â„+ 0 ))-measurable non-negative functions ğ‘“ğ‘›âˆ¶ ğ‘‹ â†’ [0,âˆ], i.e.,
âˆ€ğœ‡ğ‘¥ âˆˆ ğ‘‹âˆ¶
âˆ€ğ‘› âˆˆ â„•âˆ¶
0 â‰¤ ğ‘“ğ‘›(ğ‘¥) â‰¤ ğ‘“ğ‘›+1(ğ‘¥) â‰¤ âˆ.
Suppose further that the pointwise limits limğ‘›â†’âˆ ğ‘“ğ‘›(ğ‘¥) exist for ğœ‡-almost all ğ‘¥ âˆˆ ğ‘‹ and that the function ğ‘“ is ğœ‡-almost everywhere equal to this ğœ‡-almost everywhere pointwise limit of the sequence âŸ¨ğ‘“ğ‘›âŸ©ğ‘›âˆˆâ„•, i.e., ğ‘“(ğ‘¥) = limğ‘›â†’âˆ ğ‘“ğ‘›(ğ‘¥) for ğœ‡-almost all ğ‘¥ âˆˆ ğ‘‹. Then the function ğ‘“ is (â„±,â„¬(â„+ 0 ))-measurable, and the equality
âˆ« ğ‘‹
ğ‘“dğœ‡ = âˆ« ğ‘‹
lim ğ‘›â†’âˆ
ğ‘“ğ‘›dğœ‡ = lim ğ‘›â†’âˆ
âˆ« ğ‘‹
ğ‘“ğ‘›dğœ‡
holds.
If we assume that the sequence âŸ¨ğ‘“ğ‘›âŸ©ğ‘›âˆˆâ„• converges pointwise to a function and that is dominated by a majorant ğ‘”, then the limit limğ‘›â†’âˆ and Lebesgue integration indeed commute. This is the Lebesgue dominated convergence the- orem.
Theorem B.35 (Lebesgue dominated convergence theorem). Suppose that (Î©,â„±,ğœ‡) is a measure space, that ğ‘‹ âˆˆ â„±, and that âŸ¨ğ‘“ğ‘›âŸ©ğ‘›âˆˆâ„• is a sequence of (â„±,â„¬(â„+ 0 ))- measurable functions ğ‘“ğ‘›âˆ¶ ğ‘‹ â†’ [âˆ’âˆ,âˆ] that is ğœ‡-almost everywhere dominated by an integrable function ğ‘”âˆ¶ ğ‘‹ â†’ [0,âˆ], i.e.,
âˆ€ğœ‡ğ‘¥ âˆˆ ğ‘‹âˆ¶
âˆ€ğ‘› âˆˆ â„•âˆ¶
|ğ‘“ğ‘›(ğ‘¥)| â‰¤ ğ‘”(ğ‘¥).
Suppose further that the pointwise limits limğ‘›â†’âˆ ğ‘“ğ‘›(ğ‘¥) exist for ğœ‡-almost all ğ‘¥ âˆˆ ğ‘‹ and that the function ğ‘“ is ğœ‡-almost everywhere equal to this ğœ‡-almost everywhere pointwise limit of the sequence âŸ¨ğ‘“ğ‘›âŸ©ğ‘›âˆˆâ„•, i.e., ğ‘“(ğ‘¥) = limğ‘›â†’âˆ ğ‘“ğ‘›(ğ‘¥) for ğœ‡-almost all ğ‘¥ âˆˆ ğ‘‹. Then the function ğ‘“ is integrable, and the equality
lim ğ‘›â†’âˆ
âˆ« ğ‘‹
|ğ‘“ğ‘› âˆ’ ğ‘“|dğœ‡ = 0
holds, which also implies
âˆ« ğ‘‹
ğ‘“dğœ‡ = âˆ« ğ‘‹
lim ğ‘›â†’âˆ
ğ‘“ğ‘›dğœ‡ = lim ğ‘›â†’âˆ
âˆ« ğ‘‹
ğ‘“ğ‘›dğœ‡.
169
170
Appendix B. Measure and Probability Theory
B.6 Probability Spaces and Random Variables
A probability space is a triple (Î©,â„±,â„™) consisting of a sample space Î©, an event space â„±, and a probability function â„™. The sample space Î© is the set of all possible outcomes, where an outcome is the result of a single execution of the model. The event space â„± is the set of all events, where an event is a set of zero or more outcomes, i.e., a subset of the sample space Î©. The probability function â„™âˆ¶ â„± â†’ [0,1] returns the probability of each event; the probability of the whole sample space Î© must be equal to one.
In the following, formal definitions of a probability space and a random
variable are given.
Definition B.36 (probability measure). A probability measure â„™ is a measure that assigns value one to the entire (sample) space Î©, i.e., â„™(Î©) = 1.
Definition B.37 (probability space). A probability space (Î©,â„±,â„™) is a triple consisting of a sample space Î©, an event space â„±, and a probability function â„™ that satisfy the following properties:
1. The sample space Î© is an arbitrary non-empty set.
2. The event space â„± is a set of subsets (events) of the sample space Î© and a ğœ-algebra.
3. The probability function â„™âˆ¶ â„± â†’ [0,1] is a probability measure.
If (Î©,â„±,â„™) is a probability space, then it is also a measure space and (Î©,â„±)
is a measurable space by the definition of a probability space.
Random variables are functions on probability spaces which are compatible
with measuring probabilities.
Definition B.38 (random variable). Suppose (Î©,â„±,â„™) is a probability space and (Î¨,ğ’¢) is a measurable space. Then a (Î¨,ğ’¢)-valued random variable ğ‘‹ is a measurable function ğ‘‹âˆ¶ Î© â†’ Î¨.
The only difference between a measurable function (with domain (Î©,â„±)) and a random variable (with domain (Î©,â„±,â„™)) is that a random variable comes with a probability measure.
While the domain of a random variable ğ‘‹ is the sample space, its codomain Î¨ is called the observation space. The definition of a random variable ğ‘‹ means that the probability measure â„™ yields the probabilities of all preimages ğ‘‹âˆ’1(ğº) (as long as ğº is in the ğœ-algebra ğ’¢ of the measurable space that is the codomain In other words, a random or observation space of the random variable ğ‘‹).
B.6. Probability Spaces and Random Variables
variable ğ‘‹ maps any outcome ğœ” âˆˆ Î© to an observed quantity ğœ“ âˆˆ Î¨ such that the outcomes that lead to an observation ğº âˆˆ ğ’¢ in the observation space have a probability given by the probability measure â„™.
Real valued random variables ğ‘‹âˆ¶ Î© â†’ â„ are the important special case where the observation space are the real numbers. We can use the generator (B.1h) of the Borel ğœ-algebra â„¬(â„) (see Section B.2). Since it suffices to check measurability on any generator of the Borel ğœ-algebra and the preimages in Definition B.9 are ğ‘‹âˆ’1((âˆ’âˆ,ğ‘]) = {ğœ” âˆˆ Î© âˆ¶ ğ‘‹(ğœ”) â‰¤ ğ‘}, a function ğ‘‹âˆ¶ Î© â†’ â„ is a (real valued) random variable if {ğœ” âˆˆ Î© âˆ¶ ğ‘‹(ğœ”) â‰¤ ğ‘} âˆˆ â„± holds for all ğ‘ âˆˆ â„. Integrable random variables are Lebesgue integrable functions (see Defini-
tion B.18).
Definition B.39 (integrable random variable). A random variable ğ‘‹ is called integrable if it is a Lebesgue integrable function, i.e., if
ğ”¼[|ğ‘‹|] = âˆ« Î©
|ğ‘‹|dâ„™ < âˆ.
The definition of an integrable random variable implies that ğ”¼[ğ‘‹] exists and
has a finite value as well.
Intuitively, in the one-dimensional case, a real valued random variable ğ‘‹âˆ¶ Î© â†’
â„ has the probability density function ğ‘“ğ‘‹ if
ğ‘
â„™[ğ‘ < ğ‘‹ â‰¤ ğ‘] = âˆ« ğ‘
ğ‘“ğ‘‹(ğ‘¥)dğ‘¥
holds for all intervals [ğ‘,ğ‘] âŠ‚ â„, where ğ‘“ğ‘‹ is a non-negative Lebesgue integrable function. The cumulative distribution function of ğ‘‹ is the function
ğ‘¥
ğ¹ğ‘‹(ğ‘¥) âˆ¶= â„™[ğ‘‹ â‰¤ ğ‘¥] = âˆ«
ğ‘“ğ‘‹(ğ‘¡)dğ‘¡.
âˆ’âˆ
Using the cumulative distribution function, we can also write
ğ‘
â„™[ğ‘ < ğ‘‹ â‰¤ ğ‘] = ğ¹ğ‘‹(ğ‘) âˆ’ ğ¹ğ‘‹(ğ‘) = âˆ« ğ‘
ğ‘“ğ‘‹(ğ‘¥)dğ‘¥.
If the probability density function ğ‘“ğ‘‹ is continuous at ğ‘¥, then both functions
are related by
ğ‘“ğ‘‹(ğ‘¥) = ğ¹ â€² If the cumulative distribution function ğ¹ğ‘‹ is left-continuous at ğ‘¥, the value of
ğ‘‹(ğ‘¥).
â„™[ğ‘‹ = ğ‘] = ğ¹ğ‘‹(ğ‘) âˆ’ lim ğ‘¥â†’ğ‘âˆ’
ğ¹ğ‘‹(ğ‘¥)
171
172
Appendix B. Measure and Probability Theory
vanishes, which means that there is no discrete component at ğ‘ (continuous random variable). If it is not left-continuous, this value is called the discrete component of the probability distribution at ğ‘ (discrete random variable).
Formally, the probability density function is defined within the measure the-
oretic framework as follows.
Definition B.40 (probability density function (PDF), cumulative distribution function (CDF)). Suppose that (Î©,â„±,â„™) is a probability space, that (Î¨,ğ’¢,ğœˆ) is a measure space, and that ğ‘‹âˆ¶ Î© â†’ Î¨ is a (Î¨,ğ’¢,ğœˆ)-valued random variable. The so-called reference measure ğœˆ is the counting measure in the case that Î¨ is finite (discrete random variable) and the Lebesgue measure in the case that Î¨ = â„ğ‘‘, ğ‘‘ âˆˆ â„• ((multidimensional) continuous random variable). Then any measurable function ğ‘“ğ‘‹âˆ¶ Î¨ â†’ â„+
0 that satisfies
âˆ€ğº âˆˆ ğ’¢âˆ¶
â„™[ğ‘‹ âˆˆ ğº] âˆ¶= â„™(ğº) âˆ¶= âˆ«
ğ‘‹âˆ’1(ğº)
dâ„™ = âˆ« ğº
ğ‘“ğ‘‹dğœˆ
is called a probability density function of the random variable ğ‘‹. Furthermore, its cumulative distribution function is the function
ğ¹ğ‘‹âˆ¶ Î¨ â†’ â„+
0 , ğ¹ğ‘‹(ğ‘¥) âˆ¶= â„™[ğ‘‹ â‰¤ ğ‘¥] âˆ¶= âˆ« Î¨
âŸ¦ğ‘¡1 â‰¤ ğ‘¥1âŸ§â‹¯âŸ¦ğ‘¡ğ‘› â‰¤ ğ‘¥ğ‘›âŸ§ğ‘“ğ‘‹(ğ‘¡)dğœˆ(ğ‘¡),
where ğ‘‹ â‰¤ ğ‘¥ is understood elementwise whenever the random variable ğ‘‹ is vector valued and the ordering â‰¤ in the integrand is a total ordering on Î¨.
In other words, the probability density function ğ‘“ğ‘‹ is the Radon-Nikodym
derivative
ğ‘“ğ‘‹ =
d(ğ‘‹#â„™) dğœˆ
=
d(â„™ âˆ˜ ğ‘‹âˆ’1) dğœˆ
and as such it is almost unique. Here ğ‘‹#â„™ = â„™ âˆ˜ ğ‘‹âˆ’1 is the push-forward measure of â„™.
Next, we define commonly used operators on random variables. If the ran- dom variable is continuous, then the reference measure ğœˆ is the Lebesgue measure and dğœˆ(ğ‘¥) is commonly replaced by dğ‘¥ to denote the Lebesgue measure.
Definition B.41 (moment). Suppose ğ‘‹ is a random variable with probability density function ğ‘“ğ‘‹ as in Definition B.40. Then
ğ‘€ğ‘˜(ğ‘‹,ğ‘) âˆ¶= âˆ«
ğ‘‹âˆ’1(Î¨)
(ğ‘¥ âˆ’ ğ‘)ğ‘˜dâ„™(ğ‘¥) = âˆ« Î¨
(ğ‘¥ âˆ’ ğ‘)ğ‘˜ğ‘“ğ‘‹(ğ‘¥)dğœˆ(ğ‘¥)
is called the ğ‘˜-th moment of the random variable ğ‘‹ about the center ğ‘.
B.7.
Inequalities
Definition B.42 (mean / expected value / expectance). The expected value or expectance of a random variable ğ‘‹ is its first moment about the center zero, i.e.,
ğ”¼[ğ‘‹] âˆ¶= ğ‘€1(ğ‘‹,0) âˆ¶= âˆ«
ğ‘‹âˆ’1(Î¨)
ğ‘¥dâ„™(ğ‘¥) = âˆ« Î¨
ğ‘¥ğ‘“ğ‘‹(ğ‘¥)dğœˆ(ğ‘¥).
Definition B.43 (variance, standard deviation). The variance of a random variable ğ‘‹ is its second moment about the expected value, i.e.,
ğ•[ğ‘‹] âˆ¶= ğ‘€2(ğ‘‹,ğ”¼[ğ‘‹]) = ğ”¼[(ğ‘‹ âˆ’ ğ”¼[ğ‘‹])2] = âˆ« Î¨
(ğ‘¥ âˆ’ ğ”¼[ğ‘‹])2ğ‘“ğ‘‹(ğ‘¥)dğœˆ(ğ‘¥).
The standard deviation is the square root of the variance, i.e.,
ğœ[ğ‘‹] âˆ¶= âˆšğ•[ğ‘‹].
The formulae above can also expediently be written using Riemann-Stieltjes integrals in terms of the probability density function and the cumulative distri- bution function. The equality
âˆ«ğ‘”(ğ‘¥)dğ¹ğ‘‹(ğ‘¥) = âˆ«ğ‘”(ğ‘¥)ğ‘“ğ‘‹(ğ‘¥)dğ‘¥
holds for Riemann-Stieltjes integrals, if the involved functions are smooth enough such that all integrals exist. For example, we hence have
ğ‘€ğ‘˜(ğ‘‹,ğ‘) = âˆ«(ğ‘¥ âˆ’ ğ‘)ğ‘˜dğ¹ğ‘‹(ğ‘¥),
ğ”¼[ğ‘‹] = âˆ«ğ‘¥dğ¹ğ‘‹(ğ‘¥),
ğ•[ğ‘‹] = âˆ«(ğ‘¥ âˆ’ ğ”¼[ğ‘‹])2dğ¹ğ‘‹(ğ‘¥).
B.7 Inequalities
In this section, important inequalities connected to measure and probability theory are collected.
B.7.1 Basic Inequalities
Jensenâ€™s inequality is an important inequality, whose measure-theoretic form is stated in the following theorem.
173
174
Appendix B. Measure and Probability Theory
Definition B.44 (convex set). A set ğ¶ is called convex if
âˆ€(ğ‘¥,ğ‘¦) âˆˆ ğ¶2âˆ¶
âˆ€ğ‘¦ âˆˆ ğ¶âˆ¶
âˆ€ğ›¼ âˆˆ [0,1]âˆ¶
ğ›¼ğ‘¥ + (1 âˆ’ ğ›¼)ğ‘¦ âˆˆ ğ¶.
Definition B.45 ((strictly) convex and concave functions). Suppose ğ¶ is a convex set. A function ğ‘“ âˆ¶ ğ¶ â†’ â„ is called convex if
âˆ€(ğ‘¥,ğ‘¦) âˆˆ ğ¶2âˆ¶
âˆ€ğ›¼ âˆˆ [0,1]âˆ¶
ğ‘“(ğ›¼ğ‘¥ + (1 âˆ’ ğ›¼)ğ‘¦) â‰¤ ğ›¼ğ‘“(ğ‘¥) + (1 âˆ’ ğ›¼)ğ‘“(ğ‘¦).
It is called strictly convex if it satisfies this property with â‰¤ replaced by <. It is called (strictly) concave if âˆ’ğ‘“ is (strictly) convex.
Definition B.46 (subderivative). A subderivative of a convex function ğ‘“ âˆ¶ ğ¼ â†’ â„ at a point ğ‘¥0 âˆˆ ğ¼ in the open interval ğ¼ is a real number ğ‘ âˆˆ â„ such that
âˆ€ğ‘¥ âˆˆ ğ¼ âˆ¶
ğ‘“(ğ‘¥) âˆ’ ğ‘“(ğ‘¥0) â‰¥ ğ‘(ğ‘¥ âˆ’ ğ‘¥0).
Lemma B.47 (subderivatives). Suppose ğ‘“ âˆ¶ ğ¼ â†’ â„ is a convex function on an open interval ğ¼. Then the set of subderivatives at a point ğ‘¥0 âˆˆ ğ¼ is the non-empty closed interval
[ lim
ğ‘¥â†’ğ‘¥0âˆ’
ğ‘“(ğ‘¥) âˆ’ ğ‘“(ğ‘¥0) ğ‘¥ âˆ’ ğ‘¥0
,
lim ğ‘¥â†’ğ‘¥0+
ğ‘“(ğ‘¥) âˆ’ ğ‘“(ğ‘¥0) ğ‘¥ âˆ’ ğ‘¥0
].
Theorem B.48 (Jensenâ€™s inequality). Suppose that (Î©,â„±,ğœ‡) is a measure space, that ğ¼ âŠ‚ â„ is an interval, that the function ğ‘”âˆ¶ Î© â†’ ğ¼ is Lebesgue integrable with respect to ğœ‡, and that the function ğœ™âˆ¶ ğ¼ â†’ â„ is convex on the interval ğ¼. Then the inequality
ğœ™(
1 ğœ‡(Î©)
âˆ« Î©
ğ‘”dğœ‡) â‰¤
1 ğœ‡(Î©)
âˆ« Î©
ğœ™ âˆ˜ ğ‘”dğœ‡
holds. If the function ğœ™ is concave, the inequality is reversed.
If the space is a probability space (Î©,â„±,â„™), the inequality is commonly
written as
ğœ™(ğ”¼[ğ‘‹]) â‰¤ ğ”¼[ğœ™(ğ‘‹)].
Proof. We start by defining the point
ğ‘¥0 âˆ¶=
1 ğœ‡(Î©)
âˆ« Î©
ğ‘”dğœ‡ âˆˆ ğ¼.
Next, by the convexity of ğœ™ and by Lemma B.47, there exists a ğ‘ âˆˆ â„ for the point ğ‘¥0 âˆˆ ğ¼ such that
âˆ€ğ‘¥ âˆˆ ğ¼ âˆ¶
ğœ™(ğ‘¥) âˆ’ ğœ™(ğ‘¥0) â‰¥ ğ‘(ğ‘¥ âˆ’ ğ‘¥0).
B.7.
Inequalities
175
Since this inequality holds for all ğ‘¥ âˆˆ ğ¼, it also holds for all ğ‘¥ = ğ‘”(ğœ”) âˆˆ ğ¼, i.e.,
âˆ€ğœ” âˆˆ Î©âˆ¶
ğœ™(ğ‘”(ğœ”)) â‰¥ ğœ™(ğ‘¥0) + ğ‘(ğ‘”(ğœ”) âˆ’ ğ‘¥0).
Using the monotony of the integral, integration of both sides yields
âˆ« Î©
ğœ™ âˆ˜ ğ‘”dğœ‡ â‰¥ ğœ‡(Î©)ğœ™(ğ‘¥0) + ğ‘(âˆ« Î©
ğ‘”dğœ‡ âˆ’ ğœ‡(Î©)ğ‘¥0) = ğœ‡(Î©)ğœ™(ğ‘¥0).
If the function ğœ™ is concave, the analogous argument shows the reversed inequal- ity, which concludes the proof.
B.7.2 Concentration Inequalities
Concentration inequalities provide probability bounds on how much a random variable deviates from its expected value.
The Markov and Chebyshev Inequalities
Theorem B.49 (Markov inequality). Suppose ğ‘‹ is a non-negative random variable and ğ‘ âˆˆ â„+. Then the inequality
â„™[ğ‘‹ â‰¥ ğ‘] â‰¤
ğ”¼[ğ‘‹] ğ‘
(B.5)
holds.
Proof. Since ğ‘‹ is non-negative random variable, its expected value can be writ- ten as
âˆ
ğ”¼[ğ‘‹] = âˆ« 0
ğ‘¥ğ‘“ğ‘‹(ğ‘¥)dğ‘¥.
By splitting the interval at ğ‘ âˆˆ â„+, we can estimate
ğ‘
âˆ
ğ”¼[ğ‘‹] = âˆ« 0
ğ‘¥ğ‘“ğ‘‹(ğ‘¥)dğ‘¥ + âˆ« ğ‘
ğ‘¥ğ‘“ğ‘‹(ğ‘¥)dğ‘¥
âˆ
âˆ
â‰¥ âˆ« ğ‘
ğ‘¥ğ‘“ğ‘‹(ğ‘¥)dğ‘¥ â‰¥ ğ‘âˆ« ğ‘
ğ‘“ğ‘‹(ğ‘¥)dğ‘¥ = ğ‘â„™[ğ‘‹ â‰¥ ğ‘],
which concludes the proof.
Corollary B.50. Inequality (B.5) is equivalent to
â„™[ğ‘‹ â‰¤
ğ”¼[ğ‘‹] ğ›¿
] â‰¥ 1 âˆ’ ğ›¿.
176
Appendix B. Measure and Probability Theory
Proof. Inequality (B.5) is equivalent to â„™[ğ‘‹ â‰¤ ğ‘] = 1âˆ’ğ”¼[ğ‘‹ â‰¥ ğ‘] â‰¥ 1âˆ’ğ”¼[ğ‘‹]/ğ‘ = 1 âˆ’ ğ›¿, where we have defined ğ›¿ âˆ¶= ğ”¼[ğ‘‹]/ğ‘.
Theorem B.51 (Chebyshev inequality). Suppose ğ‘‹ is an integrable random variable with expected value ğœ‡ âˆ¶= ğ”¼[ğ‘¥] and finite, non-zero variance ğœ2 âˆ¶= ğ•[ğ‘‹] âˆˆ (0,âˆ). Then the inequality
âˆ€ğ‘˜ âˆˆ â„+âˆ¶
â„™[|ğ‘‹ âˆ’ ğœ‡| â‰¥ ğ‘˜ğœ] â‰¤
1 ğ‘˜2
holds.
Proof. The inequality can be shown by applying Markovâ€™s inequality to the random variable (ğ‘‹âˆ’ğœ‡)2 and the constant ğ‘ âˆ¶= (ğ‘˜ğœ)2 in Markovâ€™s inequality.
Hoeffdingâ€™s Inequality
In [64], variants of Hoeffdingâ€™s inequality were shown. We start by proving Hoeffdingâ€™s lemma before stating Hoeffdingâ€™s inequalities and discussing how they can be applied.
Lemma B.52 (Hoeffdingâ€™s lemma). Suppose that ğ‘‹ is a real valued random variable such that
âˆƒ(ğ‘,ğ‘) âˆˆ â„2âˆ¶
â„™[ğ‘ â‰¤ ğ‘‹ â‰¤ ğ‘] = 1.
Then the inequality
âˆ€ğœ† âˆˆ â„âˆ¶
ğ”¼[e
ğœ†(ğ‘‹âˆ’ğ”¼[ğ‘‹])
] â‰¤ e
ğœ†2(ğ‘âˆ’ğ‘)2/8
holds.
Proof. If ğ‘ = ğ‘, then the inequality simply becomes ğ”¼[1] â‰¤ ğ”¼[1].
Otherwise, if ğ‘ < ğ‘, since the function ğ‘¥ â†¦ e
ğœ†(ğ‘¥âˆ’ğ”¼[ğ‘‹])
is convex, the inequality
âˆ€ğ‘¥ âˆˆ [ğ‘,ğ‘]âˆ¶
ğœ†(ğ‘¥âˆ’ğ”¼[ğ‘‹]) e
â‰¤
ğ‘ âˆ’ ğ‘¥ ğ‘ âˆ’ ğ‘
ğœ†(ğ‘âˆ’ğ”¼[ğ‘‹]) e
+
ğ‘¥ âˆ’ ğ‘ ğ‘ âˆ’ ğ‘
e
ğœ†(ğ‘âˆ’ğ”¼[ğ‘‹])
holds. Applying the expected value to both sides yields
ğ”¼[e
ğœ†(ğ‘‹âˆ’ğ”¼[ğ‘‹])
] â‰¤
ğ‘ âˆ’ ğ”¼[ğ‘‹] ğ‘ âˆ’ ğ‘
= (1 âˆ’ ğ›¼)e
e
ğœ†(ğ‘âˆ’ğ”¼[ğ‘‹])
+
ğœ†(ğ‘âˆ’ğ”¼[ğ‘‹])
+ ğ›¼e
ğ”¼[ğ‘‹] âˆ’ ğ‘ ğ‘ âˆ’ ğ‘ ğœ†(ğ‘âˆ’ğ”¼[ğ‘‹])
,
ğœ†(ğ‘âˆ’ğ”¼[ğ‘‹]) e
where
ğ›¼ âˆ¶=
ğ”¼[ğ‘‹] âˆ’ ğ‘ ğ‘ âˆ’ ğ‘
.
B.7.
Inequalities
177
Using ğ‘¦ âˆ¶= ğœ†(ğ‘ âˆ’ ğ‘), we have
ğ”¼[e
ğœ†(ğ‘‹âˆ’ğ”¼[ğ‘‹])
] â‰¤ (1 âˆ’ ğ›¼ + ğ›¼e
ğœ†(ğ‘âˆ’ğ‘)
)e
ğœ†(ğ‘âˆ’ğ”¼[ğ‘‹])
ğœ†(ğ‘âˆ’ğ‘)
= (1 âˆ’ ğ›¼ + ğ›¼e )e = (1 âˆ’ ğ›¼ + ğ›¼eğ‘¦)eâˆ’ğ›¼ğ‘¦.
âˆ’ğ›¼ğœ†(ğ‘âˆ’ğ‘)
Defining the function
ğ‘“ âˆ¶ â„ â†’ â„,
ğ‘¦ â†¦ ln(1 âˆ’ ğ›¼ + ğ›¼eğ‘¦) âˆ’ ğ›¼ğ‘¦,
the inequality becomes
ğ”¼[e
ğœ†(ğ‘‹âˆ’ğ”¼[ğ‘‹])
] â‰¤ e
ğ‘“(ğ‘¦)
.
The function ğ‘“ is well-defined, since
1 âˆ’ ğ›¼ + ğ›¼eğ‘¦ = ğ›¼(
1 ğ›¼
âˆ’ 1 + eğ‘¦) = ğ›¼(
ğ‘ âˆ’ ğ”¼[ğ‘‹] ğ”¼[ğ‘‹] âˆ’ ğ‘
+ eğ‘¦) > 0
because of ğ‘ â‰¤ ğ”¼[ğ‘‹] â‰¤ ğ‘ by the assumption â„™[ğ‘ â‰¤ ğ‘‹ â‰¤ ğ‘] = 1.
It remains to find an upper bound for ğ‘“. Since ğ‘“ is sufficiently smooth,
Taylorâ€™s theorem yields
âˆ€ğ‘¦ âˆˆ â„âˆ¶
âˆƒğ‘§ âˆˆ [0,ğ‘¦]âˆ¶
ğ‘“(ğ‘¦) = ğ‘“(0) + ğ‘“â€²(0)ğ‘¦ +
ğ‘“â€³(ğ‘§) 2
ğ‘¦2.
Because of
ğ‘“â€²(ğ‘¦) =
ğ›¼eğ‘¦
1 âˆ’ ğ›¼ + ğ›¼eğ‘¦ âˆ’ ğ›¼,
ğ‘“â€³(ğ‘¦) =
ğ›¼eğ‘¦
1 âˆ’ ğ›¼ + ğ›¼eğ‘¦ (1 âˆ’
ğ›¼eğ‘¦
1 âˆ’ ğ›¼ + ğ›¼eğ‘¦) = ğ‘¢(1 âˆ’ ğ‘¢),
ğ‘¢ âˆ¶=
ğ›¼eğ‘¦ 1 âˆ’ ğ›¼ + ğ›¼eğ‘¦,
we have ğ‘“(0) = 0, ğ‘“â€²(0) = 0, and ğ‘“â€³(ğ‘§) â‰¤ 1/4 due to ğ‘¢(1 âˆ’ ğ‘¢) â‰¤ 1/4 for all ğ‘¢ âˆˆ â„. Therefore the estimate
ğ‘“(ğ‘¦) â‰¤
ğ‘¦2 8
=
ğœ†2(ğ‘ âˆ’ ğ‘)2 8
holds, which completes the proof.
Theorem B.53 (Hoeffdingâ€™s inequality). Suppose that {ğ‘‹ğ‘–}ğ‘› real valued random variables, each being bounded such that
ğ‘–=1 are independent
âˆ€ğ‘– âˆˆ [1âˆ¶ğ‘›]âˆ¶
âˆƒ(ğ‘ğ‘–,ğ‘ğ‘–) âˆˆ â„2âˆ¶
â„™[ğ‘ğ‘– â‰¤ ğ‘‹ğ‘– â‰¤ ğ‘ğ‘–] = 1.
178
Appendix B. Measure and Probability Theory
Then the inequalities
â„™[ğ‘‹ âˆ’ ğ”¼[ğ‘‹] â‰¥ ğ‘¡] â‰¤ exp(âˆ’
â„™[|ğ‘‹ âˆ’ ğ”¼[ğ‘‹]| â‰¥ ğ‘¡] â‰¤ 2exp(âˆ’
2ğ‘›2ğ‘¡2 ğ‘–=1(ğ‘ğ‘– âˆ’ ğ‘ğ‘–)2 2ğ‘›2ğ‘¡2 ğ‘–=1(ğ‘ğ‘– âˆ’ ğ‘ğ‘–)2
âˆ‘ğ‘›
âˆ‘ğ‘›
),
),
ğ‘‹ âˆ¶=
1 ğ‘›
ğ‘› âˆ‘ ğ‘–=1
ğ‘‹ğ‘–
hold for all ğ‘¡ âˆˆ â„+.
These inequalities also hold when the random variables {ğ‘‹ğ‘–}ğ‘›
ğ‘–=1 are obtained by sampling without replacement [64]. Better bounds for this case can be found in [65].
Proof. We first define
ğ‘› âˆ‘ ğ‘–=1 Using an arbitrary parameter ğœ† âˆˆ â„+ and Markovâ€™s inequality, Theorem B.49, we find
ğ‘† âˆ¶=
ğ‘‹ğ‘–.
â„™[ğ‘‹ âˆ’ ğ”¼[ğ‘‹] â‰¥ ğ‘¡] = â„™[ğ‘† âˆ’ ğ”¼[ğ‘†] â‰¥ ğ‘›ğ‘¡] = â„™[e
ğœ†(ğ‘†âˆ’ğ”¼[ğ‘†])
â‰¥ eğœ†ğ‘›ğ‘¡] â‰¤
ğ”¼[e
ğœ†(ğ‘†âˆ’ğ”¼[ğ‘†])
eğœ†ğ‘›ğ‘¡
]
.
Since the random variables ğ‘‹ğ‘– are independent, the inequality
â„™[ğ‘‹ âˆ’ ğ”¼[ğ‘‹] â‰¥ ğ‘¡] â‰¤ eâˆ’ğœ†ğ‘›ğ‘¡
ğ‘› âˆ ğ‘–=1
ğ”¼[e
ğœ†(ğ‘‹ğ‘–âˆ’ğ”¼[ğ‘‹ğ‘–])
]
(B.6)
holds. Using Hoeffdingâ€™s lemma, Lemma B.52, we can estimate the right-hand side to obtain
â„™[ğ‘‹ âˆ’ ğ”¼[ğ‘‹] â‰¥ ğ‘¡] â‰¤ eâˆ’ğœ†ğ‘›ğ‘¡
ğ‘› âˆ ğ‘–=1
e
ğœ†2(ğ‘ğ‘–âˆ’ğ‘ğ‘–)2/8
= exp(âˆ’ğœ†ğ‘›ğ‘¡ +
ğœ†2 8
ğ‘› âˆ‘ ğ‘–=1
(ğ‘ğ‘– âˆ’ ğ‘ğ‘–)2).
We can now use the parameter ğœ† âˆˆ â„+ to find the best possible upper bound. Because the right-hand side is a quadratic function of ğœ†, it is straightforward to calculate that it achieves its global minimum at
ğœ† âˆ¶=
4ğ‘›ğ‘¡ ğ‘–=1(ğ‘ğ‘– âˆ’ ğ‘ğ‘–)2
âˆ‘ğ‘›
> 0,
B.7.
Inequalities
which is positive since ğ‘¡ > 0. This value for ğœ† yields
â„™[ğ‘‹ âˆ’ ğ”¼[ğ‘‹] â‰¥ ğ‘¡] â‰¤ exp(âˆ’
2ğ‘›2ğ‘¡2 ğ‘–=1(ğ‘ğ‘– âˆ’ ğ‘ğ‘–)2
âˆ‘ğ‘›
),
which proves the first inequality.
The see the second one, we calculate
â„™[|ğ‘‹ âˆ’ ğ”¼[ğ‘‹]| â‰¥ ğ‘¡] = â„™[ğ‘‹ âˆ’ ğ”¼[ğ‘‹] â‰¥ ğ‘¡] + â„™[ğ‘‹ âˆ’ ğ”¼[ğ‘‹] â‰¤ âˆ’ğ‘¡]
= â„™[ğ‘‹ âˆ’ ğ”¼[ğ‘‹] â‰¥ ğ‘¡] + â„™[âˆ’ğ‘‹ + ğ”¼[ğ‘‹] â‰¥ ğ‘¡].
The first term is bounded by the first inequality. The second term is also bounded by the first inequality, but now applied to the random variables ğ‘Œğ‘– âˆ¶= âˆ’ğ‘‹ğ‘– and noting that the assumption â„™[âˆ’ğ‘ğ‘– â‰¤ ğ‘Œğ‘– â‰¤ âˆ’ğ‘ğ‘–] = 1 also results in the sum âˆ‘ğ‘›
ğ‘–=1(ğ‘ğ‘– âˆ’ ğ‘ğ‘–)2, which concludes the proof.
Corollary B.54 (Hoeffdingâ€™s inequality for sums). Suppose the assumptions of Theorem B.53 hold. Then the inequalities
â„™[ğ‘† âˆ’ ğ”¼[ğ‘†] â‰¥ ğ‘¡] â‰¤ exp(âˆ’
â„™[|ğ‘† âˆ’ ğ”¼[ğ‘†]| â‰¥ ğ‘¡] â‰¤ 2exp(âˆ’
2ğ‘¡2 ğ‘–=1(ğ‘ğ‘– âˆ’ ğ‘ğ‘–)2 2ğ‘¡2 ğ‘–=1(ğ‘ğ‘– âˆ’ ğ‘ğ‘–)2
âˆ‘ğ‘›
âˆ‘ğ‘›
),
),
ğ‘† âˆ¶=
ğ‘› âˆ‘ ğ‘–=1
ğ‘‹ğ‘–,
hold for all ğ‘¡ âˆˆ â„+.
Proof. Theorem B.53 can be applied to
â„™[ğ‘† âˆ’ ğ”¼[ğ‘†] â‰¥ ğ‘¡] = â„™[ğ‘›ğ‘‹ âˆ’ ğ‘›ğ”¼[ğ‘‹] â‰¥ ğ‘¡] = â„™[ğ‘‹ âˆ’ ğ”¼[ğ‘‹] â‰¥ ğ‘¡/ğ‘›]
since ğ‘¡/ğ‘› âˆˆ â„+.
Corollary B.55 (Hoeffdingâ€™s inequality for confidence intervals). Suppose the assumptions of Theorem B.53 hold. Then the inequalities
â„™â¡ â¢ â£
ğ”¼[ğ‘‹] > ğ‘‹ âˆ’ âˆšâˆ’ln(ğ›¿)âˆ‘ğ‘›
ğ‘–=1(ğ‘ğ‘– âˆ’ ğ‘ğ‘–)2 2ğ‘›2
â¤ â¥ â¦
â‰¥ 1 âˆ’ ğ›¿,
179
(B.7a)
180
Appendix B. Measure and Probability Theory
â„™â¡ â¢ â£
|ğ”¼[ğ‘‹] âˆ’ ğ‘‹| < âˆšâˆ’ln(ğ›¿)âˆ‘ğ‘›
ğ‘–=1(ğ‘ğ‘– âˆ’ ğ‘ğ‘–)2 2ğ‘›2
â¤ â¥ â¦
â‰¥ 1 âˆ’ 2ğ›¿
(B.7b)
hold for all ğ›¿ âˆˆ (0,1).
The first inequality is often formulated as follows: with probability at least
1 âˆ’ ğ›¿, the inequality
ğ”¼[ğ‘‹] > ğ‘‹ âˆ’ âˆšâˆ’ln(ğ›¿)âˆ‘ğ‘›
ğ‘–=1(ğ‘ğ‘– âˆ’ ğ‘ğ‘–)2 2ğ‘›2
holds. The second inequality is equivalent to saying that the inequality
|ğ”¼[ğ‘‹] âˆ’ ğ‘‹| < âˆšâˆ’ln(ğ›¿)âˆ‘ğ‘›
ğ‘–=1(ğ‘ğ‘– âˆ’ ğ‘ğ‘–)2 2ğ‘›2
holds with probability at least 1 âˆ’ 2ğ›¿.
Proof. We start from the function
ğ›¿âˆ¶ â„+ â†’ (0,1),
ğ›¿(ğ‘¡) âˆ¶= exp(âˆ’
2ğ‘›2ğ‘¡2 ğ‘–=1(ğ‘ğ‘– âˆ’ ğ‘ğ‘–)2
âˆ‘ğ‘›
)
(B.8)
and its inverse
ğ‘¡âˆ¶
(0,1) â†’ â„+,
ğ‘¡(ğ›¿) âˆ¶= âˆšâˆ’ln(ğ›¿)âˆ‘ğ‘›
ğ‘–=1(ğ‘ğ‘– âˆ’ ğ‘ğ‘–)2 2ğ‘›2
(B.9)
and note that ğ›¿ is a (monotone decreasing) bÄ³ection between ğ‘¡ âˆˆ â„+ and ğ›¿ âˆˆ (0,1).
Using these definitions, the first inequality in Theorem B.53, where ğ‘¡ âˆˆ â„+
is assumed, becomes
â„™[ğ‘‹ âˆ’ ğ”¼[ğ‘‹] â‰¥ ğ‘¡] â‰¤ ğ›¿(ğ‘¡),
which is equivalent to
â„™[ğ”¼[ğ‘‹] > ğ‘‹ âˆ’ ğ‘¡] â‰¥ 1 âˆ’ ğ›¿(ğ‘¡)
after negation. This is the first inequality.
The second inequality in Theorem B.53 becomes
â„™[|ğ‘‹ âˆ’ ğ”¼[ğ‘‹]| â‰¥ ğ‘¡] â‰¤ 2ğ›¿(ğ‘¡),
B.7.
Inequalities
whose negation leads to
â„™[|ğ”¼[ğ‘‹] âˆ’ ğ‘‹| < ğ‘¡] â‰¥ 1 âˆ’ 2ğ›¿(ğ‘¡).
This is the second inequality, which completes the proof.
This corollary shows how Hoeffdingâ€™s inequality is used to calculate one-sided and two-sided confidence intervals. We are interested in the (true) expected value ğ”¼[ğ‘‹], but we can only empirically calculate the sample mean ğ‘‹. In ac- cordance with the assumptions of Theorem B.53, we assume that the random variables are independent or that sampling is performed without replacement (by the comment below the theorem). Then (B.7a) yields the one-sided confidence interval
(ğ‘‹ âˆ’ ğ‘¡(ğ›¿),âˆ)
at confidence level 1âˆ’ğ›¿ (usually close to one) for the true value ğ”¼[ğ‘‹]. Similarly, (B.7b) yields the (symmetric) two-sided confidence interval
(ğ‘‹ âˆ’ ğ‘¡(ğ›¿),ğ‘‹ + ğ‘¡(ğ›¿))
at the confidence level 1 âˆ’ 2ğ›¿ for the true value ğ”¼[ğ‘‹].
So far we have considered the number ğ‘› of random variables (and their bounds ğ‘ğ‘– and ğ‘ğ‘–) to be fixed. If we view them as variable, another way to interpret the inequalities in Corollary B.55 is to ask the question how many samples should be obtained in order to acquire a confidence interval of given size ğ‘¡0 (smaller ğ‘¡0 is better) and of given confidence level 1 âˆ’ ğ›¿0 (larger 1 âˆ’ ğ›¿0 is better, i.e., smaller ğ›¿0 is better).
To shorten the notation, we define
ğ‘š âˆ¶=
2ğ‘›2 ğ‘–=1(ğ‘ğ‘– âˆ’ ğ‘ğ‘–)2
âˆ‘ğ‘›
and note that in the important special case that all ğ‘ğ‘– are equal to a constant ğ‘ âˆˆ â„ and all ğ‘ğ‘– are equal to a constant ğ‘ âˆˆ â„, we have
ğ‘š =
2ğ‘›2 ğ‘›(ğ‘ âˆ’ ğ‘)2 =
2ğ‘› (ğ‘ âˆ’ ğ‘)2,
meaning that ğ‘š grows just as the number ğ‘› of sampled random variables in- creases.
To acquire a given one-sided confidence interval of given size ğ‘¡0 âˆˆ â„+ and of given confidence level 1 âˆ’ ğ›¿0 with ğ›¿0 âˆˆ (0,1), we first write ğ›¿(ğ‘¡,ğ‘š) and ğ‘¡(ğ›¿,ğ‘š) for the two functions defined in (B.8) and (B.9) to underline their dependence
181
182
Appendix B. Measure and Probability Theory
on ğ‘š. In the first case, we are given the confidence level 1 âˆ’ ğ›¿0 and would like to find values of ğ‘š that also satisfy a given size ğ‘¡0, i.e., we seek ğ‘š such that
ğ‘¡(ğ›¿0,ğ‘š) â‰¤ ğ‘¡0,
which is equivalent to
âˆ’lnğ›¿0 ğ‘¡2 0 In the second case, we are given a confidence interval size ğ‘¡0 and would like to find values of ğ‘š that also satisfy a given confidence level 1 âˆ’ ğ›¿0, i.e., we seek ğ‘š such that
ğ‘š â‰¥
> 0.
ğ›¿(ğ‘¡0,ğ‘š) â‰¤ ğ›¿0,
which is equivalent to
ğ‘š â‰¥
âˆ’lnğ›¿0 ğ‘¡2 0
> 0.
In both cases we arrive at the same condition for ğ‘š, and thus define
ğ‘šâˆ¶
(0,1) Ã— â„+ â†’ â„+, ğ‘š(ğ›¿,ğ‘¡) âˆ¶=
âˆ’lnğ›¿ ğ‘¡2
.
This condition for ğ‘š can interpreted in terms of the number ğ‘› of samples needed. If the size ğ‘¡0 of the confidence interval is to be reduced by a factor ğœ† (while the confidence level 1 âˆ’ ğ›¿0 is kept constant), ğ‘š and hence the number ğ‘› of samples scales quadratically since
ğ‘š(ğ›¿0,ğœ†ğ‘¡0) ğ‘š(ğ›¿0,ğ‘¡0)
=
1 ğœ†2.
The quadratic scaling is consistent with the law of large numbers and the central limit theorem (see Section B.11).
Similarly, if ğ›¿0 is to be reduced by a factor ğœ† (while the size ğ‘¡0 is kept
constant), ğ‘š and hence the number ğ‘› of samples scales as
ğ‘š(ğœ†ğ›¿0,ğ‘¡0) ğ‘š(ğ›¿0,ğ‘¡0)
= 1 +
lnğœ† lnğ›¿0
.
Bernsteinâ€™s Inequality
Bernstein-type inequalities date back to the 1920s and 1930s and are the oldest inequalities that give bounds on the probability how much a sum of random variables deviates from its mean. While Hoeffdingâ€™s inequality, which serves the same purpose, only supposes that the random variables are bounded, Bernstein inequalities also use the variance of the distribution to get tighter bounds. A typical inequality of the Bernstein type is the following one.
B.7.
Inequalities
183
Theorem B.56 (Bernsteinâ€™s inequality). Suppose {ğ‘‹ğ‘–}ğ‘› dom variables such that
ğ‘–=1 are independent ran-
âˆ€ğ‘– âˆˆ [1âˆ¶ğ‘›]âˆ¶
ğ”¼[ğ‘‹ğ‘–] = 0
and
âˆƒğ‘€ âˆˆ â„+âˆ¶
âˆ€ğ‘– âˆˆ [1âˆ¶ğ‘›]âˆ¶
â„™[|ğ‘‹ğ‘–| â‰¤ ğ‘€] = 1.
Then the inequality
â„™[ğ‘‹ â‰¥ ğ‘¡] â‰¤ exp(âˆ’
ğ‘›ğ‘¡2 2(ğ‘€ğ‘¡/3 + ğ‘›ğœ2)
),
ğ‘‹ âˆ¶=
1 ğ‘›
ğ‘› âˆ‘ ğ‘–=1
ğ‘‹ğ‘–,
ğœ2 âˆ¶= ğ•[ğ‘‹] =
1 ğ‘›2
ğ‘› âˆ‘ ğ‘–=1
ğ•[ğ‘‹ğ‘–] =
1 ğ‘›2
ğ‘› âˆ‘ ğ‘–=1
ğ”¼[ğ‘‹2
ğ‘– ],
holds for all ğ‘¡ âˆˆ â„+.
Proof. Since the assumptions of Hoeffdingâ€™s inequality, Theorem B.53, are sat- ğœ†ğ‘‹ğ‘–], to show isfied, we will be able to use (B.6), which contains the terms ğ”¼[e the proposed inequality.
ğœ†ğ‘‹ğ‘–]. We start by using the Taylor expansion of the exponential function and the assumption that ğ”¼[ğ‘‹ğ‘–] = 0 for all ğ‘– âˆˆ [1âˆ¶ğ‘›] to find
Before we do so, we estimate the terms ğ”¼[e
âˆ€ğœ† âˆˆ â„âˆ¶
ğ”¼[e
ğœ†ğ‘‹ğ‘–] = 1 +
âˆ âˆ‘ ğ‘˜=2
ğœ†ğ‘˜ğ”¼[ğ‘‹ğ‘˜ ğ‘– ] ğ‘˜!
for all random variables indexed by ğ‘– âˆˆ [1âˆ¶ğ‘›]. We define the infinite sum
ğ‘“ âˆ¶ â„ â†’ â„, ğœ† â†¦
âˆ âˆ‘ ğ‘˜=2
ğœ†ğ‘˜âˆ’2ğ”¼[ğ‘‹ğ‘˜ ğ‘– ] ğœ2 ğ‘– ğ‘˜!
,
ğ‘– âˆ¶= ğ•[ğ‘‹ğ‘–] = ğ”¼[ğ‘‹2 ğœ2 ğ‘– ]
and note that it converges for all ğœ† âˆˆ â„, which can easily be seen by the ratio test.
Using the infinite sum ğ‘“, we can write
âˆ€ğœ† âˆˆ â„âˆ¶
ğ”¼[e
ğœ†ğ‘‹ğ‘–] = 1 + ğœ†2ğœ2
ğ‘– ğ‘“(ğœ†).
We now use the inequality 1+ğ‘¥ â‰¤ eğ‘¥ for all ğ‘¥ âˆˆ â„, which can be proved by using the starting point ğ‘¥ = 0 (for both intervals [0,âˆ) and (âˆ’âˆ,0]) and showing
184
Appendix B. Measure and Probability Theory
the differentiated inequality. By integration, the inequality follows from the differentiated one. This inequality yields
âˆ€ğœ† âˆˆ â„âˆ¶
ğ”¼[e
ğœ†ğ‘‹ğ‘–] â‰¤ e
ğœ†2ğœ2
ğ‘–ğ‘“(ğœ†)
.
(B.10)
Next, we consider the terms ğ”¼[ğ‘‹ğ‘˜
ğ‘– ] in the infinite sum ğ‘“. The Cauchy-
Schwarz inequality yields
1/2
1/2
ğ”¼[ğ‘‹ğ‘˜
ğ‘– ] = âˆ«ğ‘¥ğ‘–ğ‘¥ğ‘˜âˆ’1
ğ‘–
dâ„™(ğ‘¥ğ‘–) â‰¤ (âˆ«|ğ‘¥ğ‘–|2dâ„™(ğ‘¥ğ‘–))
(âˆ«|ğ‘¥ğ‘˜âˆ’1
ğ‘–
|2dâ„™(ğ‘¥ğ‘–))
1/2
= ğœğ‘– (âˆ«|ğ‘¥ğ‘–|2ğ‘˜âˆ’2dâ„™(ğ‘¥ğ‘–))
.
We continue to apply the Cauchy-Schwarz inequality to the last factor recur- sively. Each application of the Cauchy-Schwarz inequality has the form
1/2
1/2
âˆ«ğ‘¥ğ›¼
ğ‘– dâ„™(ğ‘¥ğ‘–) = âˆ«ğ‘¥ğ‘–ğ‘¥ğ›¼âˆ’1
ğ‘–
dâ„™(ğ‘¥ğ‘–) â‰¤ (âˆ«|ğ‘¥ğ‘–|2dâ„™(ğ‘¥ğ‘–))
(âˆ«|ğ‘¥ğ›¼âˆ’1
ğ‘–
|2dâ„™(ğ‘¥ğ‘–))
1/2
= ğœğ‘– (âˆ«|ğ‘¥ğ‘–|2(ğ›¼âˆ’1)dâ„™(ğ‘¥ğ‘–))
.
Therefore the exponents of |ğ‘¥ğ‘–| satisfy the recursion
ğ‘1 âˆ¶= 2ğ‘˜ âˆ’ 2,
ğ‘ğ‘š+1 = 2(ğ‘ğ‘š âˆ’ 1).
It is straightforward to show by induction that
ğ‘ğ‘š = 2ğ‘šğ‘˜ âˆ’ 2ğ‘š+1 + 2.
In summary, continuing to apply the Cauchy-Schwarz inequality recursively ğ‘š times in total, each time splitting off a term |ğ‘¥ğ‘–| in the last factor, results in
âˆ€ğ‘š âˆˆ â„•âˆ¶
ğ‘– ] â‰¤ ğœ1+1/2+â‹¯+(1/2)ğ‘šâˆ’1
ğ”¼[ğ‘‹ğ‘˜
ğ‘–
(âˆ«|ğ‘¥ğ‘–|2ğ‘šğ‘˜âˆ’2ğ‘š+1+2dâ„™(ğ‘¥ğ‘–))
1/2ğ‘š
= ğœ2(1âˆ’1/2ğ‘š)
ğ‘–
(âˆ«|ğ‘¥ğ‘–|2ğ‘šğ‘˜âˆ’2ğ‘š+1+2dâ„™(ğ‘¥ğ‘–))
1/2ğ‘š
.
By assumption, the absolute values of the random variables ğ‘‹ğ‘– are bounded by the constant ğ‘€ with probability one. Therefore the last factor can be bounded by
1/2ğ‘š
(âˆ«|ğ‘¥ğ‘–|2ğ‘šğ‘˜âˆ’2ğ‘š+1+2dâ„™(ğ‘¥ğ‘–))
â‰¤ (ğ‘€2ğ‘šğ‘˜âˆ’2ğ‘š+1+2)1/2ğ‘š,
B.7.
Inequalities
185
which leads to
ğ‘– ] â‰¤ ğœ2(1âˆ’1/2ğ‘š)
ğ”¼[ğ‘‹ğ‘˜
ğ‘–
ğ‘€ğ‘˜âˆ’2+1/2ğ‘šâˆ’1.
Taking the limit ğ‘š â†’ âˆ yields
ğ”¼[ğ‘‹ğ‘˜
ğ‘– ] â‰¤ lim ğ‘šâ†’âˆ
ğœ2(1âˆ’1/2ğ‘š) ğ‘–
(ğ‘€ğ‘˜âˆ’2+1/2ğ‘šâˆ’1) = ğœ2
ğ‘– ğ‘€ğ‘˜âˆ’2.
Therefore, the infinite sum can be bounded above by
âˆ€ğœ† âˆˆ â„+ 0 âˆ¶
ğ‘“(ğœ†) =
âˆ âˆ‘ ğ‘˜=2
ğœ†ğ‘˜âˆ’2ğ”¼[ğ‘‹ğ‘˜ ğ‘– ] ğœ2 ğ‘– ğ‘˜!
â‰¤
=
âˆ âˆ‘ ğ‘˜=2 1
ğ‘– ğ‘€ğ‘˜âˆ’2 ğœ†ğ‘˜âˆ’2ğœ2 ğœ2 ğ‘– ğ‘˜! ğœ†2ğ‘€2(eğœ†ğ‘€ âˆ’ 1 âˆ’ ğœ†ğ‘€)
1 ğœ†2ğ‘€2
=
âˆ âˆ‘ ğ‘˜=2
ğœ†ğ‘˜ğ‘€ğ‘˜ ğ‘˜!
if the parameter ğœ† is non-negative. Applying this estimate to (B.10) yields
âˆ€ğœ† âˆˆ â„+ 0 âˆ¶
ğ”¼[e
ğœ†ğ‘‹ğ‘–] â‰¤ exp(ğœ†2ğœ2 ğ‘–
1
ğœ†2ğ‘€2(eğœ†ğ‘€ âˆ’ 1 âˆ’ ğœ†ğ‘€)).
Next, we use inequality (B.6) as alluded to in the beginning and the assump-
tions on the random variables ğ‘‹ğ‘– (that imply âˆ‘ğ‘›
ğ‘–=1 ğœ2
ğ‘– = ğ‘›2ğœ2) to find
âˆ€ğœ† âˆˆ â„+ 0 âˆ¶
âˆ€ğ‘¡ âˆˆ â„+âˆ¶
â„™[ğ‘‹ â‰¥ ğ‘¡] â‰¤ eâˆ’ğœ†ğ‘›ğ‘¡
ğ‘› âˆ ğ‘–=1
exp(ğœ†2ğœ2 ğ‘–
1
ğœ†2ğ‘€2(eğœ†ğ‘€ âˆ’ 1 âˆ’ ğœ†ğ‘€))
= exp(âˆ’ğœ†ğ‘›ğ‘¡ +
ğ‘›2ğœ2 ğ‘€2 (eğœ†ğ‘€ âˆ’ 1 âˆ’ ğœ†ğ‘€)).
As in the proof of Hoeffdingâ€™s inequality, we now minimize the right-hand side 0 to find the best (i.e., smallest)
with respect to the unknown parameter ğœ† âˆˆ â„+ upper bound. The first derivative of the right-hand side ğ‘Ÿ is
ğ‘Ÿâ€²(ğœ†) = (âˆ’ğ‘›ğ‘¡ +
ğ‘›2ğœ2 ğ‘€
eğœ†ğ‘€ âˆ’
ğ‘›2ğœ2 ğ‘€
)exp(âˆ’ğœ†ğ‘›ğ‘¡ +
ğ‘›2ğœ2 ğ‘€2 (eğœ†ğ‘€ âˆ’ 1 âˆ’ ğœ†ğ‘€)),
which vanishes only for
ğœ†min âˆ¶=
1 ğ‘€
ln(
ğ‘¡ğ‘€ ğ‘›ğœ2 + 1) âˆˆ â„+.
The second derivative ğ‘Ÿâ€³(ğœ†min) > 0 is positive at this point. Furthermore, ğ‘Ÿâ€²(0) < 0 and limğœ†â†’âˆ ğ‘Ÿ(ğœ†) = âˆ. Therefore ğœ†min is the global minimum.
With the abbreviation
ğ‘”âˆ¶ â„+ â†’ â„,
ğ‘”(ğ‘¥) âˆ¶= (1 + ğ‘¥)ln(1 + ğ‘¥) âˆ’ ğ‘¥,
186
Appendix B. Measure and Probability Theory
we have
âˆ€ğ‘¡ âˆˆ â„+âˆ¶
â„™[ğ‘‹ â‰¥ ğ‘¡] â‰¤ exp(âˆ’
ğ‘›2ğœ2 ğ‘€2 ğ‘” (
ğ‘¡ğ‘€ ğ‘›ğœ2)),
(B.11)
which is also called Bennettâ€™s inequality.
In the next step, ğ‘” is bounded below by
â„âˆ¶ â„+ â†’ â„,
â„(ğ‘¥) âˆ¶=
3 2
ğ‘¥2 ğ‘¥ + 3
,
i.e., we have
âˆ€ğ‘¥ âˆˆ â„+âˆ¶
â„(ğ‘¥) â‰¤ ğ‘”(ğ‘¥).
This inequality is shown by differentiating both sides twice and checking the starting point ğ‘¥ = 0. More precisely, in other words, we have ğ‘”(0) = 0 = â„(0), ğ‘”â€²(0) = 0 = â„â€²(0), and
27 (ğ‘¥ + 3)3 â‰¤ which implies the inequality by integrating twice. Applying this last inequality to (B.11) yields
âˆ€ğ‘¥ âˆˆ â„+âˆ¶
â„â€³(ğ‘¥) =
1 ğ‘¥ + 1
= ğ‘”â€³(ğ‘¥),
âˆ€ğ‘¡ âˆˆ â„+âˆ¶
â„™[ğ‘‹ â‰¥ ğ‘¡] â‰¤ exp(âˆ’
ğ‘›2ğœ2 ğ‘€2 â„(
ğ‘¡ğ‘€ ğ‘›ğœ2)) = exp(
âˆ’ğ‘›ğ‘¡2 2(ğ‘€ğ‘¡/3 + ğ‘›ğœ2)
),
which concludes the proof.
The last part of the proof, going from (B.11) to the final inequality, serves two purposes. First, it serves a cosmetic purpose, as the structure of the final inequality is much simpler than the one of (B.11). Second, and closely related to the cosmetic appeal, the scaling as the number ğ‘› of random variables is increased and the influences of the bound ğ‘€ and the variance ğœ2 can be discussed more easily in the final inequality.
On the other hand, the final inequality is less strict than (B.11). Therefore
is better suited to obtain numerical bounds of the mean value ğ‘‹.
Empirical Bernsteinâ€™s Inequality
Anderson Inequality
B.8 Characteristic Functions
Definition B.57 (characteristic function). The characteristic function ğœ™ğ‘‹ of a random variable ğ‘‹ is the expected value of eiğ‘¡ğ‘‹, i.e.,
ğœ™ğ‘‹âˆ¶ â„ â†’ â„‚, ğœ™ğ‘‹(ğ‘¡) âˆ¶= ğ”¼[eiğ‘¡ğ‘‹] = âˆ« â„
eiğ‘¡ğ‘¥dğ¹ğ‘‹(ğ‘¥) = âˆ« â„
eiğ‘¡ğ‘¥ğ‘“ğ‘‹(ğ‘¥)dğ‘¥,
B.9. Types of Convergence
where ğ‘“ğ‘‹ is the probability density function of ğ‘‹ and ğ¹ğ‘‹ its cumulative distri- bution function.
If the random variable has a probability density function ğ‘“ğ‘‹, then the char- acteristic function is its (inverse) Fourier transform up to a constant in the complex exponential.
If {ğ‘‹ğ‘–}ğ‘–âˆˆâ„• is a set of independent random variables and ğ‘ğ‘– âˆˆ â„ are constants,
then the characteristic function ğœ™ğ‘†ğ‘›
of the linear combination
ğ‘†ğ‘› âˆ¶=
ğ‘› âˆ‘ ğ‘–=1
ğ‘ğ‘–ğ‘‹ğ‘–
is given by
ğœ™ğ‘†ğ‘›
(ğ‘¡) =
ğ‘› âˆ ğ‘–=1
ğœ™ğ‘‹ğ‘–
(ğ‘ğ‘–ğ‘¡).
The characteristic function of the Delta distribution ğ›¿ğ‘ is
ğœ™ğ›¿ğ‘
(ğ‘¡) = eiğ‘¡ğ‘.
The characteristic function of the normal distribution ğ‘(ğœ‡,ğœ2) is
ğœ™ğ‘(ğœ‡,ğœ2)(ğ‘¡) = exp(iğ‘¡ğœ‡ âˆ’
ğœ2ğ‘¡2 2
).
The equality
ğœ™(ğ‘˜) ğ‘‹ (0) = iğ‘˜ğ”¼[ğ‘‹ğ‘˜] is useful since it relates the derivatives of the characteristic function at zero to the moments.
B.9 Types of Convergence
In order to define almost sure convergence, the limit supremum of a sequence of sets is needed.
Definition B.58 (limit infimum and limit supremum of a sequence of real numbers). The limit infimum and the limit supremum of a sequence âŸ¨ğ‘¥ğ‘›âŸ©ğ‘›âˆˆâ„• of real numbers is defined as
liminf ğ‘›â†’âˆ
limsup ğ‘›â†’âˆ
âˆ¶= lim ğ‘›â†’âˆ
âˆ¶= lim ğ‘›â†’âˆ
inf ğ‘–â‰¥ğ‘› sup ğ‘–â‰¥ğ‘›
ğ‘¥ğ‘–,
ğ‘¥ğ‘–.
187
(B.12)
188
Appendix B. Measure and Probability Theory
Lemma B.59. Suppose âŸ¨ğ‘¥ğ‘›âŸ©ğ‘›âˆˆâ„• is a sequence of real numbers. Then the equal- ities
liminf ğ‘›â†’âˆ
limsup ğ‘›â†’âˆ
ğ‘¥ğ‘› = sup ğ‘›âˆˆâ„• ğ‘¥ğ‘› = inf ğ‘›âˆˆâ„•
inf ğ‘–â‰¥ğ‘›
sup ğ‘–â‰¥ğ‘›
ğ‘¥ğ‘–,
ğ‘¥ğ‘–
hold.
Definition B.60 (limit infimum, limit supremum, and limit of a sequence of sets). Suppose that Î© is a set and that âŸ¨ğ´ğ‘›âŸ©ğ‘›âˆˆâ„• is a sequence of subsets ğ´ğ‘› âŠ‚ Î©. Then the limit infimum and the limit supremum of the sequence âŸ¨ğ´ğ‘›âŸ©ğ‘›âˆˆâ„• are defined as
liminf ğ‘›â†’âˆ
limsup ğ‘›â†’âˆ
ğ´ğ‘› âˆ¶= â‹ƒ ğ‘›âˆˆâ„• ğ´ğ‘› âˆ¶= â‹‚ ğ‘›âˆˆâ„•
â‹‚ ğ‘–â‰¥ğ‘›
â‹ƒ ğ‘–â‰¥ğ‘›
ğ´ğ‘–,
ğ´ğ‘–.
If the limit infimum and the limit supremum two sets are equal, the limit of the sequence âŸ¨ğ´ğ‘›âŸ©ğ‘›âˆˆâ„• exists and is written as
lim ğ‘›â†’âˆ
ğ´ğ‘› âˆ¶= liminf ğ‘›â†’âˆ
ğ´ğ‘› = limsup ğ‘›â†’âˆ
ğ´ğ‘›.
The following lemma shows how the limit infimum and the limit supremum of a sequence of sets can be written in terms of the limit infimum and the limit supremum of a sequence of real numbers and the indicator function ğ‘¥ â†¦ âŸ¦ğ‘¥ âˆˆ ğ´ğ‘›âŸ§ (see Definition B.2) that indicates whether ğ‘¥ is an element of ğ´ğ‘›.
Lemma B.61. Suppose that Î© is a set and that âŸ¨ğ´ğ‘›âŸ©ğ‘›âˆˆâ„• is a sequence of subsets ğ´ğ‘› âŠ‚ Î©. Then the limit infimum and the limit supremum of the sequence âŸ¨ğ´ğ‘›âŸ©ğ‘›âˆˆâ„• are equal to
liminf ğ‘›â†’âˆ limsup ğ‘›â†’âˆ
ğ´ğ‘› = {ğ‘¥ âˆˆ Î© âˆ¶ liminf ğ‘›â†’âˆ
ğ´ğ‘› = {ğ‘¥ âˆˆ Î© âˆ¶ limsup ğ‘›â†’âˆ
âŸ¦ğ‘¥ âˆˆ ğ´ğ‘›âŸ§ = 1},
âŸ¦ğ‘¥ âˆˆ ğ´ğ‘›âŸ§ = 1}.
In other words, ğ‘¥ âˆˆ liminfğ‘›â†’âˆ ğ´ğ‘› if and only if ğ‘¥ is an element of all but finitely many sets ğ´ğ‘›. Analogously, ğ‘¥ âˆˆ limsupğ‘›â†’âˆ ğ´ğ‘› if and only if ğ‘¥ is an element of infinitely many sets ğ´ğ‘›.
B.10. LÃ©vyâ€™s Continuity Theorem
Definition B.62 (almost sure convergence, convergence with probability one). A sequence âŸ¨ğ‘‹ğ‘›âŸ©ğ‘›âˆˆâ„• of random variables converges almost surely (or converges with probability one) to a random variable ğ‘‹ if
âˆ€ğœ– âˆˆ â„+âˆ¶
â„™[limsup ğ‘›â†’âˆ
{ğœ” âˆˆ Î© âˆ¶ |ğ‘‹ğ‘›(ğœ”) âˆ’ ğ‘‹(ğœ”)| > ğœ–}] = 0
holds. We then write
ğ‘‹ğ‘›
a. s. âˆ’âˆ’âŸ¶ ğ‘›â†’âˆ
ğ‘‹.
Definition B.63 (convergence in probability). A sequence âŸ¨ğ‘‹ğ‘›âŸ©ğ‘›âˆˆâ„• of random variables converges in probability to a random variable ğ‘‹ if
âˆ€ğœ– âˆˆ â„+âˆ¶
lim ğ‘›â†’âˆ
â„™[|ğ‘‹ğ‘› âˆ’ ğ‘‹| > ğœ–] = 0
holds. We then write
ğ‘‹ğ‘›
â„™ âˆ’âˆ’âŸ¶ ğ‘›â†’âˆ
ğ‘‹.
Almost sure convergence implies convergence in probability.
Definition B.64 (convergence in distribution, weak convergence, convergence in law). A sequence âŸ¨ğ‘‹ğ‘›âŸ©ğ‘›âˆˆâ„• of real valued random variables with the cumu- lative distribution functions ğ¹ğ‘› converges in distribution (or converges weakly or converges in law) to a random variable ğ‘‹ with the cumulative distribution function ğ¹ if
âˆ€ğ‘¥ âˆˆ {ğ‘¥ âˆˆ â„ âˆ¶ ğ¹ is continuous at ğ‘¥}âˆ¶
lim ğ‘›â†’âˆ
ğ¹ğ‘›(ğ‘¥) = ğ¹(ğ‘¥)
holds. We then write
ğ‘‹ğ‘›
d âˆ’âˆ’âŸ¶ ğ‘›â†’âˆ
ğ‘‹.
Convergence in probability implies convergence in distribution.
B.10 LÃ©vyâ€™s Continuity Theorem
Theorem B.65 (LÃ©vyâ€™s continuity theorem). Suppose that âŸ¨ğ‘‹ğ‘›âŸ©ğ‘›âˆˆâ„• is a se- quence of random variables, not necessarily sharing a common probability space. If the sequence âŸ¨ğœ™ğ‘›âŸ©ğ‘›âˆˆâ„• of their characteristic functions converges pointwise to a function ğœ™âˆ¶ â„ â†’ â„‚, i.e.,
lim ğ‘›â†’âˆ
ğœ™ğ‘›(ğ‘¡) = ğœ™(ğ‘¡) âˆ€ğ‘¡ âˆˆ â„,
then the following statements are equivalent:
189
190
Appendix B. Measure and Probability Theory
1. the ğ‘‹ğ‘› converge in distribution to a random variable ğ‘‹, i.e.,
ğ‘‹ğ‘›
d âˆ’âˆ’âŸ¶ ğ‘›â†’âˆ
ğ‘‹;
2. ğœ™ is the characteristic function of a random variable ğ‘‹;
3. ğœ™ is a continuous function;
4. ğœ™ is continuous at zero;
5. the sequence âŸ¨ğ‘‹ğ‘›âŸ©ğ‘›âˆˆâ„• is tight, i.e.,
lim ğ‘¥â†’âˆ
(sup ğ‘›âˆˆâ„•
â„™[|ğ‘‹ğ‘›| > ğ‘¥]) = 0.
Proofs can be found in [66, Section 18.1] and in [67, Theorems 14.15 and
18.21].
B.11 The Laws of Large Numbers and the Central
Limit Theorem
A natural question to ask how the mean value
ğ‘‹ğ‘› âˆ¶=
ğ‘†ğ‘› ğ‘›
,
where
ğ‘†ğ‘› âˆ¶=
ğ‘› âˆ‘ ğ‘–=1
ğ‘‹ğ‘–,
of ğ‘› independent and identically distributed random variables ğ‘‹ğ‘– behaves as ğ‘› â†’ âˆ and additionally how fast it converges if this is the case.
The answers are provided by the law of large numbers and the central limit
theorem via an expansion. Informally speaking, the law of large numbers
ğ‘‹ğ‘› =
ğ‘†ğ‘› ğ‘›
â†’ ğœ‡,
which holds if each ğ‘‹ğ‘– has finite mean ğœ‡, yields the first term, and the central limit theorem
âˆš
ğ‘›(ğ‘‹ğ‘› âˆ’ ğœ‡) =
ğ‘†ğ‘› âˆ’ ğ‘›ğœ‡ âˆš ğ‘›
â†’ ğœ‰ âˆ¼ ğ‘(0,ğœ2),
B.11. The Laws of Large Numbers and the Central Limit Theorem
which holds if additionally each ğ‘‹ğ‘– has finite variance ğœ2, yields the second term in the informal expansion
ğ‘‹ğ‘› â‰ˆ ğœ‡ +
ğœ‰ âˆš ğ‘›
or
ğ‘†ğ‘› â‰ˆ ğœ‡ğ‘› + ğœ‰
âˆš
ğ‘›.
In the following, the law of large numbers and the central limit theorem are
stated and proven.
Theorem B.66 (weak law of large numbers). Suppose âŸ¨ğ‘‹ğ‘–âŸ©ğ‘–âˆˆâ„• is a sequence of independent and identically distributed integrable random variables with expected value ğœ‡ âˆ¶= ğ”¼[ğ‘‹ğ‘–]. Then
ğ‘‹ğ‘›
â„™ âˆ’âˆ’âŸ¶ ğ‘›â†’âˆ
ğœ‡.
Proof using Chebyshevâ€™s inequality and assuming finite variance. Under the ad- ditional assumption that all random variables ğ‘‹ğ‘– have finite variance, i.e., ğ•[ğ‘‹ğ‘–] < âˆ for all ğ‘– âˆˆ â„•, Chebyshevâ€™s inequality, Theorem B.51, can be used to show the weak law of large numbers.
Since the random variables are independent, we have
ğ•[ğ‘‹ğ‘›] =
1 ğ‘›2ğ•[
ğ‘› âˆ‘ ğ‘–=1
ğ‘‹ğ‘–] =
ğ‘›ğœ2 ğ‘›2 =
ğœ2 ğ‘›
.
Therefore applying Chebyshevâ€™s inequality, Theorem B.51, to ğ‘‹ğ‘› yields
âˆ€ğ‘˜ âˆˆ â„+âˆ¶
â„™[|ğ‘‹ğ‘› âˆ’ ğœ‡| â‰¥ ğ‘˜] â‰¤
ğœ2 ğ‘˜2ğ‘›
,
which implies
âˆ€ğ‘˜ âˆˆ â„+âˆ¶
lim ğ‘›â†’âˆ
â„™[|ğ‘‹ğ‘› âˆ’ ğœ‡| â‰¥ ğ‘˜] = 0,
i.e.,
ğ‘‹ğ‘›
â„™ âˆ’âˆ’âŸ¶ ğ‘›â†’âˆ
ğœ‡,
which concludes the proof.
Proof using characteristic functions. The (complex) Taylor expansion around zero of the characteristic function ğœ™ğ‘‹1 of random variable ğ‘‹1 with finite mean ğœ‡ can be written as
ğœ™ğ‘‹1
(ğ‘¡) = 1 + iğœ‡ğ‘¡ + ğ‘œ(ğ‘¡),
191
192
Appendix B. Measure and Probability Theory
where ğ‘œ(ğ‘¡) denotes a function that goes to zero more rapidly than ğ‘¡. Here we have used (B.12); for ğ‘˜ = 0 we have ğœ™ğ‘‹1 (0) = iğœ‡. The same expansion holds for the other random variables, since they are all identically distributed.
(0) = 1, and for ğ‘˜ = 1 we find ğœ™â€²
ğ‘‹1
Therefore the characteristic function ğœ™ğ‘‹ğ‘›
of the mean ğ‘‹ğ‘› is
ğœ™ğ‘‹ğ‘›
(ğ‘¡) = (ğœ™ğ‘‹1
(
ğ‘¡ ğ‘›
))
ğ‘›
= (1 + iğœ‡
ğ‘¡ ğ‘›
+ ğ‘œ(
ğ‘¡ ğ‘›
ğ‘› ))
Its limit is
âˆ€ğ‘¡ âˆˆ â„âˆ¶
lim ğ‘›â†’âˆ
ğœ™ğ‘‹ğ‘›
(ğ‘¡) = eiğœ‡ğ‘¡
due to the well-known limit limğ‘›â†’âˆ(1 + ğ‘¥/ğ‘›)ğ‘› = eğ‘¥. The limit eiğœ‡ğ‘¡ is the characteristic function of the constant random variable ğœ‡. Therefore, by LÃ©vyâ€™s continuity theorem, Theorem B.65, the ğ‘‹ğ‘› converge in distribution to ğœ‡ as ğ‘› â†’ âˆ, i.e.,
ğ‘‹ğ‘›
d âˆ’âˆ’âŸ¶ ğ‘›â†’âˆ
ğœ‡.
Finally, since ğœ‡ is a constant, convergence in distribution to ğœ‡ and conver-
gence in probability to ğœ‡ are equivalent. Therefore we even have
ğ‘‹ğ‘›
â„™ âˆ’âˆ’âŸ¶ ğ‘›â†’âˆ
ğœ‡,
which concludes the proof.
Theorem B.67 (strong law of large numbers). Suppose âŸ¨ğ‘‹ğ‘–âŸ©ğ‘–âˆˆâ„• is a sequence of independent and identically distributed integrable random variables with expected value ğœ‡ âˆ¶= ğ”¼[ğ‘‹ğ‘–]. Then
a. s. âˆ’âˆ’âŸ¶ ğ‘›â†’âˆ Theorem B.68 (central limit theorem). Suppose âŸ¨ğ‘‹ğ‘–âŸ©ğ‘–âˆˆâ„• is a sequence of in- dependent and identically distributed random variables ğ‘‹ğ‘– with expected value ğœ‡ âˆ¶= ğ”¼[ğ‘‹ğ‘–] and finite variance ğœ2 âˆ¶= ğ•[ğ‘‹ğ‘–] < âˆ. Then âˆš
ğ‘‹ğ‘›
ğœ‡.
ğ‘›(ğ‘‹ğ‘› âˆ’ ğœ‡)
d âˆ’âˆ’âŸ¶ ğ‘›â†’âˆ
ğ‘(0,ğœ2).
In other words, the pointwise convergence to the cumulative distribution
function of the normal distribution ğ‘(0,ğœ2) means that
âˆ€ğ‘¥ âˆˆ â„âˆ¶
lim ğ‘›â†’âˆ
â„™[
âˆš
ğ‘›(ğ‘‹ğ‘› âˆ’ ğœ‡) â‰¤ ğ‘¥] = Î¦(
ğ‘¥ ğœ
),
where Î¦ is the cumulative distribution function of the standard normal distri- bution.
B.11. The Laws of Large Numbers and the Central Limit Theorem
193
Proof. The classical proof uses characteristic functions. Since the random vari- ables ğ‘‹ğ‘– are independent and identically distributed by assumption, their sum âˆ‘ğ‘›
ğ‘–=1 ğ‘‹ğ‘– has mean ğ‘›ğœ‡ and variance ğ‘›ğœ2. We define the random variables
ğ‘Œğ‘– âˆ¶=
ğ‘‹ğ‘– âˆ’ ğœ‡ ğœ
that have zero mean and unit variance. Just as the ğ‘‹ğ‘–, they are also independent and identically distributed. Using the ğ‘Œğ‘–, we have
ğ‘ğ‘› âˆ¶=
âˆ‘ğ‘›
ğ‘–=1 ğ‘‹ğ‘– âˆ’ ğ‘›ğœ‡ ğ‘›ğœ2
âˆš
=
ğ‘› âˆ‘ ğ‘–=1
ğ‘Œğ‘–âˆš ğ‘›
.
The characteristic function ğœ™ğ‘ğ‘›
of ğ‘ğ‘› is the product
ğœ™ğ‘ğ‘›
(ğ‘¡) = ğœ™âˆ‘ğ‘›
ğ‘–=1
ğ‘Œğ‘–âˆš ğ‘›
(ğ‘¡) =
ğ‘› âˆ ğ‘–=1
ğœ™ğ‘Œğ‘–
(
ğ‘¡ âˆš ğ‘›
) = (ğœ™ğ‘Œ1
(
ğ‘¡ âˆš ğ‘›
ğ‘› ))
,
where the last equality holds since all the ğ‘Œğ‘– are identically distributed.
The Taylor expansion of the characteristic function ğœ™ğ‘Œ1
around zero starts
with the terms
ğœ™ğ‘Œ1
(
ğ‘¡ âˆš ğ‘›
) = 1 âˆ’
ğ‘¡2 2ğ‘›
+ ğ‘œ(
ğ‘¡2 2ğ‘›
),
where ğ‘œ(ğ‘¡2/ğ‘›) denotes a function that goes to zero more rapidly than ğ‘¡2/ğ‘›. To (0) = 1, obtain this Taylor expansion, we have used (B.12); for ğ‘˜ = 0 we have ğœ™ğ‘Œ1 for ğ‘˜ = 1 we find ğœ™â€² (0) = ğ‘Œ1 âˆ’ğ”¼[ğ‘Œ 2
(0) = 0 since ğ”¼[ğ‘Œ1] = 0, and for ğ‘˜ = 2 we have ğœ™â€³ ğ‘Œ1
1 ] = âˆ’ğ•[ğ‘Œ1] = âˆ’1.
Using this Taylor expansion, we find the characteristic function of ğ‘ğ‘› as
ğœ™ğ‘ğ‘›
(ğ‘¡) = (1 âˆ’
ğ‘¡2 2ğ‘›
+ ğ‘œ(
ğ‘¡2 2ğ‘›
ğ‘› ))
,
which has the limit
lim ğ‘›â†’âˆ
ğœ™ğ‘ğ‘›
(ğ‘¡) = e
âˆ’ğ‘¡2/2
due to the well-known limit limğ‘›â†’âˆ(1 + ğ‘¥/ğ‘›)ğ‘› = eğ‘¥.
âˆ’ğ‘¡2/2
is the characteristic function of the standard normal distri- bution ğ‘(0,1). Therefore, by LÃ©vyâ€™s continuity theorem, Theorem B.65, the ğ‘ğ‘› converge in distribution to ğ‘(0,1) as ğ‘› â†’ âˆ, i.e.,
The limit e
ğ‘ğ‘›
d âˆ’âˆ’âŸ¶ ğ‘›â†’âˆ
ğ‘(0,1),
194
Appendix B. Measure and Probability Theory
which implies
âˆ‘ğ‘›
ğ‘–=1 ğ‘‹ğ‘– âˆ’ ğ‘›ğœ‡ ğ‘›
âˆš
d âˆ’âˆ’âŸ¶ ğ‘›â†’âˆ
ğ‘(0,ğœ2).
Since
âˆ‘ğ‘›
ğ‘–=1 ğ‘‹ğ‘– âˆ’ ğ‘›ğœ‡ ğ‘›
âˆš
=
ğ‘›ğ‘‹ğ‘› âˆ’ ğ‘›ğœ‡ âˆš ğ‘›
=
âˆš
ğ‘›(ğ‘‹ğ‘› âˆ’ ğœ‡),
we find
âˆš
ğ‘›(ğ‘‹ğ‘› âˆ’ ğœ‡)
d âˆ’âˆ’âŸ¶ ğ‘›â†’âˆ
ğ‘(0,ğœ2),
which concludes the proof.
B.12 Waldâ€™s Equation
In its basic form, Waldâ€™s equation makes it possible to simplify a sum of random variables, whose number of terms is itself a random variable. More precisely, suppose that the number of terms in the sum is an integer valued random vari- able ğ‘ such that ğ‘ â‰¥ 1 and that it is independent of the sequence âŸ¨ğ‘‹ğ‘›âŸ©ğ‘›âˆˆâ„• of real valued, independent, and identically distributed random variables ğ‘‹ğ‘› to be summed. Then the expected value of the sum of ğ‘ terms of ğ‘‹ğ‘› is given by
ğ”¼[
ğ‘ âˆ‘ ğ‘›=1
ğ‘‹ğ‘›] = ğ”¼[ğ‘]ğ”¼[ğ‘‹1],
i.e., it is equal to the expected number of terms times the expected value of a single term, as can be expected since all random variables are independent.
More generally, the following theorem holds.
Theorem B.69 (Waldâ€™s equation). Suppose
1. that âŸ¨ğ‘‹ğ‘›âŸ©ğ‘›âˆˆâ„• is a sequence of real valued integrable random variables,
2. that ğ‘ is an integer valued random variable such that ğ‘(Î©) âŠ‚ â„• âˆ– {0},
3. that ğ”¼[ğ‘‹ğ‘›âŸ¦ğ‘› â‰¤ ğ‘âŸ§] = ğ”¼[ğ‘‹ğ‘›]â„™[ğ‘› â‰¤ ğ‘] for all ğ‘› âˆˆ â„•, and
4. that the infinite sum
âˆ âˆ‘ ğ‘›=1
ğ”¼[|ğ‘‹ğ‘›|âŸ¦ğ‘› â‰¤ ğ‘âŸ§] < âˆ
converges.
B.12. Waldâ€™s Equation
Then the random variables
ğ‘†ğ‘ âˆ¶=
ğ‘ âˆ‘ ğ‘›=1
ğ‘‹ğ‘›,
ğ‘‡ğ‘ âˆ¶=
ğ‘ âˆ‘ ğ‘›=1
ğ”¼[ğ‘‹ğ‘›]
are integrable and have the same expected value, i.e.,
ğ”¼[ğ‘†ğ‘] = ğ”¼[ğ‘‡ğ‘].
If additionally
5. all random variables ğ‘‹ğ‘›, ğ‘› âˆˆ â„•, have the same expected value and
6. the random variable ğ‘ is integrable,
then Waldâ€™s equation
ğ”¼[ğ‘†ğ‘] = ğ”¼[ğ‘]ğ”¼[ğ‘‹1]
holds.
Proof. In the first step, we show that the random variable ğ‘†ğ‘ is integrable, i.e., ğ”¼[|ğ‘†ğ‘|] < âˆ. Using the partial sums
ğ‘†ğ‘– âˆ¶=
ğ‘– âˆ‘ ğ‘›=1
ğ‘‹ğ‘›,
ğ‘– âˆˆ â„•,
we have
|ğ‘†ğ‘| =
âˆ âˆ‘ ğ‘–=1
|ğ‘†ğ‘–|âŸ¦ğ‘– = ğ‘âŸ§.
The Lebesgue monotone convergence theorem, Theorem B.34, applied to the partial sums ğ‘˜ â†¦ âˆ‘ğ‘˜ ğ‘–=1 |ğ‘†ğ‘–|âŸ¦ğ‘– = ğ‘âŸ§ means that integration and summation can be interchanged, yielding
ğ”¼[|ğ‘†ğ‘|] = ğ”¼[
âˆ âˆ‘ ğ‘–=1
|ğ‘†ğ‘–|âŸ¦ğ‘– = ğ‘âŸ§] =
âˆ âˆ‘ ğ‘–=1
ğ”¼[|ğ‘†ğ‘–|âŸ¦ğ‘– = ğ‘âŸ§].
The triangle inequality gives
âˆ€ğ‘– âˆˆ â„•âˆ¶
|ğ‘†ğ‘–| â‰¤
ğ‘– âˆ‘ ğ‘›=1
|ğ‘‹ğ‘›|
195
196
Appendix B. Measure and Probability Theory
which implies
ğ”¼[|ğ‘†ğ‘|] =
âˆ âˆ‘ ğ‘–=1
ğ”¼[|ğ‘†ğ‘–|âŸ¦ğ‘– = ğ‘âŸ§] â‰¤
âˆ âˆ‘ ğ‘–=1
ğ‘– âˆ‘ ğ‘›=1
ğ”¼[|ğ‘‹ğ‘›|âŸ¦ğ‘– = ğ‘âŸ§].
Here the order of summation can be changed, since all terms are non-negative. Therefore we have the estimate
ğ”¼[|ğ‘†ğ‘|] â‰¤
âˆ âˆ‘ ğ‘›=1
âˆ âˆ‘ ğ‘–=ğ‘›
ğ”¼[|ğ‘‹ğ‘›|âŸ¦ğ‘– = ğ‘âŸ§] =
âˆ âˆ‘ ğ‘›=1
ğ”¼[|ğ‘‹ğ‘›|âŸ¦ğ‘› â‰¤ ğ‘âŸ§].
Assumption 4 now yields ğ”¼[|ğ‘†ğ‘|] < âˆ, i.e., the random variable ğ‘†ğ‘ is integrable. In the second step, we show that the random variable ğ‘‡ğ‘ is integrable, i.e.,
ğ”¼[|ğ‘‡ğ‘|] < âˆ. Using the partial sums
ğ‘‡ğ‘– âˆ¶=
ğ‘– âˆ‘ ğ‘›=1
ğ”¼[ğ‘‹ğ‘›],
ğ‘– âˆˆ â„•,
we have
|ğ‘‡ğ‘| =
âˆ âˆ‘ ğ‘–=1
|ğ‘‡ğ‘–|âŸ¦ğ‘– = ğ‘âŸ§.
Analogously to the first step, the Lebesgue monotone convergence theorem, Theorem B.34, yields
ğ”¼[|ğ‘‡ğ‘|] = ğ”¼[
âˆ âˆ‘ ğ‘–=1
|ğ‘‡ğ‘–|âŸ¦ğ‘– = ğ‘âŸ§] =
âˆ âˆ‘ ğ‘–=1
ğ”¼[|ğ‘‡ğ‘–|âŸ¦ğ‘– = ğ‘âŸ§] =
âˆ âˆ‘ ğ‘–=1
|ğ‘‡ğ‘–|â„™[ğ‘– = ğ‘].
The triangle inequality gives
âˆ€ğ‘– âˆˆ â„•âˆ¶
|ğ‘‡ğ‘–| â‰¤
ğ‘– âˆ‘ ğ‘›=1
|ğ”¼[ğ‘‹ğ‘›]|,
which implies
ğ”¼[|ğ‘‡ğ‘|] â‰¤
âˆ âˆ‘ ğ‘–=1
ğ‘– âˆ‘ ğ‘›=1
|ğ”¼[ğ‘‹ğ‘›]|â„™[ğ‘– = ğ‘].
Here the order of summation can be changed, since all terms are non-negative. Therefore we have the estimate
ğ”¼[|ğ‘‡ğ‘|] â‰¤
âˆ âˆ‘ ğ‘›=1
âˆ âˆ‘ ğ‘–=ğ‘›
|ğ”¼[ğ‘‹ğ‘›]|â„™[ğ‘– = ğ‘] =
âˆ âˆ‘ ğ‘›=1
|ğ”¼[ğ‘‹ğ‘›]|
âˆ âˆ‘ ğ‘–=ğ‘›
â„™[ğ‘– = ğ‘]
B.12. Waldâ€™s Equation
197
=
âˆ âˆ‘ ğ‘›=1
|ğ”¼[ğ‘‹ğ‘›]|â„™[ğ‘› â‰¤ ğ‘].
Due to Assumption 3 and Jensenâ€™s inequality, Theorem B.48, we find the esti- mates
âˆ€ğ‘› âˆˆ â„•âˆ¶
|ğ”¼[ğ‘‹ğ‘›]|â„™[ğ‘› â‰¤ ğ‘] = |ğ”¼[ğ‘‹ğ‘›âŸ¦ğ‘› â‰¤ ğ‘âŸ§]| â‰¤ ğ”¼[|ğ‘‹ğ‘›|âŸ¦ğ‘› â‰¤ ğ‘âŸ§]
for the single terms of the sum, which result in
ğ”¼[|ğ‘‡ğ‘|] â‰¤
âˆ âˆ‘ ğ‘›=1
ğ”¼[|ğ‘‹ğ‘›|âŸ¦ğ‘› â‰¤ ğ‘âŸ§].
Assumption 4 now yields ğ”¼[|ğ‘‡ğ‘|] < âˆ, i.e., the random variable ğ‘‡ğ‘ is integrable. In the third step, the expected value ğ”¼[ğ‘†ğ‘] is calculated. The Lebesgue dominated convergence theorem, Theorem B.35, with the functions ğ‘†ğ‘– and with the majorant |ğ‘†ğ‘| (since ğ‘ â‰¥ 1) yields
ğ”¼[ğ‘†ğ‘] = ğ”¼[
âˆ âˆ‘ ğ‘–=1
ğ‘†ğ‘–âŸ¦ğ‘– = ğ‘âŸ§] =
âˆ âˆ‘ ğ‘–=1
ğ”¼[ğ‘†ğ‘–âŸ¦ğ‘– = ğ‘âŸ§],
which can also be written as
ğ”¼[ğ‘†ğ‘] =
âˆ âˆ‘ ğ‘–=1
ğ‘– âˆ‘ ğ‘›=1
ğ”¼[ğ‘‹ğ‘›âŸ¦ğ‘– = ğ‘âŸ§]
by substituting the definition of ğ‘†ğ‘–. Because of the absolute convergence of this sum shown in the first step, the order of summation can be changed such that we arrive at
âˆ âˆ‘ ğ‘–=ğ‘› We again use the Lebesgue dominated convergence theorem, Theorem B.35, with the majorant |ğ‘‹ğ‘| to change the order of the expectation operator and the inner summation to find
âˆ âˆ‘ ğ‘›=1
ğ”¼[ğ‘†ğ‘] =
ğ”¼[ğ‘‹ğ‘›âŸ¦ğ‘– = ğ‘âŸ§].
ğ”¼[ğ‘†ğ‘] =
âˆ âˆ‘ ğ‘›=1
âˆ âˆ‘ ğ‘–=ğ‘›
ğ”¼[ğ‘‹ğ‘›âŸ¦ğ‘– = ğ‘âŸ§] =
âˆ âˆ‘ ğ‘›=1
ğ”¼[
âˆ âˆ‘ ğ‘–=ğ‘›
ğ‘‹ğ‘›âŸ¦ğ‘– = ğ‘âŸ§] =
âˆ âˆ‘ ğ‘›=1
ğ”¼[ğ‘‹ğ‘›âŸ¦ğ‘› â‰¤ ğ‘âŸ§].
Due to Assumption 3 and the ğœ-additivity of the probability measure, the terms in the last sum are equal to
ğ”¼[ğ‘‹ğ‘›âŸ¦ğ‘› â‰¤ ğ‘âŸ§] = ğ”¼[ğ‘‹ğ‘›]â„™[ğ‘› â‰¤ ğ‘] = ğ”¼[ğ‘‹ğ‘›]
âˆ âˆ‘ ğ‘–=ğ‘›
â„™[ğ‘– = ğ‘],
198
Appendix B. Measure and Probability Theory
which can be rewritten as
ğ”¼[ğ‘‹ğ‘›âŸ¦ğ‘› â‰¤ ğ‘âŸ§] =
âˆ âˆ‘ ğ‘–=ğ‘›
ğ”¼[ğ”¼[ğ‘‹ğ‘›]âŸ¦ğ‘– = ğ‘âŸ§]
using the properties of the expected value. With these terms, the sum becomes
ğ”¼[ğ‘†ğ‘] =
âˆ âˆ‘ ğ‘›=1
âˆ âˆ‘ ğ‘–=ğ‘›
ğ”¼[ğ”¼[ğ‘‹ğ‘›]âŸ¦ğ‘– = ğ‘âŸ§] =
âˆ âˆ‘ ğ‘–=1
ğ‘– âˆ‘ ğ‘›=1
ğ”¼[ğ”¼[ğ‘‹ğ‘›]âŸ¦ğ‘– = ğ‘âŸ§],
where we could change the order of summation due to the absolute convergence shown in the first step, i.e., ğ”¼[|ğ‘†ğ‘|] < âˆ. By the definition of ğ‘‡ğ‘–, this is equal to
ğ”¼[ğ‘†ğ‘] =
âˆ âˆ‘ ğ‘–=1
ğ”¼[ğ‘‡ğ‘–âŸ¦ğ‘– = ğ‘âŸ§] =
âˆ âˆ‘ ğ‘–=1
ğ”¼[ğ‘‡ğ‘âŸ¦ğ‘– = ğ‘âŸ§].
The Lebesgue dominated convergence theorem, Theorem B.35, with the majo- rant |ğ‘‡ğ‘| makes it possible to change the order of the expectation operator and the summation to find
ğ”¼[ğ‘†ğ‘] =
âˆ âˆ‘ ğ‘–=1
ğ”¼[ğ‘‡ğ‘âŸ¦ğ‘– = ğ‘âŸ§] = ğ”¼[
âˆ âˆ‘ ğ‘–=1
ğ‘‡ğ‘âŸ¦ğ‘– = ğ‘âŸ§].
Furthermore, we can calculate
ğ”¼[ğ‘†ğ‘] = ğ”¼[ğ‘‡ğ‘
âˆ âˆ‘ ğ‘–=1
âŸ¦ğ‘– = ğ‘âŸ§] = ğ”¼[ğ‘‡ğ‘âŸ¦1 â‰¤ ğ‘âŸ§] = ğ”¼[ğ‘‡ğ‘],
since the codomain of the random variable ğ‘ is â„• âˆ– {0}. This proves the third statement of the theorem.
If Assumptions 5 and 6 are additionally satisfied, then ğ”¼[ğ‘‡ğ‘] can be simplified
to
ğ‘ âˆ‘ ğ‘›=1 which completes the proof.
ğ”¼[ğ‘‡ğ‘] = ğ”¼[
ğ”¼[ğ‘‹ğ‘›]] = ğ”¼[ğ‘‹1]ğ”¼[
ğ‘ âˆ‘ ğ‘›=1
1] = ğ”¼[ğ‘]ğ”¼[ğ‘‹1],
In the last line of the proof, it becomes clear that it is only required that the expected values ğ”¼[ğ‘‹ğ‘›] of the random variables are identical; this is sufficient to lift them out of the sum and the outer expected value. In particular, the random variables are not required to be independent.
The last line of the proof also explains why all the sums in the statement of the theorem start at one and why zero is excluded from the codomain of the random variable ğ‘ in Assumption 2. These two facts match such that ğ”¼[âˆ‘ğ‘
ğ‘›=1 1] = ğ”¼[ğ‘].
B.12. Waldâ€™s Equation
199
200
Appendix B. Measure and Probability Theory
Appendix C
Stochastic Approximation
Stochastic approximation is concerned with iterative methods for finding roots or solving optimization problems. The functions whose roots or extreme values are approximated cannot be computed directly, but only estimated via random samples. Stochastic approximation is of great importance in machine learn- ing and artificial intelligence for two reasons. First, optimization problems are fundamental in machine learning. Second, in machine learning, the samples are drawn from a data distribution, meaning that only random samples of the objective functions are available.
C.1 Dvoretzkyâ€™s Approximation Theorem
The following two theorems are Dvoretzkyâ€™s approximation theorem and Dvoret- zkyâ€™s extended approximation theorem [68], cited according to [69, Theorems 4 and 5].
Theorem C.1 (Dvoretzkyâ€™s approximation theorem). Suppose that
1. (Î©,â„±,â„™) is a probability space,
2. âŸ¨â„±ğ‘›âŸ©ğ‘›âˆˆâ„• is an increasing sequence of sub-ğœ-fields of â„±,
3. âŸ¨ğ‘‹ğ‘›âŸ©ğ‘›âˆˆâ„• are â„±ğ‘›-measurable random real-valued variables,
4. âŸ¨ğ‘‡ğ‘›âˆ¶ â„ğ‘› â†’ â„âŸ©ğ‘›âˆˆâ„• is a sequence of measurable functions,
5. âŸ¨ğ‘Šğ‘›âŸ©ğ‘›âˆˆâ„• are â„±ğ‘›+1-measurable real-valued random variables such that
ğ‘‹ğ‘›+1 = ğ‘‡ğ‘›(ğ‘¥1,â€¦,ğ‘¥ğ‘›) + ğ‘Šğ‘›,
201
202
Appendix C. Stochastic Approximation
6. ğ”¼[ğ‘Šğ‘›|â„±ğ‘›] = 0,
7. âˆ‘ğ‘›âˆˆâ„• ğ”¼[ğ‘Š 2 8. âŸ¨ğ‘ğ‘›âŸ©ğ‘›âˆˆâ„•, âŸ¨ğ‘ğ‘›âŸ©ğ‘›âˆˆâ„•, and âŸ¨ğ‘ğ‘›âŸ©ğ‘›âˆˆâ„• are sequences of non-negative real numbers, ğ‘›] < âˆ,
9. limğ‘›â†’âˆ ğ‘ğ‘› = 0,
10. âˆ‘ğ‘›âˆˆâ„• ğ‘ğ‘› < âˆ,
11. âˆ‘ğ‘›âˆˆâ„• ğ‘ğ‘› = âˆ, and
12. there exists a point ğ‘¥âˆ— âˆˆ â„ such that
|ğ‘‡ğ‘›(ğ‘¥1,â€¦,ğ‘¥ğ‘›) âˆ’ ğ‘¥âˆ—| â‰¤ max(ğ‘ğ‘›,(1 + ğ‘ğ‘›)|ğ‘¥ğ‘› âˆ’ ğ‘¥âˆ—| âˆ’ ğ‘ğ‘›) âˆ€ğ‘› âˆˆ â„•.
Then the sequence âŸ¨ğ‘‹ğ‘›âŸ©ğ‘›âˆˆâ„• of random variables converges to ğ‘¥âˆ— with probability one, i.e.,
â„™[ lim ğ‘›â†’âˆ
ğ‘‹ğ‘› = ğ‘¥âˆ—] = 1.
In the extended version of this theorem, the three sequences âŸ¨ğ‘ğ‘›âŸ©ğ‘›âˆˆâ„•, âŸ¨ğ‘ğ‘›âŸ©ğ‘›âˆˆâ„•, and âŸ¨ğ‘ğ‘›âŸ©ğ‘›âˆˆâ„• are promoted to real-valued functions, and the ğ‘‡ğ‘› are promoted to â„±ğ‘›-measurable random variables.
Theorem C.2 (Dvoretzkyâ€™s extended approximation theorem). Suppose that
1. (Î©,â„±,â„™) is a probability space,
2. âŸ¨â„±ğ‘›âŸ©ğ‘›âˆˆâ„• is an increasing sequence of sub-ğœ-fields of â„±,
3. âŸ¨ğ‘‹ğ‘›âŸ©ğ‘›âˆˆâ„• are â„±ğ‘›-measurable random real-valued variables,
4. âŸ¨ğ‘‡ğ‘›âˆ¶ â„ğ‘› â†’ â„âŸ©ğ‘›âˆˆâ„• is a sequence of â„±ğ‘›-measurable real-valued random vari- ables,
5. âŸ¨ğ‘Šğ‘›âŸ©ğ‘›âˆˆâ„• are â„±ğ‘›+1-measurable real-valued random variables such that
ğ‘‹ğ‘›+1 = ğ‘‡ğ‘›(ğ‘¥1,â€¦,ğ‘¥ğ‘›) + ğ‘Šğ‘›,
6. ğ”¼[ğ‘Šğ‘›|â„±ğ‘›] = 0,
7. âˆ‘ğ‘›âˆˆâ„• ğ”¼[ğ‘Š 2 8. âŸ¨ğ‘ğ‘›âˆ¶ Î© â†’ â„+
ğ‘›] < âˆ,
0 âŸ©ğ‘›âˆˆâ„•, âŸ¨ğ‘ğ‘›âˆ¶ Î© â†’ â„+
0 âŸ©ğ‘›âˆˆâ„•, and âŸ¨ğ‘ğ‘›âˆ¶ Î© â†’ â„+
0 âŸ©ğ‘›âˆˆâ„• are sequences
of real-valued non-negative functions,
C.2. Venterâ€™s Generalization
9. â„™[limğ‘›â†’âˆ ğ‘ğ‘› = 0] = 1,
10. â„™[âˆ‘ğ‘›âˆˆâ„• ğ‘ğ‘› < âˆ] = 1,
11. â„™[âˆ‘ğ‘›âˆˆâ„• ğ‘ğ‘› = âˆ] = 1, and
12. there exists a point ğ‘¥âˆ— âˆˆ â„ such that
|ğ‘‡ğ‘›(ğ‘¥1,â€¦,ğ‘¥ğ‘›) âˆ’ ğ‘¥âˆ—| â‰¤ max(ğ‘ğ‘›,(1 + ğ‘ğ‘›)|ğ‘¥ğ‘› âˆ’ ğ‘¥âˆ—| âˆ’ ğ‘ğ‘›) âˆ€ğ‘› âˆˆ â„•.
Then the sequence âŸ¨ğ‘‹ğ‘›âŸ©ğ‘›âˆˆâ„• of random variables converges to ğ‘¥âˆ— with probability one, i.e.,
â„™[ lim ğ‘›â†’âˆ
ğ‘‹ğ‘› = ğ‘¥âˆ—] = 1.
C.2 Venterâ€™s Generalization
In Venterâ€™s generalization, the ğ‘‡ğ‘› are transformations of ğ»ğ‘› Ã— Î© into itself, where ğ» is a Hilbert space. The following is [70, Theorem 1].
Theorem C.3 (Venterâ€™s generalization: almost sure convergence). Suppose that (Î©,â„±,â„™) is a probability space and that (ğ»,â€–â€–) is a real separable Hilbert space. For all ğ‘› âˆˆ â„•, let ğ‘‡ğ‘›âˆ¶ ğ»ğ‘› Ã— Î© â†’ ğ»ğ‘› Ã— Î© be a transformation of ğ»ğ‘› Ã— Î© into itself. Let ğœƒ âˆˆ ğ». Let ğ‘ be a (finite) integer-valued random variable on Î© and suppose that for each sequence âŸ¨ğ‘¥ğ‘›âŸ©ğ‘›âˆˆâ„• in ğ» and for ğœ” âˆˆ Î©0 âˆˆ â„± with â„™[Î©0] = 1, we have, for ğ‘› > ğ‘(ğœ”),
â€–ğ‘‡ğ‘›(ğ‘¥1,â€¦,ğ‘¥ğ‘›,ğœ”) âˆ’ ğœƒâ€–2 â‰¤ max(ğ‘,(1 + ğ‘ğ‘›)â€–ğ‘¥ğ‘› âˆ’ ğœƒâ€–2 âˆ’ ğ‘ğ‘›),
where
1. ğ‘ is a positive constant,
2. ğ‘ğ‘› is a non-negative real-valued function on ğ»ğ‘› Ã— Î© such that
ğ‘ğ‘›(ğ‘¥1,â€¦,ğ‘¥ğ‘›,ğœ”) â‰¤ ğ¾1
and âˆ‘ ğ‘›âˆˆâ„•
ğ‘ğ‘›(ğ‘¥1,â€¦,ğ‘¥ğ‘›,ğœ”) < âˆ
for all sequences âŸ¨ğ‘¥ğ‘›âŸ© in ğ» and for all ğœ” âˆˆ Î©0,
3. ğ‘ğ‘› is a real-valued function on ğ»ğ‘› Ã— Î© such that for all âŸ¨ğ‘¥ğ‘›âŸ© in ğ»,
ğ‘ğ‘›(ğ‘¥1,â€¦,ğ‘¥ğ‘›,ğœ”) â‰¥ 0
if ğ‘› > ğ‘(ğœ”)
203
204
Appendix C. Stochastic Approximation
and if ğœ” âˆˆ Î©0, while, if
sup ğ‘›âˆˆâ„•
â€–ğ‘¥ğ‘›â€– < âˆ,
then
âˆ‘ ğ‘›âˆˆâ„•
ğ‘ğ‘›(ğ‘¥1,â€¦,ğ‘¥ğ‘›,ğœ”) = âˆ.
Let ğ‘‹1 be an arbitrary random element in ğ» and let the sequence âŸ¨ğ‘‹ğ‘›âŸ©ğ‘›âˆˆâ„• satisfy
ğ‘‹ğ‘›+1(ğœ”) = ğ‘‡ğ‘›(ğ‘‹1(ğœ”),â€¦,ğ‘‹ğ‘›(ğœ”),ğœ”) + ğ‘ˆğ‘›(ğœ”),
where âŸ¨ğ‘ˆğ‘›âŸ©ğ‘›âˆˆâ„• is a sequence of random elements satisfying the conditions
âˆ‘ ğ‘›âˆˆâ„•
ğ”¼[â€–ğ‘ˆğ‘›â€–2] < âˆ
(C.1)
and
â„™[âˆ‘ ğ‘›âˆˆâ„•
â€–ğ”¼[ğ‘ˆğ‘›|â„±ğ‘›]â€– < âˆ] = 1,
(C.2)
where âŸ¨â„±ğ‘›âŸ©ğ‘›âˆˆâ„• is an increasing sequence of sub-ğœ-fields of â„± having the properties that the random elements {ğ‘‹1,â€¦,ğ‘‹ğ‘›,ğ‘‡1(ğ‘‹1),â€¦,ğ‘‡ğ‘›(ğ‘‹1,â€¦,ğ‘‹ğ‘›)} are measur- able with respect to â„±ğ‘› for ğ‘› âˆˆ {2,3,â€¦} and that
{ğœ” âˆˆ Î© âˆ¶ ğ‘› > ğ‘(ğœ”)} âˆˆ â„±ğ‘›.
(C.3)
Then
â„™[limsup
ğ‘›â†’âˆ
â€–ğ‘‹ğ‘› âˆ’ ğœƒâ€–2 â‰¤ ğ‘] = 1.
Under the conditions of this theorem, the sequence âŸ¨ğ‘‹ğ‘›âŸ© is stochastically attracted towards the sphere with center ğœƒ and radius ğ‘1/2. The elements ğ‘‹ğ‘› will eventually almost surely be within or arbitrarily close to this sphere.
The following is [70, Theorem 2]. It states that under somewhat stronger conditions, âŸ¨ğ‘‹ğ‘›âŸ© will eventually be within or close to this sphere in the mean square (or quadratic mean) sense.
Theorem C.4 (Venterâ€™s generalization: convergence in mean square). Consider the set-up in Theorem C.3 and let the conditions be strengthened as follows: ğ‘ is a fixed finite integer and âŸ¨ğ‘ğ‘›âŸ©ğ‘›âˆˆâ„• a fixed sequence of non-negative numbers such that
âˆ‘ ğ‘›âˆˆâ„•
ğ‘ğ‘› < âˆ.
C.3. Example: Perturbed Fixed-Point Iteration
âŸ¨ğ‘ˆğ‘›âŸ©ğ‘›âˆˆâ„• satisfies (C.1) and instead of (C.2),
âˆ‘ ğ‘›âˆˆâ„•
ğ”¼[â€–ğ”¼[ğ‘ˆğ‘›|â„±ğ‘›]â€–2]
1/2
< âˆ.
Other conditions remain unchanged except in so far as they are changed by the new conditions introduced. Thus e.g. (C.3) becomes redundant. If
ğ”¼[â€–ğ‘‹ğ‘â€–2] < âˆ
then
limsup ğ‘›â†’âˆ
ğ”¼[â€–ğ‘‹ğ‘› âˆ’ ğœƒâ€–2] â‰¤ ğ‘.
Note that if the positive constant ğ‘ âˆˆ â„ can be chosen arbitrarily small, then Theorem C.3 gives conditions for almost sure convergence of âŸ¨ğ‘‹ğ‘›âŸ© to ğœƒ, and Theorem C.4 gives conditions for convergence in mean square (or in quadratic mean) of âŸ¨ğ‘‹ğ‘›âŸ© to ğœƒ.
C.3 Example: Perturbed Fixed-Point Iteration
A well-known example of a fixed-point iteration is the approximation of ğœ‹ using the contraction
ğ¾âˆ¶
ğ¼ â†’ ğ¼,
ğ‘¥ â†¦ ğ‘¥ + sinğ‘¥
on the interval ğ¼ âˆ¶= [3ğœ‹/4,5ğœ‹/4]. The function ğ¾ maps ğ¼ to ğ¼ since ğ¾â€²(ğ‘¥) = 1 + cosğ‘¥ â‰¥ 0 for all ğ‘¥ âˆˆ ğ¼, 3ğœ‹/4 â‰ˆ 2.356 < 3.063 â‰ˆ ğ¾(3ğœ‹/4), and ğ¾(5ğœ‹/4) â‰ˆ 3.220 < 3.927 â‰ˆ 5ğœ‹/4.
Furthermore, the function ğ¾ is indeed a contraction since it is continuously âˆš 2 for all ğ‘¥ âˆˆ ğ¼ (note cos(5ğœ‹/4) =
differentiable with 0 â‰¤ ğ¾â€²(ğ‘¥) â‰¤ 1 âˆ’ 1/ 2). Therefore the Lipschitz constant âˆ’1/
âˆš
ğ‘ âˆ¶= max ğ‘¥âˆˆğ¼
ğ¾â€²(ğ‘¥) = 1 âˆ’
1 âˆš 2
< 1
serves as the contraction factor.
Therefore, the assumptions of the Banach fixed-point theorem are satisfied.
The resulting fixed-point iteration is
ğ‘¥ğ‘›+1 âˆ¶= ğ¾(ğ‘¥ğ‘›) = ğ‘¥ğ‘› + sinğ‘¥ğ‘›,
ğ‘¥0 âˆ¶= 3 âˆˆ ğ¼,
and we know
âˆƒ!ğ‘¥âˆ— âˆˆ ğ¼ âˆ¶ ğ¾(ğ‘¥âˆ—) = ğ‘¥âˆ—,
205
206
Appendix C. Stochastic Approximation
iteration
05101520253035404550556065707580859095100
Fixed-Point Iteration (Constant Coefficient)
approximation
3.003.023.043.063.083.103.123.143.163.183.203.223.243.263.283.30
approximation
05101520253035404550556065707580859095100
3.003.023.043.063.083.103.123.143.163.183.203.223.243.263.283.30
iteration
Fixed-Point Iteration (Decreasing Coefficient)
Figure C.1: Perturbed fixed-point iteration (C.4) with constant perturbation 0.02â‹…ğ‘(0,1) (left) and decreasing perturbation (0.2/ğ‘›)â‹…ğ‘(0,1) (right), where ğ‘› is the iteration number and ğ‘(0,1) means drawing a random number according to the standard normal distribution. The initial value is 3 in all cases.
lim ğ‘›â†’âˆ
ğ‘¥ğ‘› = ğ‘¥âˆ—.
Letting ğ‘› â†’ âˆ in the iteration yields sin(ğ‘¥âˆ—) = 0, showing that ğ‘¥âˆ— âˆˆ ğ¼ is indeed ğ‘¥âˆ— = ğœ‹.
Unfortunately, numerical calculations involving real numbers are not exact, but include errors. This is true in our example as well, also because the sine must be approximated. This fact strongly motivates the consideration of the perturbed fixed-point iteration
ğ‘‹ğ‘›+1 âˆ¶= ğ¾(ğ‘‹ğ‘›) + ğ‘Šğ‘›,
(C.4)
a stochastic process, where the ğ‘Šğ‘› are random variables representing the error in each iteration. Hence the iterates ğ‘‹ğ‘› are random variables as well, denoted by capital letters. We assume that
ğ”¼[ğ‘Šğ‘›] = 0,
since a biased error should not occur or â€“ if it does â€“ should become part of the fixed-point operator.
Figure C.1 shows numerical calculations for the perturbed fixed-point itera-
tion.
Does the perturbed fixed-point iteration still converge? In which sense? To which limit? We use the theorems in Section C.1 and Section C.2 to answer these fundamental questions.
Theorem C.5 (convergence of perturbed fixed-point iteration). Suppose that (ğ»,â€–â€–) is a Hilbert space and that the function ğ¾âˆ¶ ğ» â†’ ğ» is a contraction with
C.3. Example: Perturbed Fixed-Point Iteration
respect to the norm â€–â€– of the Hilbert space. Denote the unique fixed point of the contraction ğ¾ by ğ‘¥âˆ—. Suppose that (Î©,â„±,â„™) is a probability space and that ğ‘Š is a random variable with zero mean, i.e., ğ”¼[ğ‘Š] = 0, and finite variance, i.e., ğ•[ğ‘Š] < âˆ. Suppose further that âŸ¨ğœ†ğ‘›âŸ©ğ‘›âˆˆâ„• is a sequence with ğœ†ğ‘› â‰¥ 0 for all ğ‘› âˆˆ â„• and âˆ‘ğ‘›âˆˆâ„• ğœ†2
ğ‘› < âˆ. Then the perturbed fixed-point iteration
ğ‘‹ğ‘›+1 âˆ¶= ğ¾[ğ‘‹ğ‘›] + ğœ†ğ‘›ğ‘Š
converges to ğ‘¥âˆ— with probability one, i.e.,
â„™[ lim ğ‘›â†’âˆ
ğ‘‹ğ‘› = ğ‘¥âˆ—] = 1.
Proof. The aim is to apply Theorem C.1, which generalizes to Hilbert spaces because of Theorem C.3. We try to find sequences âŸ¨ğ‘ğ‘›âŸ©ğ‘›âˆˆâ„•, âŸ¨ğ‘ğ‘›âŸ©ğ‘›âˆˆâ„•, and âŸ¨ğ‘ğ‘›âŸ©ğ‘›âˆˆâ„• satisfying the condition
â€–ğ¾(ğ‘¥ğ‘›) âˆ’ ğ‘¥âˆ—â€– â‰¤ max(ğ‘ğ‘›,(1 + ğ‘ğ‘›)â€–ğ‘¥ğ‘› âˆ’ ğ‘¥âˆ—â€– âˆ’ ğ‘ğ‘›) âˆ€ğ‘› âˆˆ â„•.
Because of
â€–ğ¾(ğ‘¥ğ‘›) âˆ’ ğ‘¥âˆ—â€– = â€–ğ¾(ğ‘¥ğ‘›) âˆ’ ğ¾(ğ‘¥âˆ—)â€– â‰¤ ğ‘â€–ğ‘¥ğ‘› âˆ’ ğ‘¥âˆ—â€– âˆ€ğ‘› âˆˆ â„•,
we try to find sequences such that
ğ‘â€–ğ‘¥ğ‘› âˆ’ ğ‘¥âˆ—â€– â‰¤ max(ğ‘ğ‘›,(1 + ğ‘ğ‘›)â€–ğ‘¥ğ‘› âˆ’ ğ‘¥âˆ—â€– âˆ’ ğ‘ğ‘›) âˆ€ğ‘› âˆˆ â„•.
Suppose that the second argument of the maximum is relevant as ğ‘› â†’ âˆ.
Then the estimate
ğ‘ğ‘› â‰¤ (ğ‘ğ‘› + 1 âˆ’ ğ‘)â€–ğ‘¥ğ‘› âˆ’ ğ‘¥âˆ—â€– âˆ€ğ‘› âˆˆ â„•
would hold. However, limğ‘›â†’âˆ ğ‘ğ‘› = 0 necessarily holds, and all contractions satisfy the inequality
â€–ğ‘¥ğ‘› âˆ’ ğ‘¥âˆ—â€– â‰¤
ğ‘ğ‘› 1 âˆ’ ğ‘
â€–ğ‘¥1 âˆ’ ğ‘¥0â€– âˆ€ğ‘› âˆˆ â„•.
Therefore there exists a constant ğ¶ âˆˆ â„+ such that ğ‘ğ‘› â‰¤ ğ¶ğ‘ğ‘›â€–ğ‘¥1 âˆ’ ğ‘¥0â€– for sufficiently large ğ‘›, which contradicts âˆ‘ğ‘›âˆˆâ„• ğ‘ğ‘› = âˆ.
Next, suppose that the first argument of the maximum is relevant as ğ‘› â†’ âˆ,
i.e.,
ğ‘â€–ğ‘¥ğ‘› âˆ’ ğ‘¥âˆ—â€– â‰¤ ğ‘ğ‘› âˆ€ğ‘› âˆˆ â„•.
207
208
Appendix C. Stochastic Approximation
Setting
ğ‘ğ‘› âˆ¶=
ğ‘ğ‘›+1 1 âˆ’ ğ‘
â€–ğ‘¥1 âˆ’ ğ‘¥0â€–
satisfies this inequality and the condition limğ‘›â†’âˆ ğ‘ğ‘› = 0. To ensure that the first argument of the maximum is relevant, we must finally ensure that
(1 + ğ‘ğ‘›)â€–ğ‘¥ğ‘› âˆ’ ğ‘¥âˆ—â€– âˆ’ ğ‘ğ‘› â‰¤ ğ‘ğ‘› âˆ€ğ‘› âˆˆ â„•,
which is equivalent to
(1 + ğ‘ğ‘›)â€–ğ‘¥ğ‘› âˆ’ ğ‘¥âˆ—â€– âˆ’
ğ‘ğ‘›+1 1 âˆ’ ğ‘
â€–ğ‘¥1 âˆ’ ğ‘¥0â€– â‰¤ ğ‘ğ‘› âˆ€ğ‘› âˆˆ â„•.
The left side can be estimated by
(1 + ğ‘ğ‘›)â€–ğ‘¥ğ‘› âˆ’ ğ‘¥âˆ—â€– âˆ’
ğ‘ğ‘›+1 1 âˆ’ ğ‘
â€–ğ‘¥1 âˆ’ ğ‘¥0â€– â‰¤ (1 + ğ‘ğ‘› âˆ’ ğ‘)
ğ‘ğ‘› 1 âˆ’ ğ‘
â€–ğ‘¥1 âˆ’ ğ‘¥0â€– â‰¤ ğ‘ğ‘› âˆ€ğ‘› âˆˆ â„•.
Setting
ğ‘ğ‘› âˆ¶= 0, ğ‘ğ‘› âˆ¶= â€–ğ‘¥1 âˆ’ ğ‘¥0â€–
satisfies all conditions.
In summary, the assumptions of Theorem C.1, which also holds for sequences
âŸ¨ğ‘‹ğ‘›âŸ© in the Hilbert space ğ» because of Theorem C.3, are satisfied.
C.4 Polyak and Tsypkinâ€™s Theorem
In [71], a general theorem regarding the convergence of various stochastic algo- rithms for minimizing functionals, based on the notion of the pseudogradient, is shown. The following is [71, Theorem 1].
Theorem C.6 (Polyak and Tsypkinâ€™s theorem). Suppose that
1. (ğ»,â€–â€–) is a Hilbert space,
2. the function ğ½ âˆ¶ ğ» â†’ â„ is differentiable and bounded from below, i.e., ğ½(c) â‰¥ ğ½âˆ— > âˆ’âˆ for all c âˆˆ ğ»,
3. the gradient of ğ½ is Lipschitz continuous, i.e.,
âˆƒğ¿ âˆˆ â„âˆ¶ âˆ€âˆ€x,y âˆˆ ğ» âˆ¶
â€–âˆ‡ğ½(x) âˆ’ âˆ‡ğ½(y)â€– â‰¤ ğ¿â€–x âˆ’ yâ€–,
(C.5)
C.4. Polyak and Tsypkinâ€™s Theorem
4. the sequence âŸ¨cğ‘›âŸ©ğ‘›âˆˆâ„• âŠ‚ ğ» is generated by c0 âˆˆ ğ» being arbitrary and
cğ‘› âˆ¶= cğ‘›âˆ’1 âˆ’ ğ›¾ğ‘›sğ‘›,
where âŸ¨ğ›¾ğ‘›âŸ©ğ‘›âˆˆâ„• âŠ‚ â„+ 0 ,
5. the sequence âŸ¨sğ‘›âŸ©ğ‘›âˆˆâ„• âŠ‚ ğ» consists of pseudogradients of ğ½, i.e.,
âˆ€ğ‘› âˆˆ â„•âˆ¶
âŸ¨âˆ‡ğ½(cğ‘›âˆ’1),ğ”¼[sğ‘›]âŸ© â‰¥ 0,
6. the step lengths are constrained by
âˆƒâŸ¨ğœ†ğ‘›âŸ©ğ‘›âˆˆâ„• âŠ‚ â„+ 0 âˆ¶ ğ”¼[â€–sğ‘›â€–2] â‰¤ ğœ†ğ‘› + ğ¾1ğ½(cğ‘›âˆ’1) + ğ¾2âŸ¨âˆ‡ğ½(cğ‘›âˆ’1),ğ”¼[sğ‘›]âŸ©,
âˆƒâˆƒğ¾1,ğ¾2 âˆˆ â„+ 0 âˆ¶
7. the sequence âŸ¨ğ›¾ğ‘›âŸ©ğ‘›âˆˆâ„• âŠ‚ â„+
0 satisfies
âˆ‘ ğ‘›âˆˆâ„•
ğ›¾ğ‘› = âˆ,
8. the sequences âŸ¨ğ›¾ğ‘›âŸ©ğ‘›âˆˆâ„• and âŸ¨ğœ†ğ‘›âŸ©ğ‘›âˆˆâ„• satisfy
âˆ‘ ğ‘›âˆˆâ„•
ğ›¾2 ğ‘›ğœ†ğ‘› < âˆ,
9. either
âˆ‘ ğ‘›âˆˆâ„•
ğ›¾2 ğ‘› < âˆ
or
âˆ€ğ‘› âˆˆ â„•âˆ¶ ğœ†ğ‘› = 0 âˆ§ ğ¾1 = 0 âˆ§ limsup ğ‘›â†’âˆ
ğ›¾ğ‘› <
2 ğ¿ğ¾2
.
Then the limit limğ‘›â†’âˆ ğ½(cğ‘›) almost surely exists, and
liminf ğ‘›â†’âˆ
âŸ¨âˆ‡ğ½(cğ‘›âˆ’1),ğ”¼[sğ‘›]âŸ© = 0 almost surely.
Based on this theorem, the following three corollaries are stated in [71].
Corollary C.7. In addition to the assumptions in Theorem C.6, suppose that for all ğœ– âˆˆ â„+ the inequality ğ½(cğ‘›âˆ’1) â‰¥ ğ½âˆ— + ğœ– implies
âŸ¨âˆ‡ğ½(cğ‘›âˆ’1),ğ”¼[sğ‘›]âŸ© â‰¥ ğ›¿(ğœ–) > 0,
where ğ›¿âˆ¶ â„+ â†’ â„+ is a monotonically increasing function. Then
lim ğ‘›â†’âˆ
ğ½(cğ‘›) = ğ½âˆ—
almost surely.
209
(C.6)
(C.7)
(C.8)
(C.9)
(C.10)
(C.11)
210
Appendix C. Stochastic Approximation
Corollary C.8. In addition to the assumptions in Theorem C.6, suppose that ğ» is a finite-dimensional space (ğ» = â„ğ‘‘), that sets of the form {c âˆˆ ğ» âˆ¶ ğ½(c) â‰¥ const.} are bounded, and that for all ğœ– âˆˆ â„+ the inequality â€–âˆ‡ğ½(cğ‘›âˆ’1)â€– â‰¥ ğœ– implies
âŸ¨âˆ‡ğ½(cğ‘›âˆ’1),ğ”¼[sğ‘›]âŸ© â‰¥ ğ›¿(ğœ–) > 0, where ğ›¿âˆ¶ â„+ â†’ â„+ is a monotonically increasing function. Then there almost surely exist a subsequence âŸ¨ğ‘›ğ‘–âŸ© âŠ‚ â„• and points câˆ— such that
âˆ‡ğ½(câˆ—) = 0, = câˆ— cğ‘›ğ‘– lim ğ‘–â†’âˆ ğ½(cğ‘›ğ‘–
almost surely,
lim ğ‘–â†’âˆ
) = ğ½(câˆ—) almost surely.
For any subset ğ¶ âŠ‚ ğ» and any point ğ‘ âˆˆ ğ», the distance from c to ğ¶ is
denoted by
ğœŒ(c,ğ¶) âˆ¶= inf xâˆˆğ¶
â€–x âˆ’ câ€–
in the following.
Corollary C.9. In addition to the assumptions in Theorem C.6, suppose that the set ğ¶âˆ— of minimum points of ğ½ is not empty, that for all ğœ– âˆˆ â„+ the inequality ğœŒ(c,ğ¶âˆ—) â‰¥ ğœ– implies
inf câˆˆğ»
ğ½(c) > ğ½âˆ—,
and that for all ğœ– âˆˆ â„+ the inequality ğœŒ(c,ğ¶âˆ—) â‰¥ ğœ– implies
âŸ¨âˆ‡ğ½(cğ‘›âˆ’1),ğ”¼[sğ‘›]âŸ© â‰¥ ğ›¿(ğœ–) > 0,
where ğ›¿âˆ¶ â„+ â†’ â„+ is a monotonically increasing function. Then
lim ğ‘›â†’âˆ
ğœŒ(cğ‘›,ğ¶âˆ—) = 0 almost surely,
lim ğ‘›â†’âˆ
ğ½(cğ‘›) = ğ½âˆ—
almost surely.
In particular, if ğ¶âˆ— consists of the single point câˆ—, then
lim ğ‘›â†’âˆ
cğ‘› = câˆ—
almost surely.
Proof of the theorem. The proof follows [71, Theorem 1]. By (C.5), Taylor expansion of ğ½ around c yields
âˆ€âˆ€c,z âˆˆ ğ» âˆ¶
|ğ½(c + z) âˆ’ ğ½(c) âˆ’ âŸ¨âˆ‡ğ½(c),zâŸ©| â‰¤
ğ¿ 2
â€–zâ€–2 2.
C.4. Polyak and Tsypkinâ€™s Theorem
211
By setting c âˆ¶= cğ‘› and z âˆ¶= ğ›¾ğ‘›sğ‘›, we have c + z = cğ‘›âˆ’1 and find
ğ½(cğ‘›) â‰¤ ğ½(cğ‘›âˆ’1) âˆ’ ğ›¾ğ‘›âŸ¨âˆ‡ğ½(cğ‘›âˆ’1),sğ‘›âŸ© +
ğ¿ 2
ğ‘›â€–sğ‘›â€–2 ğ›¾2 2.
We define â„±ğ‘›âˆ’1 to be the minimum ğœ-algebra generated by the random vari-
ables s1,â€¦,sğ‘›âˆ’1, and we define
ğœ‡ğ‘› âˆ¶= ğ½(cğ‘›) âˆ’ ğ½âˆ—.
Subtracting ğ½âˆ— and taking the conditional expectation of both sides of the last inequality yields
ğ”¼[ğœ‡ğ‘›|â„±ğ‘›âˆ’1] â‰¤ ğœ‡ğ‘›âˆ’1 âˆ’ ğ›¾ğ‘›âŸ¨âˆ‡ğ½(cğ‘›âˆ’1),ğ”¼[sğ‘›|â„±ğ‘›âˆ’1]âŸ© +
ğ¿ 2
ğ‘›ğ”¼[â€–sğ‘›â€–2 ğ›¾2
2|â„±ğ‘›âˆ’1]
â‰¤ ğœ‡ğ‘›âˆ’1 (1 +
+
ğ¿ 2
ğ›¾2 ğ‘›ğœ†ğ‘› +
ğ¾1ğ¿ 2 ğ¾1ğ¿ 2
ğ›¾2 ğ‘›) âˆ’ ğ›¾ğ‘›âŸ¨âˆ‡ğ½(cğ‘›âˆ’1),ğ”¼[sğ‘›|â„±ğ‘›âˆ’1]âŸ©(1 âˆ’
ğ›¾2 ğ‘›ğ½âˆ—,
ğ¾2ğ¿ 2
ğ›¾ğ‘›)
(C.12)
where we have used assumption (C.7) and the equation ğ½(cğ‘›âˆ’1) = ğœ‡ğ‘›âˆ’1 + ğ½âˆ—. Because of assumption (C.10) or (C.11), the inequality
1 âˆ’
ğ¾2ğ¿ 2
ğ›¾ğ‘› â‰¥ 0
holds for sufficiently large ğ‘›. Therefore and because of assumption (C.6), the second term on the right side of (C.12) is non-negative (before it is subtracted). Therefore inequality (C.12) simplifies to
ğ”¼[ğœ‡ğ‘›|â„±ğ‘›âˆ’1] â‰¤ ğœ‡ğ‘›âˆ’1 (1 +
ğ¾1ğ¿ 2
ğ›¾2 ğ‘›) +
ğ¿ 2
ğ›¾2 ğ‘›ğœ†ğ‘› +
ğ¾1ğ¿ 2
ğ›¾2 ğ‘›ğ½âˆ—.
Next, we show that âŸ¨ğœ‡ğ‘›âŸ©ğ‘›âˆˆâ„• is a semimartingale, i.e., ğ”¼[ğœ‡ğ‘›|â„±ğ‘›âˆ’1] â‰¤ ğœ‡ğ‘›âˆ’1 for
all ğ‘› âˆˆ â„•, under assumption (C.10) or (C.11).
Under assumption (C.10), â€¦ Under assumption (C.11), we have ğ¾1 = 0 and ğœ†ğ‘› = 0, showing that âŸ¨ğœ‡ğ‘›âŸ© is
a semimartingale.
In both cases, âŸ¨ğœ‡ğ‘›âŸ© is semimartingale, and hence limğ‘›â†’âˆ ğœ‡ğ‘› almost surely
exists, and the unconditional expectations ğ”¼[ğœ‡ğ‘›] are uniformly bounded, i.e.,
âˆƒğ‘ âˆˆ â„+ 0 âˆ¶
âˆ€ğ‘› âˆˆ â„•âˆ¶
ğ”¼[ğœ‡ğ‘›] â‰¤ ğ‘.
(C.13)
We note that all conclusions until now remain valid if assumption (C.10) is replaced by the weaker assumption limğ‘›â†’âˆ ğ›¾ğ‘› = 0.
212
Appendix C. Stochastic Approximation
Therefore we can replace the conditional expectations in (C.12) by uncondi-
tional expectations, yielding
ğ”¼[ğœ‡ğ‘›] â‰¤ (1 +
ğ¾1ğ¿ 2
ğ›¾2 ğ‘›)ğ”¼[ğœ‡ğ‘›âˆ’1]
âˆ’ ğ›¾ğ‘› (1 âˆ’
ğ¾2ğ¿ 2
ğ›¾ğ‘›)ğ”¼[âŸ¨âˆ‡ğ½(cğ‘›âˆ’1),ğ”¼[sğ‘›|â„±ğ‘›âˆ’1]âŸ©] +
ğ¿ 2
ğ›¾2 ğ‘›ğœ†ğ‘› +
ğ¾1ğ¿ 2
ğ›¾2 ğ‘›ğ½âˆ—.
Next, we sum the last inequality over ğ‘› from 1 to âˆ. Using (C.13) as well as assumption (C.9), we obtain
ğ¾1ğ¿ 2
ğ›¾2 ğ‘›ğ”¼[ğœ‡ğ‘›âˆ’1] < âˆ,
âˆ‘ ğ‘›âˆˆâ„• ğ¿ 2
ğ›¾2 ğ‘›ğœ†ğ‘› < âˆ,
âˆ‘ ğ‘›âˆˆâ„•
ğ¾1ğ¿ 2
|ğ½âˆ—|âˆ‘ ğ‘›âˆˆâ„•
ğ›¾2 ğ‘› < âˆ
for both assumptions (C.10) and (C.11). Hence, summing the inequality implies
âˆ‘ ğ‘›âˆˆâ„•
ğ›¾ğ‘› (1 âˆ’
ğ¾2ğ¿ 2
ğ›¾ğ‘›)ğ”¼[âŸ¨âˆ‡ğ½(cğ‘›âˆ’1),ğ”¼[sğ‘›|â„±ğ‘›âˆ’1]âŸ©] < âˆ.
For both assumptions (C.10) and (C.11) and for sufficiently large ğ‘›, the estimate
1 âˆ’
ğ¾2ğ¿ 2
ğ›¾ğ‘› â‰¥ ğœ– > 0
holds, which implies
âˆ‘ ğ‘›âˆˆâ„•
ğ›¾ğ‘›ğ”¼[âŸ¨âˆ‡ğ½(cğ‘›âˆ’1),ğ”¼[sğ‘›|â„±ğ‘›âˆ’1]âŸ©] < âˆ.
Assumption (C.6) means âŸ¨âˆ‡ğ½(cğ‘›âˆ’1),ğ”¼[sğ‘›|â„±ğ‘›âˆ’1]âŸ© â‰¥ 0, and assumption (C.8) means âˆ‘ğ‘›âˆˆâ„• ğ›¾ğ‘› = âˆ. Therefore, the last inequality implies that there exists a subsequence âŸ¨ğ‘›ğ‘–âŸ©ğ‘–âˆˆâ„• such that
lim ğ‘–â†’âˆ
ğ”¼[âŸ¨âˆ‡ğ½(cğ‘›ğ‘–âˆ’1),ğ”¼[sğ‘›ğ‘–
|â„±ğ‘›ğ‘–âˆ’1]âŸ©] = 0.
The last equation implies that there exists another subsequence âŸ¨ğ‘›ğ‘–ğ‘— that
âŸ©ğ‘—âˆˆâ„• such
lim ğ‘—â†’âˆ
âŸ¨âˆ‡ğ½(cğ‘›ğ‘–ğ‘—
âˆ’1),ğ”¼[sğ‘›ğ‘–ğ‘—
|â„±ğ‘›ğ‘–ğ‘—
âˆ’1]âŸ© = 0
almost surely.
C.4. Polyak and Tsypkinâ€™s Theorem
Because of assumption (C.6), the last equation implies
liminf ğ‘›â†’âˆ
âŸ¨âˆ‡ğ½(cğ‘›âˆ’1),ğ”¼[sğ‘›|â„±ğ‘›âˆ’1]âŸ© = 0,
which concludes the proof.
The following theorem is useful for checking the pseudogradient condition (C.6) in certain situations. There are three points: ğ‘¥âˆ— (the limit), ğ‘¥1 (the current iterate), and ğ‘¥2. We assume that the point ğ‘¥2 is closer (with respect to the ğ¿2-norm) to the limit ğ‘¥âˆ— than the current iterate ğ‘¥1. Then the angle between ğ‘¥âˆ— âˆ’ ğ‘¥1 and ğ‘¥2 âˆ’ ğ‘¥1 is acute.
Theorem C.10 (acute angle). Suppose that ğ» is a Hilbert space, and that the three arbitrary points ğ‘¥âˆ— âˆˆ ğ», ğ‘¥1 âˆˆ ğ», and ğ‘¥2 âˆˆ ğ» satisfy the inequality
âˆƒğ›¾ âˆˆ [0,1]âˆ¶
â€–ğ‘¥2 âˆ’ ğ‘¥âˆ—â€–2 â‰¤ ğ›¾â€–ğ‘¥1 âˆ’ ğ‘¥âˆ—â€–2.
Then the inequality
âŸ¨ğ‘¥âˆ— âˆ’ ğ‘¥1,ğ‘¥2 âˆ’ ğ‘¥1âŸ© â‰¥ (1 âˆ’ ğ›¾)â€–ğ‘¥âˆ— âˆ’ ğ‘¥1â€–2
2 â‰¥ 0
holds.
Proof. We start from the equation
âŸ¨ğ‘¥âˆ— âˆ’ ğ‘¥1,ğ‘¥âˆ— âˆ’ ğ‘¥2âŸ© + âŸ¨ğ‘¥âˆ— âˆ’ ğ‘¥1,ğ‘¥2 âˆ’ ğ‘¥1âŸ© = âŸ¨ğ‘¥âˆ— âˆ’ ğ‘¥1,ğ‘¥âˆ— âˆ’ ğ‘¥1âŸ© = â€–ğ‘¥âˆ— âˆ’ ğ‘¥1â€–2 2,
which yields
âŸ¨ğ‘¥âˆ— âˆ’ ğ‘¥1,ğ‘¥âˆ— âˆ’ ğ‘¥2âŸ© â€–ğ‘¥âˆ— âˆ’ ğ‘¥1â€–2
+
âŸ¨ğ‘¥âˆ— âˆ’ ğ‘¥1,ğ‘¥2 âˆ’ ğ‘¥1âŸ© â€–ğ‘¥âˆ— âˆ’ ğ‘¥1â€–2
= â€–ğ‘¥âˆ— âˆ’ ğ‘¥1â€–2.
In order to estimate the first term, we use the Cauchy-Schwarz inequality and the assumption to find
âŸ¨ğ‘¥âˆ— âˆ’ ğ‘¥1,ğ‘¥âˆ— âˆ’ ğ‘¥2âŸ© â‰¤ â€–ğ‘¥âˆ— âˆ’ ğ‘¥1â€–2â€–ğ‘¥âˆ— âˆ’ ğ‘¥2â€–2 â‰¤ ğ›¾â€–ğ‘¥âˆ— âˆ’ ğ‘¥1â€–2 2,
which implies
âŸ¨ğ‘¥âˆ— âˆ’ ğ‘¥1,ğ‘¥âˆ— âˆ’ ğ‘¥2âŸ© â€–ğ‘¥âˆ— âˆ’ ğ‘¥1â€–2
â‰¤ ğ›¾â€–ğ‘¥âˆ— âˆ’ ğ‘¥1â€–2.
Equation (C.14) and the last inequality imply
âŸ¨ğ‘¥âˆ— âˆ’ ğ‘¥1,ğ‘¥2 âˆ’ ğ‘¥1âŸ© â€–ğ‘¥âˆ— âˆ’ ğ‘¥1â€–2
â‰¥ (1 âˆ’ ğ›¾)â€–ğ‘¥âˆ— âˆ’ ğ‘¥1â€–2 â‰¥ 0,
which concludes the proof.
213
(C.14)
214
Appendix C. Stochastic Approximation
ğ‘¥2
ğ‘¥3
ğ‘¥1
ğ‘¥âˆ—
Figure C.2: Illustration of the geometry of the three points ğ‘¥âˆ—, ğ‘¥1, and ğ‘¥2 in Theorem C.10.
Equation (C.14) has a geometric interpretation. We project the point ğ‘¥2 onto ğ‘¥âˆ— âˆ’ ğ‘¥1. The first term is the length of the line segment from ğ‘¥âˆ— to the projection of ğ‘¥2, and the second term is the length of the line segment from the projection of ğ‘¥2 to ğ‘¥1; of course, the sum of the two lengths is the length of the line segment ğ‘¥âˆ— âˆ’ ğ‘¥1.
Unfortunately, Theorem C.10 does not hold for the maximum norm, as illus- trated in Figure C.2. In an application of Theorem C.6, ğ‘¥âˆ— would be the limit, ğ‘¥1 the current iterate, and ğ‘¥2 âˆ’ ğ‘¥1 the search direction. Then Theorem C.10
C.5. Bibliographical and Historical Remarks
can be used to show the pseudogradient condition, as the angle
ğœ™ = arccos
âŸ¨ğ‘¥âˆ— âˆ’ ğ‘¥1,ğ‘¥2 âˆ’ ğ‘¥1âŸ© â€–ğ‘¥âˆ— âˆ’ ğ‘¥1â€–2â€–ğ‘¥2 âˆ’ ğ‘¥1â€–2
indicated in the figure is acute. The blue line is normal to ğ‘¥âˆ— âˆ’ ğ‘¥1, which is the direction of the gradient, and therefore all points that lie in the same half- space as ğ‘¥âˆ— with respect to the blue line satisfy the pseudogradient condition. In particular, all points ğ‘¥2 that are closer to ğ‘¥âˆ— than ğ‘¥1 with respect to the ğ¿2-norm â€“ this is the assumption in Theorem C.10 and indicated by the blue circle â€“ satisfy the pseudogradient condition.
However, this is not true for points ğ‘¥3 that are closer to ğ‘¥âˆ— than ğ‘¥1 with respect to the maximum norm, i.e., if â€–ğ‘¥3âˆ’ğ‘¥âˆ—â€–âˆ â‰¤ â€–ğ‘¥1âˆ’ğ‘¥âˆ—â€–âˆ holds. All points with the same maximum norm â€–ğ‘¥1 âˆ’ ğ‘¥âˆ—â€–âˆ as the vector ğ‘¥1 âˆ’ ğ‘¥âˆ— are indicated by the red square. Points such as ğ‘¥3 satisfy this condition on the distance, but lie in the opposite half-space with respect to the blue line and hence their angle is not acute.
This effect is known as the maximization bias of ğ‘„-learning. Maximization bias is a known problem of ğ‘„-learning and means that the action-value function is overestimated, as taking the maximum of random approximations may lead to some state-action pairs to appear more valuable than they are. In the present context, we can interpret maximization bias as moving away (with respect to the Euclidean norm) from the true value.
C.5 Bibliographical and Historical Remarks
The Robbins-Monro algorithms was introduced in 1951 by Herbert Robbins and Sutton Monro in a paper entitled â€œa Stochastic Approximation Methodâ€ [72]. Dvoretzkyâ€™s theorem was published in 1956 [68]. A generalization to variables that take values in generic Hilbert spaces was shown in [70]. A survey of stochas- tic approximation can be found in [73]. A formalization of Dvoretzkyâ€™s theorem was given in [69].
C.6 Exercises
Exercise C.1 (implementation of sine). Find out how sine is implemented in your computer.
Exercise C.2 (perturbed approximation of ğœ‹). Implement the perturbed ap- proximation of ğœ‹ shown in Figure C.1. Plot histograms for both cases and discuss them.
215
216
Appendix C. Stochastic Approximation
Exercise C.3 (an inequality for squares). Show that the inequality
âˆ€ğ‘ âˆˆ â„âˆ¶ âˆ€ğ‘ âˆˆ â„âˆ¶
(ğ‘ + ğ‘)2 â‰¤ 2(ğ‘2 + ğ‘2)
(C.15)
holds.
Exercise C.4 (equivalence of ğ¿2- and ğ¿âˆ-norms). Show that the inequalities
âˆ€ğ‘¥ âˆˆ â„ğ‘‘âˆ¶
â€–ğ‘¥â€–âˆ â‰¤ â€–ğ‘¥â€–2 â‰¤
âˆš
ğ‘‘â€–ğ‘¥â€–âˆ
(C.16)
hold.
Appendix D
Software Libraries
This chapter contains an overview of software libraries that may be useful in implementing RL systems. The overview is in alphabetical order and probably not exhaustive.
D.1 Reinforcement Learning
D.1.1 Environments and Applications
1. BTGym is an event driven RL financial backtesting library.
https://github.com/Kismuz/btgym
2. Gymnasium is a fork of OpenAI Gym. It is an open-source Python library for developing and comparing RL algorithms by providing a standard API to communicate between learning algorithms and environments. It also provides a standard set of environments. https://github.com/Faramaâ€‘Foundation/Gymnasium
3. MuJoCo is a library for advanced physics simulation.
https://mujoco.org
https://github.com/googleâ€‘deepmind/mujoco
4. OpenAI Gym provides a standard API to communicate between learning algorithms and environments as well as a standard set of environments. It has been superseded by Gymnasium. https://github.com/openai/gym
217
218
Appendix D. Software Libraries
5. Personae provides RL algorithms and environments for quantitative trad- ing.
https://github.com/Ceruleanacg/Personae
6. RecSim is a platform for authoring simulation environments for recom- mender systems [20].
https://github.com/googleâ€‘research/recsim
7. TensorTrade is an RL library for trading.
https://github.com/tensortradeâ€‘org/tensortrade
D.1.2 Learning
1. Awesome Deep RL is a list of deep RL libraries.
https://github.com/kengz/awesomeâ€‘deepâ€‘rl
2. CleanRL is a deep RL library that provides single-file implementations.
https://github.com/vwxyzjn/cleanrl
3. DeepMind Acme is research framework for RL.
https://github.com/googleâ€‘deepmind/acme
4. DeepMind OpenSpiel is a framework for RL in games.
https://github.com/googleâ€‘deepmind/open_spiel
5. Google Dopamine is a research framework for prototyping RL algorithms.
https://github.com/google/dopamine
6. Keras RL provides deep RL algorithms and integration with the deep- learning library Keras.
https://github.com/kerasâ€‘rl/kerasâ€‘rl
7. Meta Pearl calls itself a production ready RL library.
https://pearlagent.github.io
https://github.com/facebookresearch/pearl/
8. Mushroom RL is Python RL library providing classical and deep RL al- gorithms.
https://github.com/MushroomRL/mushroomâ€‘rl
D.2. Artificial Neural Networks / Deep Learning
9. OpenAI Baselines is in maintenance mode.
https://github.com/openai/baselines
10. PFRL is a deep RL library based on PyTorch.
https://github.com/pfnet/pfrl
11. Ray RLlib is an open-source library with support for a variety of industry applications. RLlib is part of the Ray AI libraries.
https://github.com/rayâ€‘project/ray
https://docs.ray.io/en/master/rllib
12. Stable Baselines is a fork of the OpenAI Baselines library.
https://github.com/hillâ€‘a/stableâ€‘baselines
13. TensorFlow Agents is a RL library for TensorFlow.
https://www.tensorflow.org/agents/overview
14. Tensorforce is a TensorFlow library for deep RL.
https://github.com/tensorforce/tensorforce
15. Tianshou is an RL library based on PyTorch and Gymnasium.
https://github.com/thuâ€‘ml/tianshou/
D.1.3 Policy Evaluation
1. DICE, the Distribution Correction Estimation library.
https://github.com/googleâ€‘research/dice_rl
D.2 Artificial Neural Networks / Deep Learning
1. Flax can be used to use JAX with artificial neural networks.
https://flax.readthedocs.io/en/latest
2. JAX is library for high-performance numerical computing and large-scale machine learning.
https://jax.readthedocs.io/en/latest
3. Keras is a deep-learning API written in Python that runs on top of either JAX, TensorFlow, or PyTorch.
https://keras.io
219
220
Appendix D. Software Libraries
4. PyTorch is more research focused.
https://pytorch.org
5. TensorFlow is more industry focused.
https://www.tensorflow.org
D.3 Large Language Models
1. LLMs from Scratch contains code for coding, pre-training, and fine-tuning LLM.
https://github.com/rasbt/LLMsâ€‘fromâ€‘scratch
2. minGPT is a reimplementation of GPT training and inference using Py- Torch and well-suited for educational purposes.
https://github.com/karpathy/minGPT
3. nanoGPT is a rewrite of minGPT by the same author.
https://github.com/karpathy/nanoGPT
4. spaCy is a Python library for natural language processing and contains support for embeddings.
https://spacy.io
https://pypi.org/project/spacy/
D.3. Large Language Models
221
222
Appendix D. Software Libraries
Bibliography
[1] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun. â€œCARLA: an open urban driving simulatorâ€. In: Proceed- ings of the 1st Annual Conference on Robot Learning. 2017, pp. 1â€“16.
[2] Peter R. Wurman et al. â€œOutracing champion Gran Turismo drivers with deep reinforcement learningâ€. In: Nature 602 (2022), pp. 223â€“228. doi: 10.1038/s41586â€‘021â€‘04357â€‘7.
[3] Helmut Horvath. â€œDeep reinforcement learning with applications to au- tonomous drivingâ€. MA thesis. Vienna, Austria: TU Wien, 2024. url: http://Clemens.Heitzinger.name.
[4] Tobias Kietreiber. â€œCombining maximum entropy reinforcement learning with distributional ğ‘„-value approximation methodsâ€. MA thesis. Vienna, Austria: TU Wien, 2023. url: http://Clemens.Heitzinger.name.
[5] Pierrick Lorang, Horvath Helmut, Tobias Kietreiber, Patrik Zips, Clemens Heitzinger, and Matthias Scheutz. â€œAdapting to the â€œopen worldâ€: the util- ity of hybrid hierarchical reinforcement learning and symbolic planningâ€. In: Proc. 2024 IEEE International Conference on Robotics and Automa- tion (ICRA 2024). At press. May 2024. doi: TBD. url: TBD.
[6] Gerald Tesauro. â€œTemporal difference learning and TD-Gammonâ€. In: Com- munications of the ACM 38.3 (1995), pp. 58â€“68. doi: 10.1145/203330. 203343.
[7]
â€œTD-Gammonâ€. In: Encyclopedia of Machine Learning. Ed. by Claude Sammut and Geoffrey I. Webb. Springer US, 2010, pp. 955â€“956. doi: 10.1007/978â€‘0â€‘387â€‘30164â€‘8_813.
[8] David Silver et al. â€œMastering the game of Go without human knowledgeâ€.
In: Nature 550 (2017), pp. 354â€“359. doi: 10.1038/nature24270.
[9] David Silver et al. â€œA general reinforcement learning algorithm that mas-
ters chess, shogi, and Go through self-playâ€. In: Science 362 (2018), pp. 1140â€“ 1144.
223
224
Bibliography
[10] Noam Brown and Thomas Sandholm. â€œSuperhuman AI for multiplayer pokerâ€. In: Science 365.6456 (2019), pp. 885â€“890. doi: 10.1126/science. aay2400.
[11] Tobias Salzer. â€œReinforcement learning for games with imperfect infor- mation â€“ teaching an agent the game of Schnapsenâ€. MA thesis. Vienna, Austria: TU Wien, 2023. url: http://Clemens.Heitzinger.name.
[12] Stephen W. Falken. â€œComputers and Theorem Proofs: Toward an Artificial Intelligenceâ€. Cited in War Games (1983). PhD thesis. Cambridge, MA, USA: MIT, 1960.
[13] Volodymyr Mnih et al. â€œHuman-level control through deep reinforcement
learningâ€. In: Nature 518 (2015), pp. 529â€“533.
[14] Max Jaderberg et al. â€œHuman-level performance in 3D multiplayer games with population-based reinforcement learningâ€. In: Science 364 (2019), pp. 859â€“865.
[15] Aniruddh Raghu, Matthieu Komorowski, Imran Ahmed, Leo Celi, Peter Szolovits, and Marzyeh Ghassemi. Deep reinforcement learning for sepsis treatment. 2017. arXiv: 1711.09602.
[16] Markus BÃ¶ck, Julien Malle, Daniel Pasterk, Hrvoje Kukina, Ramin Hasani, and Clemens Heitzinger. â€œSuperhuman performance on sepsis MIMIC- III data by distributional reinforcement learningâ€. In: PLOS ONE 17.11 (2022). Impact factor of PLOS ONE: 3.752., e0275358/1â€“18. doi: 10.1371/ journal.pone.0275358. url: https://doi.org/10.1371/journal.pone. 0275358.
[17] Razvan Bologheanu, Lorenz Kapral, Daniel Laxar, Mathias Maleczek, Christoph Dibiasi, Sebastian Zeiner, Asan Agibetov, Ari Ercole, Patrick Thoral, Paul Elbers, Clemens Heitzinger, and Oliver Kimberger. â€œDevelopment of a re- inforcement learning algorithm to optimize corticosteroid therapy in criti- cally ill patients with sepsisâ€. In: Journal of Clinical Medicine 12.4 (2023). Impact factor of Journal of Clinicial Medicine: 4.964., pp. 1513/1â€“13. doi: 10.3390/jcm12041513. url: https://doi.org/10.3390/jcm12041513.
[18] Jongchan Baek, Changhyeon Lee, Young Sam Lee, Soo Jeon, and Soohee Han. â€œReinforcement learning to achieve real-time control of triple in- verted pendulumâ€. In: Engineering Applications of Artificial Intelligence 128 (2024), p. 107518. doi: 10.1016/j.engappai.2023.107518.
Bibliography
[19] Carlotta Tubeuf, Jakob aus der Schmitten, RenÃ© Hofmann, Clemens Heitzinger, and Felix Birkelbach. â€œImproving control of energy systems with reinforce- ment learning: application to a reversible pump turbineâ€. In: Proc. 18th ASME International Conference on Energy Sustainability (ES 2024). At press. Anaheim, CA, USA, July 2024, TBD. doi: TBD. url: TBD.
[20] Eugene Ie, Chih-wei Hsu, Martin Mladenov, Vihan Jain, Sanmit Narvekar, Jing Wang, Rui Wu, and Craig Boutilier. RecSim: a configurable simulation platform for recommender systems. 2019. arXiv: 1909.04847.
[21] M. Mehdi Afsar, Trafford Crump, and Behrouz Far. Reinforcement learn- ing based recommender systems: a survey. 2021. arXiv: 2101.06286.
[22] Federico Tomasi, Joseph Cauteruccio, Surya Kanoria, Kamil Ciosek, Mat- teo Rinaldi, and Zhenwen Dai. â€œAutomatic music playlist generation via simulation-based reinforcement learningâ€. In: Proc. 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD 2023). 2023, pp. 4948â€“4957. doi: 10.1145/3580305.3599777.
[23] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: an In-
troduction. 2nd edition. The MIT Press, 2018.
[24] Dimitri P. Bertsekas. Reinforcement learning and optimal control. Athena
Scientific, 2019.
[25] Dimitri P. Bertsekas. Rollout, policy iteration, and distributed reinforce-
ment learning. Athena Scientific, 2020.
[26] Dimitri P. Bertsekas. Abstract dynamic programming. Athena Scientific,
2022.
[27] Csaba SzepesvÃ¡ri. Algorithms for Reinforcement Learning. Morgan and
Claypool Publishers, 2010.
[28] Marc G. Bellemare, Will Dabney, and Mark Rowland. Distributional Re-
inforcement Learning. MIT Press, 2023.
[29] Warren B. Powell. Reinforcement Learning and Stochastic Optimization: a Unified Framework for Sequential Decisions. John Wiley & Sons, Inc., 2022.
[30] Laura Graesser and Wah Loon Keng. Foundations of Deep Reinforcement
Learning: Theory and Practice in Python. Addison-Wesley, 2020.
[31] Maxim Lapan. Deep Reinforcement Learning Hands-On. 2nd edition. Packt
Publishing, 2020.
[32] Miguel Morales. Grokking Deep Reinforcement Learning. Manning Publi-
cations Co., 2020.
225
226
Bibliography
[33] Alexander Zai and Brandon Brown. Deep Reinforcement Learning in Ac-
tion. Manning Publications Co., 2020.
[34] Richard Bellman. Eye of the Hurricane â€“ an Autobiography. World Scien-
tific Publishing, 1984.
[35] Richard Bellman. Dynamic Programming. Princeton University Press, 1957.
[36] Dimitri P. Bertsekas and John N. Tsitsiklis. Neuro-Dynamic Programming.
Athena Scientific, 1996.
[37] S.P. Singh and R.S. Sutton. â€œReinforcement learning with replacing eligi-
bility tracesâ€. In: Machine Learning 22.1â€“3 (1996), pp. 123â€“158.
[38] Richard Bellman. â€œDynamic programmingâ€. In: Science 153.3731 (1966),
pp. 34â€“37. doi: 10.1126/science.153.3731.34.
[39] Christopher J.C.H. Watkins. â€œLearning from Delayed Rewardsâ€. PhD the-
sis. University of Cambridge, 1989.
[40] Christopher J.C.H. Watkins and Peter Dayan. â€œQ-learningâ€. In: Machine
Learning 8 (1992), pp. 279â€“292. doi: 10.1007/BF00992698.
[41] Harold J. Kushner and Dean S. Clark. Stochastic Approximation Methods for Constrained and Unconstrained Systems. Springer-Verlag New York Inc., 1978.
[42] Tommi Jaakkola, Michael I. Jordan, and Satinder P. Singh. â€œOn the con- vergence of stochastic iterative dynamic programming algorithmsâ€. In: Neural Computation 6.6 (1994), pp. 1185â€“1201.
[43] John N. Tsitsiklis. â€œAsynchronous stochastic approximation and Q-learningâ€.
In: Machine Learning 16 (1994), pp. 185â€“202.
[44] Michael L. Littman and Csaba SzepesvÃ¡ri. â€œA generalized reinforcement- learning model: convergence and applicationsâ€. In: Proceedings of the 13th International Conference on Machine Learning (ICML 1996). Ed. by Lorenzo Saitta. Morgan Kaufmann, 1996, pp. 310â€“318.
[45] Csaba SzepesvÃ¡ri and Michael L. Littman. Generalized Markov decision processes: dynamic-programming and reinforcement-learning algorithms. Tech. rep. Technical Report CS-96-11. Providence, Rhode Island, USA: Department of Computer Science, Brown University, Nov. 1996.
[46]
IstvÃ¡n Szita, BÃ¡lint TakÃ¡cs, and AndrÃ¡s LÅ‘rincz. â€œğœ–-MDPs: learning in varying environmentsâ€. In: Journal of Machine Learning Research 3 (2002), pp. 145â€“174.
Bibliography
[47] LÃ©on Bottou, Frank E. Curtis, and Jorge Nocedal. â€œOptimization methods for large-scale machine learningâ€. In: SIAM Review 60.2 (2018), pp. 223â€“ 311.
[48] Richard S. Sutton, David A. McAllester, Satinder P. Singh, and Yishay Mansour. â€œPolicy gradient methods for reinforcement learning with func- tion approximationâ€. In: Advances in Neural Information Processing Sys- tems 12 (NIPS 1999). Ed. by S.A. Solla, T.K. Leen, and K. MÃ¼ller. MIT Press, 2000, pp. 1057â€“1063. url: http://papers.nips.cc/paper/ 1713â€‘ policyâ€‘ gradientâ€‘ methodsâ€‘ forâ€‘ reinforcementâ€‘ learningâ€‘ withâ€‘ functionâ€‘approximation.pdf.
[49] Wendell H. Fleming and Halil Mete Soner. Controlled Markov Processes
and Viscosity Solutions. 2nd edition. Springer, 2006.
[50] RÃ©mi Munos. â€œA study of reinforcement learning in the continuous case by the means of viscosity solutionsâ€. In: Machine Learning 40.3 (2000), pp. 265â€“299.
[51] Michael G. Crandall and Pierre-Louis Lions. â€œViscosity solutions of Hamilton- Jacobi equationsâ€. In: Trans. Amer. Math. Soc. 277.1 (1983), pp. 1â€“42.
[52] Lawrence C. Evans. Partial Differential Equations. 2nd edition. American
Mathematical Society, 2010.
[53] Michael G. Crandall, Hitoshi Ishii, and Pierre-Louis Lions. â€œUserâ€™s guide to viscosity solutions of second order partial differential equationsâ€. In: Bull. Amer. Math. Soc. 27.1 (1992), pp. 1â€“67.
[54] David Silver et al. â€œMastering the game of Go with deep neural networks
and tree searchâ€. In: Nature 529 (2016), pp. 484â€“489.
[55] Oriol Vinyals et al. â€œGrandmaster level in StarCraft II using multi-agent
reinforcement learningâ€. In: Nature 575 (2019), pp. 350â€“354.
[56] Matteo Hessel, Joseph Modayil, Hado van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver. â€œRainbow: combining improvements in deep reinforcement learningâ€. In: Proceedings of the 32nd AAAI Conference on Artificial In- telligence (AAAI-2018). Ed. by Sheila A. McIlraith and Kilian Q. Wein- berger. AAAI Press, 2018, pp. 3215â€“3222. url: https://www.aaai.org/ ocs/index.php/AAAI/AAAI18/paper/view/17204.
[57] Markus BÃ¶ck and Clemens Heitzinger. â€œSpeedy categorical distributional reinforcement learning and complexity analysisâ€. In: SIAM Journal on Mathematics of Data Science 4.2 (2022), pp. 675â€“693. doi: 10 . 1137 / 20M1364436. url: https://doi.org/10.1137/20M1364436.
227
228
Bibliography
[58] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A.N. Gomez, L. Kaiser, and I. Polosukhin. â€œAttention is all you needâ€. In: Advances in Neural Information Processing Systems 30 (NIPS 2017). Ed. by I. Guyon, U.V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett. Curran Associates, Inc., 2017, pp. 6000â€“6010. arXiv: 1706.03762. url: https://proceedings.neurips.cc/paper_files/paper/2017/file/ 3f5ee243547dee91fbd053c1c4a845aaâ€‘Paper.pdf.
[59] O. Press and L. Wolf. Using the output embedding to improve language
models. 2016. arXiv: 1608.05859.
[60] Long Ouyang et al. Training language models to follow instructions with
human feedback. 2022. arXiv: 2203.02155.
[61] Clemens Heitzinger. Algorithms with Julia â€“ Optimization, Machine Learn- ing, and Differential Equations using the Julia Language. ISBN 978-3-031- 16559-7, ISBN 978-3-031-16560-3 (eBook), DOI: 10.1007/978-3-031-16560- 3, https://doi.org/10.1007/978â€‘3â€‘031â€‘16560â€‘3. Springer Nature, 2022.
[62] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. 2017. arXiv: 1707.06347.
[63] John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, and Pieter Abbeel. Trust region policy optimization. 2015. arXiv: 1502.05477.
[64] Wassily Hoeffding. â€œProbability inequalities for sums of bounded ran- dom variablesâ€. In: Journal of the American Statistical Association 58.301 (1963), pp. 13â€“30.
[65] Robert J. Serfling. â€œProbability inequalities for the sum in sampling with-
out replacementâ€. In: Annals of Statistics 2.1 (1974), pp. 39â€“48.
[66] D. Williams. Probability with Martingales. Cambridge University Press,
1991.
[67] B.E. Fristedt and L.F. Gray. A Modern Approach to Probability Theory.
BirkhÃ¤user, 1996.
[68] Aryeh Dvoretzky. â€œOn stochastic approximationâ€. In: Proceedings of the Third Berkeley Symposium on Mathematical Statistics and Probability (1954â€“1955). University of California Press, 1956, pp. 39â€“55.
[69] Koundinya Vajjha, Barry Trager, Avraham Shinnar, and Vasily Pestun. â€œFormalization of a stochastic approximation theoremâ€. In: 13th Interna- tional Conference on Interactive Theorem Proving (ITP 2022). Ed. by June Andronick and Leonardo de Moura. Vol. 237. Leibniz International Proceedings in Informatics (LIPIcs). Dagstuhl, Germany: Schloss Dagstuhl
Bibliography
â€“ Leibniz-Zentrum fÃ¼r Informatik, 2022, 31:1â€“31:18. isbn: 978-3-95977- 252-5. doi: 10.4230/LIPIcs.ITP.2022.31. url: https://drops.dagstuhl. de/entities/document/10.4230/LIPIcs.ITP.2022.31.
[70] J.H. Venter. â€œOn Dvoretzky stochastic approximation theoremsâ€. In: An- nals of Mathematical Statistics 37.6 (1966), pp. 1534â€“1544. doi: 10.1214/ aoms/1177699145. url: https://doi.org/10.1214/aoms/1177699145.
[71] B.T. Polyak and Ya.Z. Tsypkin. â€œPseudogradient adaptation and training algorithmsâ€. In: Automation and Remote Control 34.3 (1973). Translation of Russian original, pp. 377â€“397.
[72] Herbert Robbins and Sutton Monro. â€œA stochastic approximation modelâ€. In: Annals of Mathematical Statistics 22.3 (1951), pp. 400â€“407. doi: 10. 1214/aoms/1177729586.
[73] Tze Leung Lai. â€œStochastic approximation (invited paper)â€. In: Annals of Statistics 31.2 (2003), pp. 391â€“406. doi: 10.1214/aos/1051027873. url: https://doi.org/10.1214/aos/1051027873.
229
230
Bibliography
List of Algorithms
1 2 3 4 5
6 7 8 9 10 11 12 13 14 15
. . . . . . . . . a simple algorithm for the multi-bandit problem. iterative policy evaluation for approximating v â‰ˆ ğ‘£ğœ‹ given ğœ‹ âˆˆ ğ’«. policy iteration for calculating v â‰ˆ ğ‘£âˆ— and ğœ‹ â‰ˆ ğœ‹âˆ—. . . . . . . . . . . . . . . . . . . value iteration for calculating v â‰ˆ ğ‘£âˆ— and ğœ‹ â‰ˆ ğœ‹âˆ—. first/every-visit MC prediction for calculating ğ‘£ â‰ˆ ğ‘£âˆ— given the 50 policy ğœ‹. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52 on-policy first-visit MC control for calculating ğœ‹ â‰ˆ ğœ‹âˆ—. . . . . . . 60 . . . . . . . . . . . . . . . TD(0) for calculating ğ‘‰ â‰ˆ ğ‘£ğœ‹ given ğœ‹. 62 . . . . . . . . . . . . . . . SARSA for calculating ğ‘„ â‰ˆ ğ‘âˆ— and ğœ‹âˆ—. 63 ğ‘„-learning for calculating ğ‘„ â‰ˆ ğ‘âˆ— and ğœ‹ â‰ˆ ğœ‹âˆ—. . . . . . . . . . . . 65 double ğ‘„-learning for calculating ğ‘„ â‰ˆ ğ‘âˆ— and ğœ‹ â‰ˆ ğœ‹âˆ—. . . . . . . . 66 deep ğ‘„-learning for calculating ğ‘„ â‰ˆ ğ‘âˆ—. . . . . . . . . . . . . . . . 68 ğ‘›-step TD for calculating ğ‘‰ â‰ˆ ğ‘£ğœ‹ given ğœ‹. . . . . . . . . . . . . . ğ‘›-step SARSA for calculating ğ‘„ â‰ˆ ğ‘âˆ— and ğœ‹ â‰ˆ ğœ‹âˆ—. . . . . . . . . . 70 gradient MC prediction for calculating Ì‚ğ‘£ğ‘¤ â‰ˆ ğ‘£ğœ‹ given the policy ğœ‹. 92 semigradient TD(0) prediction for calculating Ì‚ğ‘£ğ‘¤ â‰ˆ ğ‘£ğœ‹ given the policy ğœ‹. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
93 16 REINFORCE for calculating ğœ‹ğœƒ â‰ˆ ğœ‹âˆ—. . . . . . . . . . . . . . . . 106 17 REINFORCE with baseline for calculating ğœ‹ğœƒ â‰ˆ ğœ‹âˆ—. . . . . . . . . 108 . . . . . . . . . . . . 110 18 19 . . . . . . . . . . 122 20 Proximal policy optimization (PPO) for policy optimization [62]. 146
one-step actor critic for calculating ğœ‹ğœƒ â‰ˆ ğœ‹âˆ—. deep Q-network (DQN) with experience replay.
13 30 34 35
231
232
List of Algorithms
Index
action, 1 advantage function, 143 agent, 1 alignment, 142 artificial intelligence, 2, 135 attention, 136, 137
applications, 141 multi-head, 137, 140 scaled dot-product, 137, 140
auto-regression, 141 auto-regressive, 139
bootstrapping, 44
chess, 4 contraction, 151 creativity, 135 cumulative probability distribution, 150
Dartmouth, 135 decoder, 137 distribution state, 42 stationary, 43
embedding, 137, 139 encoder, 137 environment, 1 episode, 1 error
mean squared
Bellman error, 44 projected Bellman, 44
return, 44 temporal-difference, 45 value, 43
expectation, 150 experience, 17 experience-replay buffer, 17
function
piecewise constant, 149, 150 unit-step, 149
GPT
ChatGPT, 136, 142 InstructGPT, 142
importance-sampling ratio, 53 information
hidden, 2
integral
Riemann, 149 Riemann-Stieltjes, 148
integration
by parts, 149
Kullback-Leibler divergence, 144
large language model, 135 learnability, 45 learning
off-policy, 45 reinforcement, 1 supervised, 3 unsupervised, 3
233
234
Lipschitz constant, 151
machine translation, 137 Markov
decision process, 15 property, 15
maximization bias, 63, 215 model dimension, 137
natural language, 135 neural network, 135 deep, 137
partition, 147 policy, 1
behavior, 45 optimal, 2 target, 45
positional encoding, 137, 141 probability density function, 150
ğ‘„-learning, 63
return, 2 reward, 1 Rock Paper Scissors, 2
self-attention, 141 self-improvement, 135 softmax layer, 138 state, 1 superhuman, 136 syllable, 139
task
continuing, 43 episodic, 42
theorem
Banach fixed-point, 152
token, 137, 139 tokenization, 139 tokenizer, 139
transformer, 136, 137 transition, 17
variation
bounded, 148 total, 148
vocabulary, 139
Index